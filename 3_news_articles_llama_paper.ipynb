{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3be8afe",
   "metadata": {},
   "source": [
    "# Implementing \"FX sentiment analysis with large language models\" (Ballinari et al.)\n",
    "This paper can be found at "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a21868",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423e5cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Amogh/OneDrive - University of Cambridge/Programming-New/Part-II-Project/venv_wsl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report\n",
    "# import bitsandbytes as bnb\n",
    "# import accelerate\n",
    "# import trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb648241",
   "metadata": {},
   "source": [
    "## 1. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccdfb1",
   "metadata": {},
   "source": [
    "### 1.1. Filtering\n",
    "- Load the datasets\n",
    "- Drop articles with <20 words\n",
    "- Remove duplicate articles \n",
    "- Convert time to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39896162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dailyfx news articles dataset (Title,Author,Date,Full Text,URL)\n",
    "df_news = pd.read_csv('datasets/news_articles/dailyfx_articles_012011-062024.csv')\n",
    "df_news = df_news[['Title', 'Date', 'Full Text', 'URL']]\n",
    "\n",
    "# Load the fxstreet news articles dataset into same df (Title,Date,Full Text,URL)\n",
    "temp = pd.read_csv('datasets/news_articles/fxstreet_articles_062024-072024.csv')\n",
    "temp = temp[['Title', 'Date', 'Full Text', 'URL']]\n",
    "df_news = pd.concat([df_news, temp], ignore_index=True)\n",
    "\n",
    "# Load investing.com news articles dataset into same df (Title,Full Text,URL,Date,Author)\n",
    "temp = pd.read_csv('datasets/news_articles/investing_articles_062024-072024.csv')\n",
    "temp = temp[['Title', 'Date', 'Full Text', 'URL']]\n",
    "df_news = pd.concat([df_news, temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11701c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop articles with <20 words\n",
    "df_news = df_news[df_news['Full Text'].str.split().str.len() >= 20]\n",
    "\n",
    "# Remove duplicate articles\n",
    "df_news = df_news.drop_duplicates(subset=['Full Text'])\n",
    "df_news = df_news.drop_duplicates(subset=['Title'])\n",
    "df_news = df_news.drop_duplicates(subset=['URL'])\n",
    "\n",
    "# Convert time to datetime\n",
    "df_news['Date'] = pd.to_datetime(df_news['Date'], utc=True)\n",
    "\n",
    "# Only keep articles before 2020\n",
    "df_news = df_news[df_news['Date'] < pd.to_datetime('2020-01-01', utc=True)]\n",
    "\n",
    "# Truncate articles to max 32,767 characters\n",
    "df_news['Full Text'] = df_news['Full Text'].str[:32767]\n",
    "\n",
    "# Randomly sample 30000 articles\n",
    "df_news = df_news.sample(n=30000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb17a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to New York timezone\n",
    "df_news['Date'] = df_news['Date'].dt.tz_convert('America/New_York')\n",
    "\n",
    "# Remove time info - paper says use date\n",
    "df_news['Date'] = df_news['Date'].dt.normalize()\n",
    "\n",
    "# Sort by date\n",
    "df_news = df_news.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea60f85",
   "metadata": {},
   "source": [
    "### 1.2. Creating mentioned_currency column\n",
    "- Use regex to capture all the currencies used in an article\n",
    "- Make use of common synomyms\n",
    "- Filter articles that don't mention any of the G10 currencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e119ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping ISO codes to the regex patterns (synonyms) from Figure A.1\n",
    "currency_synonyms = {\n",
    "    \"EUR\": [r\"EUR\", r\"Euro\"],\n",
    "    \"USD\": [r\"USD\", r\"Dollar\", r\"Dollars\", r\"US Dollar\", r\"US-Dollar\", r\"U\\.S\\. Dollar\", \n",
    "            r\"US Dollars\", r\"US-Dollars\", r\"U\\.S\\. Dollars\", r\"Greenback\"],\n",
    "    \"JPY\": [r\"JPY\", r\"Yen\", r\"Japanese Yen\"],\n",
    "    \"GBP\": [r\"GBP\", r\"Pound\", r\"Pounds\", r\"Sterling\", r\"British Pound\", r\"British Pounds\"],\n",
    "    \"AUD\": [r\"AUD\", r\"Australian Dollar\", r\"Australian Dollars\", r\"Aussie\"],\n",
    "    \"CAD\": [r\"CAD\", r\"Canadian Dollar\", r\"Canadian Dollars\"],\n",
    "    \"CHF\": [r\"CHF\", r\"Swiss Franc\", r\"Swiss Francs\", r\"Swissie\"],\n",
    "    \"NZD\": [r\"NZD\", r\"New Zealand Dollar\", r\"New Zealand Dollars\", r\"Kiwi\"],\n",
    "    \"NOK\": [r\"NOK\", r\"Norwegian Krone\", r\"Norwegian Kroner\"],\n",
    "    \"SEK\": [r\"SEK\", r\"Swedish Krona\", r\"Swedish Kronor\"]\n",
    "}\n",
    "\n",
    "# Get list of mentioned currencies from text\n",
    "def get_mentioned_currencies(text):\n",
    "    mentioned_currencies = list()\n",
    "\n",
    "    for currency, patterns in currency_synonyms.items():\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                mentioned_currencies.append(currency)\n",
    "                break\n",
    "\n",
    "    return mentioned_currencies\n",
    "\n",
    "df_news['mentioned_currencies'] = df_news['Full Text'].apply(get_mentioned_currencies)\n",
    "\n",
    "# Filter articles to keep only those where 'mentioned_currencies' is non empty\n",
    "df_news = df_news[df_news['mentioned_currencies'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# So that it is incrimenting by 1 properly due to dropped values from before\n",
    "df_news = df_news.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de90cbd",
   "metadata": {},
   "source": [
    "### 1.3. Getting historical prices\n",
    "Using nominal narrow effective exchange rate (daily) for each country.\n",
    "\n",
    "Narrow effective exchange rate is a good proxy for the tradable currency index that the authors used.\n",
    "\n",
    "Allows us to put a number to the currency rather than using a pair as then the currency can be effected by the other in the pair. \n",
    "\n",
    "It is done by taking the geometric mean from the exchange rate of various other currencies (narrow means only a small number of industrialised countries so that the average isn't skewed by some other non industrialised country going down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec17aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USD</th>\n",
       "      <th>EUR</th>\n",
       "      <th>JPY</th>\n",
       "      <th>GBP</th>\n",
       "      <th>CAD</th>\n",
       "      <th>AUD</th>\n",
       "      <th>CHF</th>\n",
       "      <th>SEK</th>\n",
       "      <th>NOK</th>\n",
       "      <th>NZD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-03</th>\n",
       "      <td>82.07</td>\n",
       "      <td>103.41</td>\n",
       "      <td>120.99</td>\n",
       "      <td>105.23</td>\n",
       "      <td>129.89</td>\n",
       "      <td>130.38</td>\n",
       "      <td>87.30</td>\n",
       "      <td>116.49</td>\n",
       "      <td>136.00</td>\n",
       "      <td>98.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>82.05</td>\n",
       "      <td>103.98</td>\n",
       "      <td>119.81</td>\n",
       "      <td>105.84</td>\n",
       "      <td>129.59</td>\n",
       "      <td>128.97</td>\n",
       "      <td>86.05</td>\n",
       "      <td>116.47</td>\n",
       "      <td>135.74</td>\n",
       "      <td>97.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>82.77</td>\n",
       "      <td>102.87</td>\n",
       "      <td>120.35</td>\n",
       "      <td>106.70</td>\n",
       "      <td>128.98</td>\n",
       "      <td>128.61</td>\n",
       "      <td>86.01</td>\n",
       "      <td>116.37</td>\n",
       "      <td>135.51</td>\n",
       "      <td>97.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-06</th>\n",
       "      <td>83.09</td>\n",
       "      <td>102.38</td>\n",
       "      <td>119.32</td>\n",
       "      <td>106.95</td>\n",
       "      <td>130.06</td>\n",
       "      <td>128.83</td>\n",
       "      <td>85.40</td>\n",
       "      <td>116.25</td>\n",
       "      <td>135.96</td>\n",
       "      <td>97.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-07</th>\n",
       "      <td>83.46</td>\n",
       "      <td>101.62</td>\n",
       "      <td>119.31</td>\n",
       "      <td>107.42</td>\n",
       "      <td>130.42</td>\n",
       "      <td>128.88</td>\n",
       "      <td>86.29</td>\n",
       "      <td>115.74</td>\n",
       "      <td>136.06</td>\n",
       "      <td>97.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              USD     EUR     JPY     GBP     CAD     AUD    CHF     SEK  \\\n",
       "date                                                                       \n",
       "2011-01-03  82.07  103.41  120.99  105.23  129.89  130.38  87.30  116.49   \n",
       "2011-01-04  82.05  103.98  119.81  105.84  129.59  128.97  86.05  116.47   \n",
       "2011-01-05  82.77  102.87  120.35  106.70  128.98  128.61  86.01  116.37   \n",
       "2011-01-06  83.09  102.38  119.32  106.95  130.06  128.83  85.40  116.25   \n",
       "2011-01-07  83.46  101.62  119.31  107.42  130.42  128.88  86.29  115.74   \n",
       "\n",
       "               NOK    NZD  \n",
       "date                       \n",
       "2011-01-03  136.00  98.33  \n",
       "2011-01-04  135.74  97.42  \n",
       "2011-01-05  135.51  97.75  \n",
       "2011-01-06  135.96  97.37  \n",
       "2011-01-07  136.06  97.72  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All links to get data from for effective exchage rate\n",
    "urls = {\n",
    "    \"USD\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.US?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"EUR\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.XM?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"JPY\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.JP?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"GBP\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.GB?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"CAD\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.CA?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"AUD\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.AU?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"CHF\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.CH?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\", \n",
    "    \"SEK\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.SE?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"NOK\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.NO?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"NZD\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.NZ?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\"\n",
    "}\n",
    "\n",
    "# Initialise an empty DataFrame (EER = effective exchange rate)\n",
    "df_EER = pd.DataFrame()\n",
    "\n",
    "for code, url in urls.items():\n",
    "    # Read only the required columns from the CSV\n",
    "    df_temp = pd.read_csv(url, usecols=lambda c: c in [\"TIME_PERIOD\", \"OBS_VALUE\"])\n",
    "    \n",
    "    # Convert OBS_VALUE to float for log calculations later\n",
    "    df_temp[\"OBS_VALUE\"] = pd.to_numeric(df_temp[\"OBS_VALUE\"], errors=\"coerce\")\n",
    "    \n",
    "    # Rename \"OBS_VALUE\" to currency code\n",
    "    df_temp = df_temp.rename(columns={\n",
    "        \"OBS_VALUE\": code,\n",
    "        \"TIME_PERIOD\": \"date\"\n",
    "    })\n",
    "    \n",
    "    # If the main df is empty, set it to this df\n",
    "    if df_EER.empty:\n",
    "        df_EER = df_temp\n",
    "    else:\n",
    "        # Join on \"date\", keep all records (outer join)\n",
    "        df_EER = pd.merge(df_EER, df_temp, on=[\"date\"], how='outer')\n",
    "\n",
    "\n",
    "df_EER['date'] = pd.to_datetime(df_EER['date'])\n",
    "df_EER = df_EER.set_index('date')\n",
    "\n",
    "# drop all NaNs in the data\n",
    "df_EER.dropna(inplace=True)\n",
    "\n",
    "df_EER.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee101bb9",
   "metadata": {},
   "source": [
    "### 1.4 Calculate log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2966a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily log returns\n",
    "df_log_returns = np.log(df_EER / df_EER.shift(1))\n",
    "\n",
    "df_log_returns.dropna(inplace=True)  # created in the shifting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31907a01",
   "metadata": {},
   "source": [
    "### 1.5 Calculate cumulative 5 day windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ff1f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future returns\n",
    "# At index t, we want the sum of t+1, t+2, t+3, t+4, t+5 returns\n",
    "df_future_returns = df_log_returns.rolling(window=5, min_periods=5).sum().shift(-5)\n",
    "df_future_returns.dropna(inplace=True)\n",
    "\n",
    "# Past returns\n",
    "# At index t, we want the sum of t-1, t-2, t-3, t-4, t-5 returns\n",
    "df_past_returns = df_log_returns.rolling(window=5, min_periods=5).sum().shift(1)\n",
    "df_past_returns.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Merge future and past returns DataFrames into df_log_returns, aligning on date index.\n",
    "df_log_returns = df_log_returns.join(df_future_returns.add_suffix('_future'), how='inner')\n",
    "df_log_returns = df_log_returns.join(df_past_returns.add_suffix('_past'), how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df54968",
   "metadata": {},
   "source": [
    "### 1.6 Get sentiment labels\n",
    "\n",
    "Based of future returns:\n",
    "\n",
    "For each timestep:\n",
    "- Top 3 (30%) -> \"appreciation\"\n",
    "- Middle 4 (40%) -> \"unchanged\"\n",
    "- Bottom 3 (30%) -> \"depreciation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87564836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'USD_future_label', 'EUR_future_label', 'JPY_future_label',\n",
       "       'GBP_future_label', 'CAD_future_label', 'AUD_future_label',\n",
       "       'CHF_future_label', 'SEK_future_label', 'NOK_future_label',\n",
       "       'NZD_future_label', 'USD_past_label', 'EUR_past_label',\n",
       "       'JPY_past_label', 'GBP_past_label', 'CAD_past_label', 'AUD_past_label',\n",
       "       'CHF_past_label', 'SEK_past_label', 'NOK_past_label', 'NZD_past_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of currency codes (G10 currencies)\n",
    "currency_codes = ['USD', 'EUR', 'JPY', 'GBP', 'CAD', 'AUD', 'CHF', 'SEK', 'NOK', 'NZD']\n",
    "\n",
    "# Initialize label columns for each currency\n",
    "for currency in currency_codes:\n",
    "    df_log_returns[f'{currency}_label'] = None\n",
    "\n",
    "# For each date (row), rank currencies by their future returns and assign labels\n",
    "for date in df_log_returns.index:\n",
    "    # Get future returns for this date\n",
    "    future_returns = {}\n",
    "    for currency in currency_codes:\n",
    "        value = df_log_returns.loc[date, f'{currency}_future']\n",
    "        if pd.notna(value):\n",
    "            future_returns[currency] = value\n",
    "\n",
    "    # Get past returns for this date\n",
    "    past_returns = {}\n",
    "    for currency in currency_codes:\n",
    "        value = df_log_returns.loc[date, f'{currency}_past']\n",
    "        if pd.notna(value):\n",
    "            past_returns[currency] = value\n",
    "    \n",
    "    # Rank currencies by future returns (highest to lowest)\n",
    "    sorted_currencies_future = sorted(future_returns.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Rank currencies by past returns (highest to lowest)\n",
    "    sorted_currencies_past = sorted(past_returns.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Assign labels based on ranking\n",
    "    # Top 3 (30%) -> \"appreciation\"\n",
    "    # Middle 4 (40%) -> \"unchanged\"\n",
    "    # Bottom 3 (30%) -> \"depreciation\"\n",
    "    for i, (currency, _) in enumerate(sorted_currencies_future):\n",
    "        if i < 3:  # Top 3 (0, 1, 2)\n",
    "            df_log_returns.loc[date, f'{currency}_future_label'] = 'appreciation'\n",
    "        elif i >= 7:  # Bottom 3 (7, 8, 9)\n",
    "            df_log_returns.loc[date, f'{currency}_future_label'] = 'depreciation'\n",
    "        else:  # Middle 4 (3, 4, 5, 6)\n",
    "            df_log_returns.loc[date, f'{currency}_future_label'] = 'unchanged'\n",
    "    \n",
    "    for i, (currency, _) in enumerate(sorted_currencies_past):\n",
    "        if i < 3:  # Top 3 (0, 1, 2)\n",
    "            df_log_returns.loc[date, f'{currency}_past_label'] = 'depreciation'\n",
    "        elif i >= 7:  # Bottom 3 (7, 8, 9)\n",
    "            df_log_returns.loc[date, f'{currency}_past_label'] = 'appreciation'\n",
    "        else:  # Middle 4 (3, 4, 5, 6)\n",
    "            df_log_returns.loc[date, f'{currency}_past_label'] = 'unchanged'\n",
    "\n",
    "# Only keep labels\n",
    "df_labels = df_log_returns[\n",
    "    [f'{currency}_future_label' for currency in currency_codes] + \n",
    "    [f'{currency}_past_label' for currency in currency_codes]\n",
    "]\n",
    "\n",
    "# Convert to New York timezone\n",
    "df_labels.index = df_labels.index.tz_localize('UTC').tz_convert('America/New_York')\n",
    "\n",
    "# Get rid of time info\n",
    "df_labels.index = df_labels.index.normalize()\n",
    "\n",
    "# Have date as a column\n",
    "df_labels = df_labels.reset_index()\n",
    "\n",
    "df_labels.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6da9ca",
   "metadata": {},
   "source": [
    "### 1.7 Assign labels to news articles\n",
    "- df_labels - labels for sentiment for each valid day\n",
    "- df_news_future and past - mapping between all real dates and valid dates based on the way we backfill\n",
    "- df_news - now will contain these new sentiment labels joined using the dates from the news_future/past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb1b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps news date to future trading date\n",
    "df_news_future = pd.merge_asof(\n",
    "    df_news[['Date']],\n",
    "    df_labels[['date']],\n",
    "    left_on='Date',\n",
    "    right_on='date',\n",
    "    direction='forward'\n",
    ").rename(columns={'Date': 'news_date', 'date': 'trading_date_future'})\n",
    "\n",
    "\n",
    "# maps news date to past trading date\n",
    "df_news_past = pd.merge_asof(\n",
    "    df_news[['Date']],\n",
    "    df_labels[['date']],\n",
    "    left_on='Date',\n",
    "    right_on='date',\n",
    "    direction='backward'\n",
    ").rename(columns={'Date': 'news_date', 'date': 'trading_date_past'})\n",
    "\n",
    "# Perform a concat of df_news and df_news_future and df_news_past\n",
    "# Example: Only concat 'news_date' from df_news_future, and 'trading_date_past' from df_news_past\n",
    "df_news = pd.concat([\n",
    "    df_news,\n",
    "    df_news_future[['trading_date_future']],\n",
    "    df_news_past[['trading_date_past']]\n",
    "], axis=1)\n",
    "\n",
    "\n",
    "# Merge the future labels into the news dataframe\n",
    "future_cols = ['date'] + [col for col in df_labels.columns if col.endswith('_future_label')]\n",
    "df_labels_future = df_labels[future_cols]\n",
    "\n",
    "df_news = df_news.merge(df_labels_future, left_on='trading_date_future', right_on='date', how='left')\n",
    "df_news = df_news.drop(columns=['date'])  # Drop the date column as we don't need it\n",
    "\n",
    "# Merge the past labels into the news dataframe\n",
    "past_cols = ['date'] + [col for col in df_labels.columns if col.endswith('_past_label')]\n",
    "df_labels_past = df_labels[past_cols]\n",
    "\n",
    "df_news = df_news.merge(df_labels_past, left_on='trading_date_past', right_on='date', how='left')\n",
    "df_news = df_news.drop(columns=['date'])  # Drop the date column as we don't need it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5091e1d",
   "metadata": {},
   "source": [
    "### 1.8 Cleaning final DataFrame\n",
    "- drop rows with nulls\n",
    "- Removing unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07681d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Full Text</th>\n",
       "      <th>mentioned_currencies</th>\n",
       "      <th>USD_future_label</th>\n",
       "      <th>EUR_future_label</th>\n",
       "      <th>JPY_future_label</th>\n",
       "      <th>GBP_future_label</th>\n",
       "      <th>CAD_future_label</th>\n",
       "      <th>AUD_future_label</th>\n",
       "      <th>CHF_future_label</th>\n",
       "      <th>...</th>\n",
       "      <th>USD_past_label</th>\n",
       "      <th>EUR_past_label</th>\n",
       "      <th>JPY_past_label</th>\n",
       "      <th>GBP_past_label</th>\n",
       "      <th>CAD_past_label</th>\n",
       "      <th>AUD_past_label</th>\n",
       "      <th>CHF_past_label</th>\n",
       "      <th>SEK_past_label</th>\n",
       "      <th>NOK_past_label</th>\n",
       "      <th>NZD_past_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EUR/CHF Trades Near All-Time Low</td>\n",
       "      <td>Sha...</td>\n",
       "      <td>[EUR, CHF]</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>...</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Higher Probability Trading</td>\n",
       "      <td>Sha...</td>\n",
       "      <td>[USD, CHF]</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>...</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Higher Probability Entries</td>\n",
       "      <td>Sha...</td>\n",
       "      <td>[USD, CAD]</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>...</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EUR/AUD Bounces Up Off of the Lows</td>\n",
       "      <td>Sha...</td>\n",
       "      <td>[EUR, AUD]</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>...</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EUR/CHF Finds Support</td>\n",
       "      <td>Sha...</td>\n",
       "      <td>[EUR, CHF]</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>...</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47622</th>\n",
       "      <td>Japanese Yen Sentiment Analysis &amp; Outlook – US...</td>\n",
       "      <td>Sha...</td>\n",
       "      <td>[EUR, USD, JPY, GBP]</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>...</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47623</th>\n",
       "      <td>US Dollar Technical Outlook: EUR/USD, GBP/USD,...</td>\n",
       "      <td>Sha...</td>\n",
       "      <td>[EUR, USD, GBP, NZD]</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>...</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47624</th>\n",
       "      <td>Gold Prices Sink, Support Breakdown Heralds Mo...</td>\n",
       "      <td>Sha...</td>\n",
       "      <td>[EUR, USD, JPY, GBP]</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>...</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47625</th>\n",
       "      <td>Gold Price Outlook: XAU/USD Turns on Hawkish F...</td>\n",
       "      <td>Sha...</td>\n",
       "      <td>[USD]</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>...</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47626</th>\n",
       "      <td>Dow Sustains Losses but Nasdaq 100 Hits New Hi...</td>\n",
       "      <td>Sha...</td>\n",
       "      <td>[USD, AUD]</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>...</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>depreciation</td>\n",
       "      <td>depreciation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47624 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "3                       EUR/CHF Trades Near All-Time Low   \n",
       "4                             Higher Probability Trading   \n",
       "5                             Higher Probability Entries   \n",
       "6                     EUR/AUD Bounces Up Off of the Lows   \n",
       "7                                  EUR/CHF Finds Support   \n",
       "...                                                  ...   \n",
       "47622  Japanese Yen Sentiment Analysis & Outlook – US...   \n",
       "47623  US Dollar Technical Outlook: EUR/USD, GBP/USD,...   \n",
       "47624  Gold Prices Sink, Support Breakdown Heralds Mo...   \n",
       "47625  Gold Price Outlook: XAU/USD Turns on Hawkish F...   \n",
       "47626  Dow Sustains Losses but Nasdaq 100 Hits New Hi...   \n",
       "\n",
       "                                               Full Text  \\\n",
       "3                                                 Sha...   \n",
       "4                                                 Sha...   \n",
       "5                                                 Sha...   \n",
       "6                                                 Sha...   \n",
       "7                                                 Sha...   \n",
       "...                                                  ...   \n",
       "47622                                             Sha...   \n",
       "47623                                             Sha...   \n",
       "47624                                             Sha...   \n",
       "47625                                             Sha...   \n",
       "47626                                             Sha...   \n",
       "\n",
       "       mentioned_currencies USD_future_label EUR_future_label  \\\n",
       "3                [EUR, CHF]     depreciation     appreciation   \n",
       "4                [USD, CHF]     depreciation     appreciation   \n",
       "5                [USD, CAD]     depreciation     appreciation   \n",
       "6                [EUR, AUD]     depreciation     appreciation   \n",
       "7                [EUR, CHF]        unchanged     appreciation   \n",
       "...                     ...              ...              ...   \n",
       "47622  [EUR, USD, JPY, GBP]     depreciation     depreciation   \n",
       "47623  [EUR, USD, GBP, NZD]     depreciation     depreciation   \n",
       "47624  [EUR, USD, JPY, GBP]     depreciation     depreciation   \n",
       "47625                 [USD]     depreciation     depreciation   \n",
       "47626            [USD, AUD]     depreciation     depreciation   \n",
       "\n",
       "      JPY_future_label GBP_future_label CAD_future_label AUD_future_label  \\\n",
       "3         depreciation     appreciation        unchanged        unchanged   \n",
       "4         depreciation     appreciation        unchanged        unchanged   \n",
       "5         depreciation     appreciation        unchanged        unchanged   \n",
       "6         appreciation        unchanged     depreciation        unchanged   \n",
       "7         appreciation        unchanged     depreciation     depreciation   \n",
       "...                ...              ...              ...              ...   \n",
       "47622        unchanged     depreciation        unchanged        unchanged   \n",
       "47623        unchanged     depreciation        unchanged        unchanged   \n",
       "47624        unchanged     depreciation        unchanged        unchanged   \n",
       "47625        unchanged     depreciation        unchanged        unchanged   \n",
       "47626        unchanged     depreciation        unchanged        unchanged   \n",
       "\n",
       "      CHF_future_label  ... USD_past_label EUR_past_label JPY_past_label  \\\n",
       "3         depreciation  ...   depreciation   appreciation      unchanged   \n",
       "4         depreciation  ...   depreciation   appreciation      unchanged   \n",
       "5         depreciation  ...   depreciation   appreciation      unchanged   \n",
       "6         depreciation  ...   depreciation   appreciation      unchanged   \n",
       "7         appreciation  ...   depreciation   appreciation   appreciation   \n",
       "...                ...  ...            ...            ...            ...   \n",
       "47622     appreciation  ...      unchanged      unchanged   appreciation   \n",
       "47623     appreciation  ...      unchanged      unchanged   appreciation   \n",
       "47624     appreciation  ...      unchanged      unchanged   appreciation   \n",
       "47625     appreciation  ...      unchanged      unchanged   appreciation   \n",
       "47626     appreciation  ...      unchanged      unchanged   appreciation   \n",
       "\n",
       "      GBP_past_label CAD_past_label AUD_past_label CHF_past_label  \\\n",
       "3       depreciation   depreciation   appreciation   appreciation   \n",
       "4       depreciation   depreciation   appreciation   appreciation   \n",
       "5       depreciation   depreciation   appreciation   appreciation   \n",
       "6       depreciation   depreciation   appreciation   appreciation   \n",
       "7       depreciation   depreciation      unchanged   appreciation   \n",
       "...              ...            ...            ...            ...   \n",
       "47622   depreciation   appreciation      unchanged   appreciation   \n",
       "47623   depreciation   appreciation      unchanged   appreciation   \n",
       "47624   depreciation   appreciation      unchanged   appreciation   \n",
       "47625   depreciation   appreciation      unchanged   appreciation   \n",
       "47626   depreciation   appreciation      unchanged   appreciation   \n",
       "\n",
       "      SEK_past_label NOK_past_label NZD_past_label  \n",
       "3          unchanged      unchanged      unchanged  \n",
       "4          unchanged      unchanged      unchanged  \n",
       "5          unchanged      unchanged      unchanged  \n",
       "6          unchanged      unchanged      unchanged  \n",
       "7          unchanged      unchanged      unchanged  \n",
       "...              ...            ...            ...  \n",
       "47622      unchanged   depreciation   depreciation  \n",
       "47623      unchanged   depreciation   depreciation  \n",
       "47624      unchanged   depreciation   depreciation  \n",
       "47625      unchanged   depreciation   depreciation  \n",
       "47626      unchanged   depreciation   depreciation  \n",
       "\n",
       "[47624 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only keep title, full text and all labels\n",
    "df_news = df_news[['Title', 'Full Text', 'mentioned_currencies', *future_cols[1:], *past_cols[1:]]]\n",
    "\n",
    "df_news = df_news.dropna()\n",
    "\n",
    "df_news.to_pickle(\"df_news.pkl\")\n",
    "\n",
    "df_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c37e09",
   "metadata": {},
   "source": [
    "### 1.9 Train Test Split\n",
    "- 200 examples for final eval\n",
    "- Otherwise 80/20 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c776a646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set:  2\n",
      "Size of test set:  2\n",
      "Size of eval set:  200\n"
     ]
    }
   ],
   "source": [
    "df_news = pd.read_pickle(\"df_news.pkl\")\n",
    "\n",
    "df_news_before_2020 = df_news[df_news['Date'] < pd.to_datetime('2020-01-01', utc=True)]     # we train the model on this for now\n",
    "# df_news_after_2020 = df_news[df_news['Date'] >= pd.to_datetime('2020-01-01', utc=True)]   # we use this for the trading strategy\n",
    "\n",
    "df_rest, df_eval = train_test_split(df_news_before_2020, test_size=200, random_state=42)\n",
    "df_train, df_test = train_test_split(df_rest, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Size of train set: \", len(df_train))\n",
    "print(\"Size of test set: \", len(df_test))\n",
    "print(\"Size of eval set: \", len(df_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84da036",
   "metadata": {},
   "source": [
    "## 2 Generate Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3230b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(row):\n",
    "    title = row.get('Title', '')\n",
    "    text = row.get('Full Text', '')\n",
    "    currencies = row.get('mentioned_currencies')\n",
    "\n",
    "    target_currencies = ''\n",
    "    for c in currencies:\n",
    "        target_currencies += f'{c}_past: \"appreciation, depreciation, or unchanged\",\\n'\n",
    "        target_currencies += f'{c}_future: \"appreciation, depreciation, or unchanged\",\\n'\n",
    "    target_currencies = target_currencies.strip().rstrip(\",\") # Remove last comma\n",
    "\n",
    "    # Same structure as per paper\n",
    "    return (\n",
    "        f\"Title: {title}\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"Objective: For each mentioned currency, answer the following questions:\\n\"\n",
    "        \"- What has been the current/past movement of the currency (appreciation, depreciation, or unchanged)?\\n\"\n",
    "        \"- What is the future expectation for the currency (appreciation, depreciation, or unchanged)?\\n\\n\"\n",
    "        \"You must answer these two questions for each of the following currencies mentioned in the article:\\n\"\n",
    "        f\"{target_currencies}\\n\\n\"\n",
    "        \"Output Format:\\n\"\n",
    "        \"- Important: Provide your answer in separate rows for each currency as shown above.\\n\"\n",
    "        \"- Do not combine multiple currencies in the same row.\\n\"\n",
    "        '- Each currency should have its own line with \"_past\" or \"_future\" specified.\\n\\n'\n",
    "        \"Example:\\n\"\n",
    "        '- If the article states, \"The EUR is expected to appreciate,\" the output should be:\\n'\n",
    "        '    EUR_past: \"unchanged\",\\n'\n",
    "        '    EUR_future: \"appreciation\"\\n'\n",
    "        '- If the article states, \"EUR/USD depreciated last week,\" the output should be:\\n'\n",
    "        '    EUR_past: \"depreciation\",\\n'\n",
    "        '    USD_past: \"appreciation\"\\n'\n",
    "        '- If only future movements are mentioned for a currency, the past movement should be labelled as \"unchanged\" and vice versa.\\n\\n'\n",
    "        \"Currency Pair Interpretation:\\n\"\n",
    "        \"- If currencies are discussed in pairs, interpret as follows:\\n\"\n",
    "        '    - If \"EUR/USD appreciated,\" label EUR_past as \"appreciation\" and USD_past as \"depreciation\".\\n'\n",
    "        '    - If \"EUR/USD depreciated,\" label EUR_past as \"depreciation\" and USD_past as \"appreciation\".\\n\\n'\n",
    "        \"Synonyms:\\n\"\n",
    "        \"- Recognize the following synonyms for each currency:\\n\"\n",
    "        \"- **EUR**: EUR, Euro\\n\"\n",
    "        \"- **USD**: USD, Dollar, Dollars, US Dollar, US-Dollar, U.S. Dollar, US Dollars, US-Dollars, U.S. Dollars, Greenback\\n\"\n",
    "        \"- **JPY**: JPY, Yen, Japanese Yen\\n\"\n",
    "        \"- **GBP**: GBP, Pound, Pounds, Sterling, British Pound, British Pounds\\n\"\n",
    "        \"- **AUD**: AUD, Australian Dollar, Australian Dollars, Aussie\\n\"\n",
    "        \"- **CAD**: CAD, Canadian Dollar, Canadian Dollars\\n\"\n",
    "        \"- **CHF**: CHF, Swiss Franc, Swiss Francs, Swissie\\n\"\n",
    "        \"- **NZD**: NZD, New Zealand Dollar, New Zealand Dollars, Kiwi\\n\"\n",
    "        \"- **NOK**: NOK, Norwegian Krone, Norwegian Kroner\\n\"\n",
    "        \"- **SEK**: SEK, Swedish Krona, Swedish Kronor\\n\"\n",
    "        \"Answer below in the given format:\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120af922",
   "metadata": {},
   "source": [
    "## 3 Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59412ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model explicitly loaded onto: cuda:0\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "# 'meta-llama/Llama-3.1-8B-Instruct' - real\n",
    "# \"meta-llama/Llama-3.2-1B-Instruct\" - for local testing as smallest possible model\n",
    "model_id =  \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# quntisation config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,      # \"Double Quantization\"\n",
    "    bnb_4bit_quant_type=\"nf4\",           # 4-bit NormalFloat data type\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Compute in bfloat16 for stability\n",
    ")\n",
    "\n",
    "# load tokeniser\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.model_max_length = 8192\n",
    "tokenizer.pad_token = tokenizer.eos_token # Llama has no default pad token\n",
    "tokenizer.padding_side = \"right\"  # TODO Check this\n",
    "\n",
    "# load model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\", # Automatically puts model on GPU\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model = model.to(device)\n",
    "    print(f\"Model explicitly loaded onto: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    print(\"CUDA not available. Model loaded onto CPU.\")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Prepare for training \n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config \n",
    "# Params from Table A.1\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",           # TODO Check this\n",
    "    task_type=\"CAUSAL_LM\", # TODO Check this\n",
    "    \n",
    "    # inject low-rank adaptation matrices into all linear layers TODO check this\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # Attention projections\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"     # MLP projections\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee7cd6",
   "metadata": {},
   "source": [
    "## 4 LLM Fine Tuning\n",
    "- Stopping criterion is used\n",
    "    - Optimisises for least loss in the validation stage rather than most traning epochs\n",
    "    - So if the model with best validation loss is in epoch 1 or 2, then the weights in epoch 3 will be discarded\n",
    "    - Used to prevent overfitting due to this being a small dataset\n",
    "    - Stops traning if the validation loss stagnates due to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3680190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 2/2 [00:00<00:00, 26.50 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 2/2 [00:00<00:00, 119.03 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 2/2 [00:00<00:00, 11.37 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 2/2 [00:00<00:00, 92.35 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 2/2 [00:00<00:00, 130.05 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 2/2 [00:00<00:00, 155.58 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 2/2 [00:00<00:00, 58.27 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 2/2 [00:00<00:00, 243.13 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n",
      "/mnt/c/Users/Amogh/OneDrive - University of Cambridge/Programming-New/Part-II-Project/venv_wsl/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,      # TODO Check this\n",
    "    per_device_eval_batch_size=1,       # TODO Check this\n",
    "    gradient_accumulation_steps=32, \n",
    "    optim=\"paged_adamw_32bit\",          # \n",
    "    save_steps=50,                      # TODO get better number\n",
    "    learning_rate=1e-5,                 #  Note: significantly lower than standard\n",
    "    weight_decay=0.1,                   #  High weight decay\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,                  # TODO apparenly this is the best for lora??? - not said in the paper\n",
    "    warmup_ratio=0.0,                   # \n",
    "    lr_scheduler_type=\"cosine\",         #                  \n",
    "    save_strategy=\"steps\",              # for early stopping   (could be epoch)\n",
    "    eval_strategy=\"steps\",              # for early stopping   (could be epoch)\n",
    "    load_best_model_at_end=True,         # for early stopping\n",
    "    metric_for_best_model=\"eval_loss\",   # for early stopping\n",
    "    greater_is_better=False,     # less loss is better\n",
    "    logging_steps=10,                   # TODO get a better number\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\"                    # Disable wandb unless needed\n",
    ")\n",
    "\n",
    "\n",
    "df_train = Dataset.from_pandas(df_train)\n",
    "df_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=df_train, # Ensure this is loaded\n",
    "    eval_dataset=df_test,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=generate_prompt,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    # packing=False,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)] # to stop after epoch 1 if validaiton loss gets worse\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained('llama_small_finetuned')\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fc2f3",
   "metadata": {},
   "source": [
    "## 5 Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121dc035",
   "metadata": {},
   "source": [
    "### 5.1 Predict sentiment\n",
    "- Gets the sentiment for a single article\n",
    "- Used for evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a6048aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(row):\n",
    "    prompt = generate_prompt(row)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,  # to avoid crashing model due to very large article\n",
    "        max_length=8192\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=512,     # only needs to generate enough for sentiment\n",
    "            temperature=0.1,        # incase there was sampling\n",
    "            do_sample=False,        # no sampling - so no randomness\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "    response = response[len(prompt):].strip()    # skips over prompt\n",
    "\n",
    "    # Parse response to get labels into a dict\n",
    "    sentiment = {}\n",
    "    for line in response.split('\\n'):\n",
    "        if line.strip():\n",
    "            currency, label = line.split(':')\n",
    "            currency = currency.strip()\n",
    "            label = label.strip()\n",
    "            sentiment[currency] = label\n",
    "\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aee84e",
   "metadata": {},
   "source": [
    "### 5.2 Get evaulation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc2bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 1.0, 'f1': 1.0, 'precision_per_class': {'appreciation': np.float64(1.0), 'depreciation': np.float64(1.0), 'unchanged': np.float64(1.0)}, 'recall_per_class': {'appreciation': np.float64(1.0), 'depreciation': np.float64(1.0), 'unchanged': np.float64(1.0)}, 'f1_per_class': {'appreciation': np.float64(1.0), 'depreciation': np.float64(1.0), 'unchanged': np.float64(1.0)}, 'support_per_class': {'appreciation': np.int64(60), 'depreciation': np.int64(60), 'unchanged': np.int64(80)}}\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "appreciation       1.00      1.00      1.00        60\n",
      "depreciation       1.00      1.00      1.00        60\n",
      "   unchanged       1.00      1.00      1.00        80\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "currency_codes = ['EUR', 'USD', 'GBP', 'JPY', 'AUD', 'CAD', 'CHF', 'NZD', 'NOK', 'SEK']\n",
    "\n",
    "all_actual = []\n",
    "all_predictions = []\n",
    "\n",
    "for i, row in df_eval.iterrows():\n",
    "    sentiment = get_sentiment(row)\n",
    "    for c in currency_codes:\n",
    "        for t in ['past', 'future']:\n",
    "            all_actual.append(row[f'{c}_{t}_label'])\n",
    "            all_predictions.append(sentiment.get(f'{c}_{t}', 'unchanged'))\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy = accuracy_score(all_actual, all_predictions)\n",
    "f1 = f1_score(all_actual, all_predictions, average='macro')\n",
    "precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(all_actual, all_predictions, labels=['appreciation', 'depreciation', 'unchanged'])\n",
    "\n",
    "stats = {\n",
    "    'accuracy': accuracy,\n",
    "    'f1': f1,\n",
    "    'precision_per_class': dict(zip(['appreciation', 'depreciation', 'unchanged'], precision_per_class)),\n",
    "    'recall_per_class': dict(zip(['appreciation', 'depreciation', 'unchanged'], recall_per_class)),\n",
    "    'f1_per_class': dict(zip(['appreciation', 'depreciation', 'unchanged'], f1_per_class)),\n",
    "    'support_per_class': dict(zip(['appreciation', 'depreciation', 'unchanged'], support_per_class))\n",
    "}\n",
    "\n",
    "report = classification_report(all_actual, all_predictions)\n",
    "\n",
    "print(stats)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
