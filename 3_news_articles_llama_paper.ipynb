{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3be8afe",
   "metadata": {},
   "source": [
    "# Implementing \"FX sentiment analysis with large language models\" (Ballinari et al.)\n",
    "This paper can be found at "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a21868",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423e5cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import re\n",
    "import pandas_datareader.data as web\n",
    "# import transformers\n",
    "# import bitsandbytes as bnb\n",
    "# import accelerate\n",
    "# import peft\n",
    "# import trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb648241",
   "metadata": {},
   "source": [
    "## 1. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccdfb1",
   "metadata": {},
   "source": [
    "### 1.1. Filtering\n",
    "- Load the datasets\n",
    "- Drop articles with <20 words\n",
    "- Remove duplicate articles \n",
    "- Convert time to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39896162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amogh\\AppData\\Local\\Temp\\ipykernel_94028\\3918084552.py:12: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_news['Date'] = pd.to_datetime(df_news['Date'])\n"
     ]
    }
   ],
   "source": [
    "# Loading the dailyfx news articles dataset\n",
    "df_news = pd.read_csv('datasets/news articles/dailyfx_articles_012011-062024.csv')\n",
    "\n",
    "# Drop articles with <20 words\n",
    "df_news = df_news[df_news['Full Text'].str.split().str.len() > 20]\n",
    "\n",
    "# Remove duplicate articles\n",
    "df_news = df_news.drop_duplicates(subset=['Full Text'])\n",
    "df_news = df_news.drop_duplicates(subset=['Title'])\n",
    "\n",
    "# Convert time to datetime\n",
    "df_news['Date'] = pd.to_datetime(df_news['Date'])\n",
    "\n",
    "# Sort by date\n",
    "df_news = df_news.sort_values(by='Date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea60f85",
   "metadata": {},
   "source": [
    "### 1.2. Creating mentioned_currency column\n",
    "- Use regex to capture all the currencies used in an article\n",
    "- Make use of common synomyms\n",
    "- Filter articles that don't mention any of the G10 currencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e119ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping ISO codes to the regex patterns (synonyms) from Figure A.1\n",
    "currency_synonyms = {\n",
    "    \"EUR\": [r\"EUR\", r\"Euro\"],\n",
    "    \"USD\": [r\"USD\", r\"Dollar\", r\"Dollars\", r\"US Dollar\", r\"US-Dollar\", r\"U\\.S\\. Dollar\", \n",
    "            r\"US Dollars\", r\"US-Dollars\", r\"U\\.S\\. Dollars\", r\"Greenback\"],\n",
    "    \"JPY\": [r\"JPY\", r\"Yen\", r\"Japanese Yen\"],\n",
    "    \"GBP\": [r\"GBP\", r\"Pound\", r\"Pounds\", r\"Sterling\", r\"British Pound\", r\"British Pounds\"],\n",
    "    \"AUD\": [r\"AUD\", r\"Australian Dollar\", r\"Australian Dollars\", r\"Aussie\"],\n",
    "    \"CAD\": [r\"CAD\", r\"Canadian Dollar\", r\"Canadian Dollars\"],\n",
    "    \"CHF\": [r\"CHF\", r\"Swiss Franc\", r\"Swiss Francs\", r\"Swissie\"],\n",
    "    \"NZD\": [r\"NZD\", r\"New Zealand Dollar\", r\"New Zealand Dollars\", r\"Kiwi\"],\n",
    "    \"NOK\": [r\"NOK\", r\"Norwegian Krone\", r\"Norwegian Kroner\"],\n",
    "    \"SEK\": [r\"SEK\", r\"Swedish Krona\", r\"Swedish Kronor\"]\n",
    "}\n",
    "\n",
    "# Get list of mentioned currencies from text\n",
    "def get_mentioned_currencies(text):\n",
    "    mentioned_currencies = list()\n",
    "\n",
    "    for currency, patterns in currency_synonyms.items():\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                mentioned_currencies.append(currency)\n",
    "                break\n",
    "\n",
    "    return mentioned_currencies\n",
    "\n",
    "df_news['mentioned_currencies'] = df_news['Full Text'].apply(get_mentioned_currencies)\n",
    "\n",
    "# Filter articles to keep only those where 'mentioned_currencies' is non empty\n",
    "df_news = df_news[df_news['mentioned_currencies'].apply(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de90cbd",
   "metadata": {},
   "source": [
    "### 1.3. Getting historical prices\n",
    "Using nominal narrow effective exchange rate (daily) for each country.\n",
    "\n",
    "Narrow effective exchange rate is a good proxy for the tradable currency index that the authors used.\n",
    "\n",
    "Allows us to put a number to the currency rather than using a pair as then the currency can be effected by the other in the pair. \n",
    "\n",
    "It is done by taking the geometric mean from the exchange rate of various other currencies (narrow means only a small number of industrialised countries so that the average isn't skewed by some other non industrialised country going down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ec17aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USD</th>\n",
       "      <th>EUR</th>\n",
       "      <th>JPY</th>\n",
       "      <th>GBP</th>\n",
       "      <th>CAD</th>\n",
       "      <th>AUD</th>\n",
       "      <th>CHF</th>\n",
       "      <th>SEK</th>\n",
       "      <th>NOK</th>\n",
       "      <th>NZD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-03</th>\n",
       "      <td>82.07</td>\n",
       "      <td>103.41</td>\n",
       "      <td>120.99</td>\n",
       "      <td>105.23</td>\n",
       "      <td>129.89</td>\n",
       "      <td>130.38</td>\n",
       "      <td>87.30</td>\n",
       "      <td>116.49</td>\n",
       "      <td>136.00</td>\n",
       "      <td>98.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>82.05</td>\n",
       "      <td>103.98</td>\n",
       "      <td>119.81</td>\n",
       "      <td>105.84</td>\n",
       "      <td>129.59</td>\n",
       "      <td>128.97</td>\n",
       "      <td>86.05</td>\n",
       "      <td>116.47</td>\n",
       "      <td>135.74</td>\n",
       "      <td>97.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>82.77</td>\n",
       "      <td>102.87</td>\n",
       "      <td>120.35</td>\n",
       "      <td>106.70</td>\n",
       "      <td>128.98</td>\n",
       "      <td>128.61</td>\n",
       "      <td>86.01</td>\n",
       "      <td>116.37</td>\n",
       "      <td>135.51</td>\n",
       "      <td>97.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-06</th>\n",
       "      <td>83.09</td>\n",
       "      <td>102.38</td>\n",
       "      <td>119.32</td>\n",
       "      <td>106.95</td>\n",
       "      <td>130.06</td>\n",
       "      <td>128.83</td>\n",
       "      <td>85.40</td>\n",
       "      <td>116.25</td>\n",
       "      <td>135.96</td>\n",
       "      <td>97.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-07</th>\n",
       "      <td>83.46</td>\n",
       "      <td>101.62</td>\n",
       "      <td>119.31</td>\n",
       "      <td>107.42</td>\n",
       "      <td>130.42</td>\n",
       "      <td>128.88</td>\n",
       "      <td>86.29</td>\n",
       "      <td>115.74</td>\n",
       "      <td>136.06</td>\n",
       "      <td>97.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              USD     EUR     JPY     GBP     CAD     AUD    CHF     SEK  \\\n",
       "date                                                                       \n",
       "2011-01-03  82.07  103.41  120.99  105.23  129.89  130.38  87.30  116.49   \n",
       "2011-01-04  82.05  103.98  119.81  105.84  129.59  128.97  86.05  116.47   \n",
       "2011-01-05  82.77  102.87  120.35  106.70  128.98  128.61  86.01  116.37   \n",
       "2011-01-06  83.09  102.38  119.32  106.95  130.06  128.83  85.40  116.25   \n",
       "2011-01-07  83.46  101.62  119.31  107.42  130.42  128.88  86.29  115.74   \n",
       "\n",
       "               NOK    NZD  \n",
       "date                       \n",
       "2011-01-03  136.00  98.33  \n",
       "2011-01-04  135.74  97.42  \n",
       "2011-01-05  135.51  97.75  \n",
       "2011-01-06  135.96  97.37  \n",
       "2011-01-07  136.06  97.72  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All links to get data from for effective exchage rate\n",
    "urls = {\n",
    "    \"USD\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.US?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"EUR\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.XM?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"JPY\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.JP?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"GBP\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.GB?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"CAD\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.CA?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"AUD\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.AU?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"CHF\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.CH?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\", \n",
    "    \"SEK\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.SE?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"NOK\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.NO?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\",\n",
    "    \"NZD\": \"https://stats.bis.org/api/v2/data/dataflow/BIS/WS_EER/1.0/D.N.N.NZ?startPeriod=2011-01-01&endPeriod=2024-06-01&format=csv\"\n",
    "}\n",
    "\n",
    "# Initialise an empty DataFrame (EER = effective exchange rate)\n",
    "df_EER = pd.DataFrame()\n",
    "\n",
    "for code, url in urls.items():\n",
    "    # Read only the required columns from the CSV\n",
    "    df_temp = pd.read_csv(url, usecols=lambda c: c in [\"TIME_PERIOD\", \"OBS_VALUE\"])\n",
    "    \n",
    "    # Convert OBS_VALUE to float for log calculations later\n",
    "    df_temp[\"OBS_VALUE\"] = pd.to_numeric(df_temp[\"OBS_VALUE\"], errors=\"coerce\")\n",
    "    \n",
    "    # Rename \"OBS_VALUE\" to currency code\n",
    "    df_temp = df_temp.rename(columns={\n",
    "        \"OBS_VALUE\": code,\n",
    "        \"TIME_PERIOD\": \"date\"\n",
    "    })\n",
    "    \n",
    "    # If the main df is empty, set it to this df\n",
    "    if df_EER.empty:\n",
    "        df_EER = df_temp\n",
    "    else:\n",
    "        # Join on \"date\", keep all records (outer join)\n",
    "        df_EER = pd.merge(df_EER, df_temp, on=[\"date\"], how='outer')\n",
    "\n",
    "\n",
    "df_EER['date'] = pd.to_datetime(df_EER['date'])\n",
    "df_EER = df_EER.set_index('date')\n",
    "\n",
    "# drop all NaNs in the data\n",
    "df_EER.dropna(inplace=True)\n",
    "\n",
    "df_EER.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee101bb9",
   "metadata": {},
   "source": [
    "### 1.4 Calculate log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2966a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily log returns\n",
    "df_log_returns = np.log(df_EER / df_EER.shift(1))\n",
    "\n",
    "df_log_returns.dropna(inplace=True)  # created in the shifting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31907a01",
   "metadata": {},
   "source": [
    "### 1.5 Calculate cumulative 5 day windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1f87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [USD, EUR, JPY, GBP, CAD, AUD, CHF, SEK, NOK, NZD, USD_future, EUR_future, JPY_future, GBP_future, CAD_future, AUD_future, CHF_future, SEK_future, NOK_future, NZD_future, USD_past, EUR_past, JPY_past, GBP_past, CAD_past, AUD_past, CHF_past, SEK_past, NOK_past, NZD_past]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Future returns\n",
    "# At index t, we want the sum of t+1, t+2, t+3, t+4, t+5 returns\n",
    "df_future_returns = df_log_returns.rolling(window=5, min_periods=5).sum().shift(-5)\n",
    "df_future_returns.dropna(inplace=True)\n",
    "\n",
    "# Past returns\n",
    "# At index t, we want the sum of t-1, t-2, t-3, t-4, t-5 returns\n",
    "df_past_returns = df_log_returns.rolling(window=5, min_periods=5).sum().shift(1)\n",
    "df_past_returns.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Merge future and past returns DataFrames into df_log_returns, aligning on date index.\n",
    "df_log_returns = df_log_returns.join(df_future_returns.add_suffix('_future'), how='inner')\n",
    "df_log_returns = df_log_returns.join(df_past_returns.add_suffix('_past'), how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df54968",
   "metadata": {},
   "source": [
    "### 1.6 Get sentiment labels\n",
    "\n",
    "Based of future returns:\n",
    "\n",
    "For each timestep:\n",
    "- Top 3 (30%) -> \"Appreciation\"\n",
    "- Middle 4 (40%) -> \"Unchanged\"\n",
    "- Bottom 3 (30%) -> \"Depreciation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87564836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls [USD]: log_ret=0, future_win=0, past_win=0\n",
      "Nulls [EUR]: log_ret=0, future_win=0, past_win=0\n",
      "Nulls [JPY]: log_ret=0, future_win=0, past_win=0\n",
      "Nulls [GBP]: log_ret=0, future_win=0, past_win=0\n",
      "Nulls [CAD]: log_ret=0, future_win=0, past_win=0\n",
      "Nulls [AUD]: log_ret=0, future_win=0, past_win=0\n",
      "Nulls [CHF]: log_ret=0, future_win=0, past_win=0\n",
      "Nulls [SEK]: log_ret=0, future_win=0, past_win=0\n",
      "Nulls [NOK]: log_ret=0, future_win=0, past_win=0\n",
      "Nulls [NZD]: log_ret=0, future_win=0, past_win=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['USD_forward_label', 'EUR_forward_label', 'JPY_forward_label',\n",
       "       'GBP_forward_label', 'CAD_forward_label', 'AUD_forward_label',\n",
       "       'CHF_forward_label', 'SEK_forward_label', 'NOK_forward_label',\n",
       "       'NZD_forward_label', 'USD_backward_label', 'EUR_backward_label',\n",
       "       'JPY_backward_label', 'GBP_backward_label', 'CAD_backward_label',\n",
       "       'AUD_backward_label', 'CHF_backward_label', 'SEK_backward_label',\n",
       "       'NOK_backward_label', 'NZD_backward_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of currency codes (G10 currencies)\n",
    "currency_codes = ['USD', 'EUR', 'JPY', 'GBP', 'CAD', 'AUD', 'CHF', 'SEK', 'NOK', 'NZD']\n",
    "\n",
    "# Initialize label columns for each currency\n",
    "for currency in currency_codes:\n",
    "    df_log_returns[f'{currency}_label'] = None\n",
    "\n",
    "# For each date (row), rank currencies by their future returns and assign labels\n",
    "for date in df_log_returns.index:\n",
    "    # Get future returns for this date\n",
    "    future_returns = {}\n",
    "    for currency in currency_codes:\n",
    "        value = df_log_returns.loc[date, f'{currency}_future']\n",
    "        if pd.notna(value):\n",
    "            future_returns[currency] = value\n",
    "\n",
    "    # Get past returns for this date\n",
    "    past_returns = {}\n",
    "    for currency in currency_codes:\n",
    "        value = df_log_returns.loc[date, f'{currency}_past']\n",
    "        if pd.notna(value):\n",
    "            past_returns[currency] = value\n",
    "    \n",
    "    # Rank currencies by future returns (highest to lowest)\n",
    "    sorted_currencies_future = sorted(future_returns.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Rank currencies by past returns (highest to lowest)\n",
    "    sorted_currencies_past = sorted(past_returns.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Assign labels based on ranking\n",
    "    # Top 3 (30%) -> \"Appreciation\"\n",
    "    # Middle 4 (40%) -> \"Unchanged\"\n",
    "    # Bottom 3 (30%) -> \"Depreciation\"\n",
    "    for i, (currency, _) in enumerate(sorted_currencies_future):\n",
    "        if i < 3:  # Top 3 (0, 1, 2)\n",
    "            df_log_returns.loc[date, f'{currency}_forward_label'] = 'Appreciation'\n",
    "        elif i >= 7:  # Bottom 3 (7, 8, 9)\n",
    "            df_log_returns.loc[date, f'{currency}_forward_label'] = 'Depreciation'\n",
    "        else:  # Middle 4 (3, 4, 5, 6)\n",
    "            df_log_returns.loc[date, f'{currency}_forward_label'] = 'Unchanged'\n",
    "    \n",
    "    for i, (currency, _) in enumerate(sorted_currencies_past):\n",
    "        if i < 3:  # Top 3 (0, 1, 2)\n",
    "            df_log_returns.loc[date, f'{currency}_backward_label'] = 'Depreciation'\n",
    "        elif i >= 7:  # Bottom 3 (7, 8, 9)\n",
    "            df_log_returns.loc[date, f'{currency}_backward_label'] = 'Appreciation'\n",
    "        else:  # Middle 4 (3, 4, 5, 6)\n",
    "            df_log_returns.loc[date, f'{currency}_backward_label'] = 'Unchanged'\n",
    "\n",
    "# Only keep labels\n",
    "df_labels = df_log_returns[\n",
    "    [f'{currency}_forward_label' for currency in currency_codes] + \n",
    "    [f'{currency}_backward_label' for currency in currency_codes]\n",
    "]\n",
    "\n",
    "df_labels.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6da9ca",
   "metadata": {},
   "source": [
    "### 1.7 Assign labels to news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb1b7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Records in df_labels where USD_forward_label is null:\n",
      "Empty DataFrame\n",
      "Columns: [USD_forward_label, EUR_forward_label, JPY_forward_label, GBP_forward_label, CAD_forward_label, AUD_forward_label, CHF_forward_label, SEK_forward_label, NOK_forward_label, NZD_forward_label, USD_backward_label, EUR_backward_label, JPY_backward_label, GBP_backward_label, CAD_backward_label, AUD_backward_label, CHF_backward_label, SEK_backward_label, NOK_backward_label, NZD_backward_label]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "MergeError",
     "evalue": "Incompatible merge dtype, dtype('<M8[ns]') and dtype('O'), both sides must have numeric dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMergeError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     63\u001b[39m df_news_temp = df_news[[\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m]].copy()\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# For FUTURE labels: Use next available trading day if article is on non-trading day\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# direction='forward' means: for each article date, find the next trading day\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m df_news_future = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_asof\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_news_temp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_trading_dates\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrading_date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrading_date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mforward\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Forward: next available trading day\u001b[39;49;00m\n\u001b[32m     73\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# For PAST labels: Use most recent preceding trading day if article is on non-trading day\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# direction='backward' means: for each article date, find the previous trading day\u001b[39;00m\n\u001b[32m     77\u001b[39m df_news_past = pd.merge_asof(\n\u001b[32m     78\u001b[39m     df_news_temp.sort_values(\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     79\u001b[39m     df_trading_dates.sort_values(\u001b[33m'\u001b[39m\u001b[33mtrading_date\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m     direction=\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Backward: previous trading day\u001b[39;00m\n\u001b[32m     83\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amogh\\OneDrive - University of Cambridge\\Programming-New\\Part-II-Project\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:691\u001b[39m, in \u001b[36mmerge_asof\u001b[39m\u001b[34m(left, right, on, left_on, right_on, left_index, right_index, by, left_by, right_by, suffixes, tolerance, allow_exact_matches, direction)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge_asof\u001b[39m(\n\u001b[32m    441\u001b[39m     left: DataFrame | Series,\n\u001b[32m    442\u001b[39m     right: DataFrame | Series,\n\u001b[32m   (...)\u001b[39m\u001b[32m    454\u001b[39m     direction: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    455\u001b[39m ) -> DataFrame:\n\u001b[32m    456\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[33;03m    Perform a merge by key distance.\u001b[39;00m\n\u001b[32m    458\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    689\u001b[39m \u001b[33;03m    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\u001b[39;00m\n\u001b[32m    690\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m     op = \u001b[43m_AsOfMerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_by\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_by\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_by\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_by\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43masof\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_exact_matches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_exact_matches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amogh\\OneDrive - University of Cambridge\\Programming-New\\Part-II-Project\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2000\u001b[39m, in \u001b[36m_AsOfMerge.__init__\u001b[39m\u001b[34m(self, left, right, on, left_on, right_on, left_index, right_index, by, left_by, right_by, suffixes, how, tolerance, allow_exact_matches, direction)\u001b[39m\n\u001b[32m   1994\u001b[39m     msg = (\n\u001b[32m   1995\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mallow_exact_matches must be boolean, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1996\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpassed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.allow_exact_matches\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1997\u001b[39m     )\n\u001b[32m   1998\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m2000\u001b[39m \u001b[43m_OrderedMerge\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   2001\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2007\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2010\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2011\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfill_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2012\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amogh\\OneDrive - University of Cambridge\\Programming-New\\Part-II-Project\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1912\u001b[39m, in \u001b[36m_OrderedMerge.__init__\u001b[39m\u001b[34m(self, left, right, on, left_on, right_on, left_index, right_index, suffixes, fill_method, how)\u001b[39m\n\u001b[32m   1898\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m   1899\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1900\u001b[39m     left: DataFrame | Series,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1909\u001b[39m     how: JoinHow | Literal[\u001b[33m\"\u001b[39m\u001b[33masof\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mouter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1910\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1911\u001b[39m     \u001b[38;5;28mself\u001b[39m.fill_method = fill_method\n\u001b[32m-> \u001b[39m\u001b[32m1912\u001b[39m     \u001b[43m_MergeOperation\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# factorize sorts\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amogh\\OneDrive - University of Cambridge\\Programming-New\\Part-II-Project\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:786\u001b[39m, in \u001b[36m_MergeOperation.__init__\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    779\u001b[39m     msg = (\n\u001b[32m    780\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNot allowed to merge between different levels. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    781\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_left.columns.nlevels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m levels on the left, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    782\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_right.columns.nlevels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on the right)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    783\u001b[39m     )\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(msg)\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m \u001b[38;5;28mself\u001b[39m.left_on, \u001b[38;5;28mself\u001b[39m.right_on = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_left_right_on\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m (\n\u001b[32m    789\u001b[39m     \u001b[38;5;28mself\u001b[39m.left_join_keys,\n\u001b[32m    790\u001b[39m     \u001b[38;5;28mself\u001b[39m.right_join_keys,\n\u001b[32m   (...)\u001b[39m\u001b[32m    793\u001b[39m     right_drop,\n\u001b[32m    794\u001b[39m ) = \u001b[38;5;28mself\u001b[39m._get_merge_keys()\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amogh\\OneDrive - University of Cambridge\\Programming-New\\Part-II-Project\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2073\u001b[39m, in \u001b[36m_AsOfMerge._validate_left_right_on\u001b[39m\u001b[34m(self, left_on, right_on)\u001b[39m\n\u001b[32m   2065\u001b[39m     ro_dtype = \u001b[38;5;28mself\u001b[39m.right.index.dtype\n\u001b[32m   2067\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2068\u001b[39m     is_object_dtype(lo_dtype)\n\u001b[32m   2069\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(ro_dtype)\n\u001b[32m   2070\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m is_string_dtype(lo_dtype)\n\u001b[32m   2071\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m is_string_dtype(ro_dtype)\n\u001b[32m   2072\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m2073\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[32m   2074\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIncompatible merge dtype, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(ro_dtype)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2075\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(lo_dtype)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, both sides must have numeric dtype\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2076\u001b[39m     )\n\u001b[32m   2078\u001b[39m \u001b[38;5;66;03m# add 'by' to our key-list so we can have it in the\u001b[39;00m\n\u001b[32m   2079\u001b[39m \u001b[38;5;66;03m# output as a key\u001b[39;00m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.left_by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mMergeError\u001b[39m: Incompatible merge dtype, dtype('<M8[ns]') and dtype('O'), both sides must have numeric dtype"
     ]
    }
   ],
   "source": [
    "# Prepare trading dates DataFrame from df_labels index\n",
    "# This contains all the trading days where we have labels\n",
    "df_trading_dates = pd.DataFrame({'trading_date': df_labels.index}).sort_values('trading_date')\n",
    "\n",
    "# Prepare news articles - ensure sorted and reset index\n",
    "df_news = df_news.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "# Create a temporary DataFrame with Date for merge_asof\n",
    "df_news_temp = df_news[['Date']].copy()\n",
    "\n",
    "# For FUTURE labels: Use next available trading day if article is on non-trading day\n",
    "# direction='forward' means: for each article date, find the next trading day\n",
    "df_news_future = pd.merge_asof(\n",
    "    df_news_temp.sort_values('Date'),\n",
    "    df_trading_dates.sort_values('trading_date'),\n",
    "    left_on='Date',\n",
    "    right_on='trading_date',\n",
    "    direction='forward'  # Forward: next available trading day\n",
    ")\n",
    "\n",
    "# For PAST labels: Use most recent preceding trading day if article is on non-trading day\n",
    "# direction='backward' means: for each article date, find the previous trading day\n",
    "df_news_past = pd.merge_asof(\n",
    "    df_news_temp.sort_values('Date'),\n",
    "    df_trading_dates.sort_values('trading_date'),\n",
    "    left_on='Date',\n",
    "    right_on='trading_date',\n",
    "    direction='backward'  # Backward: previous trading day\n",
    ")\n",
    "\n",
    "# Merge the aligned trading dates back to df_news using Date as key\n",
    "df_news = df_news.merge(\n",
    "    df_news_future[['Date', 'trading_date']].rename(columns={'trading_date': 'trading_date_future'}),\n",
    "    on='Date',\n",
    "    how='left'\n",
    ")\n",
    "df_news = df_news.merge(\n",
    "    df_news_past[['Date', 'trading_date']].rename(columns={'trading_date': 'trading_date_past'}),\n",
    "    on='Date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Prepare labels DataFrames with trading_date as a column for merging\n",
    "df_labels_future = df_labels[[col for col in df_labels.columns if col.endswith('_forward_label')]].copy()\n",
    "df_labels_future['trading_date'] = df_labels_future.index\n",
    "df_labels_future = df_labels_future.reset_index(drop=True)\n",
    "\n",
    "df_labels_past = df_labels[[col for col in df_labels.columns if col.endswith('_backward_label')]].copy()\n",
    "df_labels_past['trading_date'] = df_labels_past.index\n",
    "df_labels_past = df_labels_past.reset_index(drop=True)\n",
    "\n",
    "# Merge forward labels using the aligned future trading dates\n",
    "df_news = df_news.merge(\n",
    "    df_labels_future,\n",
    "    left_on='trading_date_future',\n",
    "    right_on='trading_date',\n",
    "    how='left',\n",
    "    suffixes=('', '_future')\n",
    ")\n",
    "\n",
    "# Merge backward labels using the aligned past trading dates\n",
    "df_news = df_news.merge(\n",
    "    df_labels_past,\n",
    "    left_on='trading_date_past',\n",
    "    right_on='trading_date',\n",
    "    how='left',\n",
    "    suffixes=('', '_past')\n",
    ")\n",
    "\n",
    "# Drop the temporary trading_date columns from merges\n",
    "df_news = df_news.drop(columns=['trading_date', 'trading_date_future', 'trading_date_past'], errors='ignore')\n",
    "\n",
    "print(f\"Total articles: {len(df_news)}\")\n",
    "print(f\"Articles with forward labels: {df_news[[col for col in df_news.columns if col.endswith('_forward_label')]].notna().any(axis=1).sum()}\")\n",
    "print(f\"Articles with backward labels: {df_news[[col for col in df_news.columns if col.endswith('_backward_label')]].notna().any(axis=1).sum()}\")\n",
    "print(\"\\nSample of aligned articles:\")\n",
    "label_cols = [col for col in df_news.columns if 'label' in col]\n",
    "display_cols = ['Date', 'Title'] + label_cols[:6] if len(label_cols) > 6 else ['Date', 'Title'] + label_cols\n",
    "print(df_news[display_cols].head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
