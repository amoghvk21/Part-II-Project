{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de4e9b9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e642e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report\n",
    "from tqdm import tqdm\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841054f",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 8192\n",
    "\n",
    "# LoRA/PEFT parameters\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_BIAS = \"none\"\n",
    "LORA_TASK_TYPE = \"CAUSAL_LM\"\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8e58f",
   "metadata": {},
   "source": [
    "# Prompt Generation\n",
    "\n",
    "Need a different structure of prompt due to it being an instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(row):\n",
    "    title = row.get('Title', '')\n",
    "    text = row.get('Full Text', '')\n",
    "    currencies = row.get('mentioned_currencies')\n",
    "\n",
    "    target_currencies = ''\n",
    "    for c in currencies:\n",
    "        target_currencies += f'{c}_past: \"appreciation, depreciation, or unchanged\",\\n'\n",
    "        target_currencies += f'{c}_future: \"appreciation, depreciation, or unchanged\",\\n'\n",
    "    target_currencies = target_currencies.strip().rstrip(\",\") # Remove last comma\n",
    "\n",
    "    # Same structure as per paper\n",
    "    text = (\n",
    "        f\"Title: {title}\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"Objective: For each mentioned currency, answer the following questions:\\n\"\n",
    "        \"- What has been the current/past movement of the currency (appreciation, depreciation, or unchanged)?\\n\"\n",
    "        \"- What is the future expectation for the currency (appreciation, depreciation, or unchanged)?\\n\\n\"\n",
    "        \"You must answer these two questions for each of the following currencies mentioned in the article:\\n\"\n",
    "        f\"{target_currencies}\\n\\n\"\n",
    "        \"Output Format:\\n\"\n",
    "        \"- Important: Provide your answer in separate rows for each currency as shown above.\\n\"\n",
    "        \"- Do not combine multiple currencies in the same row.\\n\"\n",
    "        '- Each currency should have its own line with \"_past\" or \"_future\" specified.\\n\\n'\n",
    "        \"Example:\\n\"\n",
    "        '- If the article states, \"The EUR is expected to appreciate,\" the output should be:\\n'\n",
    "        '    EUR_past: \"unchanged\",\\n'\n",
    "        '    EUR_future: \"appreciation\"\\n'\n",
    "        '- If the article states, \"EUR/USD depreciated last week,\" the output should be:\\n'\n",
    "        '    EUR_past: \"depreciation\",\\n'\n",
    "        '    USD_past: \"appreciation\"\\n'\n",
    "        '- If only future movements are mentioned for a currency, the past movement should be labelled as \"unchanged\" and vice versa.\\n\\n'\n",
    "        \"Currency Pair Interpretation:\\n\"\n",
    "        \"- If currencies are discussed in pairs, interpret as follows:\\n\"\n",
    "        '    - If \"EUR/USD appreciated,\" label EUR_past as \"appreciation\" and USD_past as \"depreciation\".\\n'\n",
    "        '    - If \"EUR/USD depreciated,\" label EUR_past as \"depreciation\" and USD_past as \"appreciation\".\\n\\n'\n",
    "        \"Synonyms:\\n\"\n",
    "        \"- Recognize the following synonyms for each currency:\\n\"\n",
    "        \"- **EUR**: EUR, Euro\\n\"\n",
    "        \"- **USD**: USD, Dollar, Dollars, US Dollar, US-Dollar, U.S. Dollar, US Dollars, US-Dollars, U.S. Dollars, Greenback\\n\"\n",
    "        \"- **JPY**: JPY, Yen, Japanese Yen\\n\"\n",
    "        \"- **GBP**: GBP, Pound, Pounds, Sterling, British Pound, British Pounds\\n\"\n",
    "        \"- **AUD**: AUD, Australian Dollar, Australian Dollars, Aussie\\n\"\n",
    "        \"- **CAD**: CAD, Canadian Dollar, Canadian Dollars\\n\"\n",
    "        \"- **CHF**: CHF, Swiss Franc, Swiss Francs, Swissie\\n\"\n",
    "        \"- **NZD**: NZD, New Zealand Dollar, New Zealand Dollars, Kiwi\\n\"\n",
    "        \"- **NOK**: NOK, Norwegian Krone, Norwegian Kroner\\n\"\n",
    "        \"- **SEK**: SEK, Swedish Krona, Swedish Kronor\\n\"\n",
    "        \"Answer below in the given format:\"\n",
    "    )\n",
    "\n",
    "    return f\"[INST] {text} [/INST]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5ffa6",
   "metadata": {},
   "source": [
    "# Mistral Nemo Instuct Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(model_id):\n",
    "\n",
    "    # quantisation\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # tokeniser\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.model_max_length = MAX_SEQ_LENGTH\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        if tokenizer.unk_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "            print(\"Using unk_token as pad_token.\")\n",
    "        else:\n",
    "            raise Exception('No padding token')\n",
    "        \n",
    "    else:\n",
    "        print(f\"Padding token is already set to: {tokenizer.pad_token}\")\n",
    "            \n",
    "    tokenizer.padding_side = \"right\" # Right padding for training\n",
    "\n",
    "    # load model \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "\n",
    "    # Enable gradient checkpointing to save memory\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Prepare for kbit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_R,       \n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=LORA_BIAS,\n",
    "        task_type=LORA_TASK_TYPE,\n",
    "        target_modules=LORA_TARGET_MODULES\n",
    "    )\n",
    "\n",
    "    return model, tokenizer, peft_config\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d168cb4",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276950db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, tokenizer, peft_config, df_train, df_test, save_name):\n",
    "\n",
    "    tokenizer.padding_side = \"right\"   # for finetuning\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
