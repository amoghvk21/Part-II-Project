{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1c8db9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e161c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report\n",
    "from tqdm import tqdm\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d686e9",
   "metadata": {},
   "source": [
    "# Hyperparamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 8192\n",
    "\n",
    "\n",
    "# LoRA/PEFT parameters\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.1\n",
    "LORA_BIAS = \"none\"\n",
    "LORA_TASK_TYPE = \"CAUSAL_LM\"\n",
    "LORA_TARGET_MODULES = [   # Injecting into all linear layers as per paper\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention projections\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"      # MLP projections\n",
    "]\n",
    "\n",
    "\n",
    "# Llama parameters\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 32\n",
    "SAVE_STEPS = 50\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 0.1\n",
    "MAX_GRAD_NORM = 0.3\n",
    "LOGGING_STEPS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39985e",
   "metadata": {},
   "source": [
    "# Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab36307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(row, is_training=False):\n",
    "    title = row.get('Title', '')\n",
    "    text = row.get('Full Text', '')\n",
    "    currencies = row.get('mentioned_currencies')\n",
    "\n",
    "    target_currencies = ''\n",
    "    for c in currencies:\n",
    "        target_currencies += f'{c}_past: \"appreciation, depreciation, or unchanged\",\\n'\n",
    "        target_currencies += f'{c}_future: \"appreciation, depreciation, or unchanged\",\\n'\n",
    "    target_currencies = target_currencies.strip().rstrip(\",\") # Remove last comma\n",
    "\n",
    "    # Same structure as per paper\n",
    "    prompt = (\n",
    "        f\"Title: {title}\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"Objective: For each mentioned currency, answer the following questions:\\n\"\n",
    "        \"- What has been the current/past movement of the currency (appreciation, depreciation, or unchanged)?\\n\"\n",
    "        \"- What is the future expectation for the currency (appreciation, depreciation, or unchanged)?\\n\\n\"\n",
    "        \"You must answer these two questions for each of the following currencies mentioned in the article:\\n\"\n",
    "        f\"{target_currencies}\\n\\n\"\n",
    "        \"Output Format:\\n\"\n",
    "        \"- Important: Provide your answer in separate rows for each currency as shown above.\\n\"\n",
    "        \"- Do not combine multiple currencies in the same row.\\n\"\n",
    "        '- Each currency should have its own line with \"_past\" or \"_future\" specified.\\n\\n'\n",
    "        \"Example:\\n\"\n",
    "        '- If the article states, \"The EUR is expected to appreciate,\" the output should be:\\n'\n",
    "        '    EUR_past: \"unchanged\",\\n'\n",
    "        '    EUR_future: \"appreciation\"\\n'\n",
    "        '- If the article states, \"EUR/USD depreciated last week,\" the output should be:\\n'\n",
    "        '    EUR_past: \"depreciation\",\\n'\n",
    "        '    USD_past: \"appreciation\"\\n'\n",
    "        '- If only future movements are mentioned for a currency, the past movement should be labelled as \"unchanged\" and vice versa.\\n\\n'\n",
    "        \"Currency Pair Interpretation:\\n\"\n",
    "        \"- If currencies are discussed in pairs, interpret as follows:\\n\"\n",
    "        '    - If \"EUR/USD appreciated,\" label EUR_past as \"appreciation\" and USD_past as \"depreciation\".\\n'\n",
    "        '    - If \"EUR/USD depreciated,\" label EUR_past as \"depreciation\" and USD_past as \"appreciation\".\\n\\n'\n",
    "        \"Synonyms:\\n\"\n",
    "        \"- Recognize the following synonyms for each currency:\\n\"\n",
    "        \"- **EUR**: EUR, Euro\\n\"\n",
    "        \"- **USD**: USD, Dollar, Dollars, US Dollar, US-Dollar, U.S. Dollar, US Dollars, US-Dollars, U.S. Dollars, Greenback\\n\"\n",
    "        \"- **JPY**: JPY, Yen, Japanese Yen\\n\"\n",
    "        \"- **GBP**: GBP, Pound, Pounds, Sterling, British Pound, British Pounds\\n\"\n",
    "        \"- **AUD**: AUD, Australian Dollar, Australian Dollars, Aussie\\n\"\n",
    "        \"- **CAD**: CAD, Canadian Dollar, Canadian Dollars\\n\"\n",
    "        \"- **CHF**: CHF, Swiss Franc, Swiss Francs, Swissie\\n\"\n",
    "        \"- **NZD**: NZD, New Zealand Dollar, New Zealand Dollars, Kiwi\\n\"\n",
    "        \"- **NOK**: NOK, Norwegian Krone, Norwegian Kroner\\n\"\n",
    "        \"- **SEK**: SEK, Swedish Krona, Swedish Kronor\\n\"\n",
    "        \"Answer below in the given format:\"\n",
    "    )\n",
    "    \n",
    "    if is_training:\n",
    "        # Exptected output for currencies mentioned in the article\n",
    "        expected_output = \"\"\n",
    "        for c in currencies:\n",
    "            past_label = row.get(f'{c}_past_label', 'unchanged')\n",
    "            future_label = row.get(f'{c}_future_label', 'unchanged')\n",
    "            \n",
    "            expected_output += f'{c}_past: \"{past_label}\"\\n'\n",
    "            expected_output += f'{c}_future: \"{future_label}\"\\n'\n",
    "        \n",
    "        return prompt + \"\\n\" + expected_output\n",
    "    else:\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8efbc3",
   "metadata": {},
   "source": [
    "# Finetuned Llama Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce73105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(model_id):\n",
    "    \n",
    "\n",
    "    # quntisation config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True, \n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # load tokeniser\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.model_max_length = MAX_SEQ_LENGTH\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        if '<|finetune_right_pad_id|>' in tokenizer.get_vocab():\n",
    "            tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "            tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<|finetune_right_pad_id|>')\n",
    "            print(\"Using Llama 4 dedicated finetune pad token.\")   \n",
    "        \n",
    "        elif '<|reserved_special_token_0|>' in tokenizer.get_vocab():\n",
    "            tokenizer.pad_token = '<|reserved_special_token_0|>'\n",
    "            tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<|reserved_special_token_0|>')\n",
    "            print(\"Using reserved token for padding.\")\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Can't find padding token\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Padding token is already set to: {tokenizer.pad_token}\")\n",
    "\n",
    "    tokenizer.padding_side = \"right\"    # Use right for finetuning\n",
    "\n",
    "    # load model \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        model = model.to(device)\n",
    "        print(f\"Model explicitly loaded onto: {device}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = model.to(device)\n",
    "        print(\"CUDA not available. Model loaded onto CPU.\")\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    # Prepare for training \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # LoRA config\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=LORA_BIAS,\n",
    "        task_type=LORA_TASK_TYPE,\n",
    "        target_modules=LORA_TARGET_MODULES\n",
    "    )\n",
    "\n",
    "    return model, tokenizer, peft_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e4ad4",
   "metadata": {},
   "source": [
    "### 4 LLM Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd3d1c",
   "metadata": {},
   "source": [
    "- Stopping criterion is used\n",
    "    - Optimisises for least loss in the validation stage rather than most traning epochs\n",
    "    - So if the model with best validation loss is in epoch 1 or 2, then the weights in epoch 3 will be discarded\n",
    "    - Used to prevent overfitting due to this being a small dataset\n",
    "    - Stops traning if the validation loss stagnates due to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, tokenizer, peft_config, df_train, df_test, save_name):\n",
    "\n",
    "    tokenizer.padding_side = \"right\"   # for finetuning\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/{save_name}/checkpoints\",\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,      # TODO Check this\n",
    "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,       # TODO Check this\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, \n",
    "        optim=\"paged_adamw_32bit\",          # \n",
    "        save_steps=SAVE_STEPS,                      # TODO get better number\n",
    "        learning_rate=LEARNING_RATE,                 #  Note: significantly lower than standard\n",
    "        weight_decay=WEIGHT_DECAY,                   #  High weight decay\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        max_grad_norm=MAX_GRAD_NORM,                  # TODO apparenly this is the best for lora??? - not said in the paper\n",
    "        warmup_ratio=0.0,                   # \n",
    "        lr_scheduler_type=\"cosine\",         #                  \n",
    "        save_strategy=\"steps\",              # for early stopping   (could be epoch)\n",
    "        eval_strategy=\"steps\",              # for early stopping   (could be epoch)\n",
    "        load_best_model_at_end=True,         # for early stopping\n",
    "        metric_for_best_model=\"eval_loss\",   # for early stopping\n",
    "        greater_is_better=False,     # less loss is better\n",
    "        logging_steps=LOGGING_STEPS,                   # TODO get a better number\n",
    "        group_by_length=True,\n",
    "        report_to=\"none\"                    # Disable wandb unless needed\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    df_train = Dataset.from_pandas(df_train)\n",
    "    df_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=df_train, # Ensure this is loaded\n",
    "        eval_dataset=df_test,\n",
    "        peft_config=peft_config,\n",
    "        formatting_func=generate_prompt,\n",
    "        processing_class=tokenizer,\n",
    "        args=training_args,\n",
    "        # packing=False,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)] # to stop after epoch 1 if validaiton loss gets worse\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.model.save_pretrained(f\"./models/{save_name}\")\n",
    "    print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3f748",
   "metadata": {},
   "source": [
    "# Evaulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ee3bc",
   "metadata": {},
   "source": [
    "## 5.1 Predict sentiment\n",
    "- Gets the sentiment for a single article\n",
    "- Used for evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d719156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(row, model, tokenizer):\n",
    "    prompt = generate_prompt(row)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,  # to avoid crashing model due to very large article\n",
    "        max_length=8192\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=512,     # only needs to generate enough for sentiment\n",
    "            temperature=0.1,        # incase there was sampling\n",
    "            do_sample=False,        # no sampling - so no randomness\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response[len(prompt):].strip()    # skips over prompt\n",
    "\n",
    "    # Parse response to get labels into a dict\n",
    "    sentiment = {}\n",
    "    for line in response.split('\\n'):\n",
    "        if line.strip():\n",
    "            currency, label = line.split(':')\n",
    "            currency = currency.strip()\n",
    "            label = label.strip()\n",
    "            sentiment[currency] = label\n",
    "\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc5a9c3",
   "metadata": {},
   "source": [
    "## 5.2 Get evaulation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, tokenizer, df_eval):\n",
    "    currency_codes = ['EUR', 'USD', 'GBP', 'JPY', 'AUD', 'CAD', 'CHF', 'NZD', 'NOK', 'SEK']\n",
    "\n",
    "    all_actual = []\n",
    "    all_predictions = []\n",
    "\n",
    "    tokenizer.padding_side = \"left\"   # for inference\n",
    "\n",
    "    for i, row in df_eval.iterrows():\n",
    "        sentiment = get_sentiment(row, model, tokenizer)\n",
    "        for c in currency_codes:\n",
    "            for t in ['past', 'future']:\n",
    "                all_actual.append(row[f'{c}_{t}_label'])\n",
    "                all_predictions.append(sentiment.get(f'{c}_{t}', 'unchanged'))\n",
    "\n",
    "        \n",
    "        \n",
    "    accuracy = accuracy_score(all_actual, all_predictions)\n",
    "    f1 = f1_score(all_actual, all_predictions, average='macro')\n",
    "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(all_actual, all_predictions, labels=['appreciation', 'depreciation', 'unchanged'])\n",
    "\n",
    "    stats = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision_per_class': dict(zip(['appreciation', 'depreciation', 'unchanged'], precision_per_class)),\n",
    "        'recall_per_class': dict(zip(['appreciation', 'depreciation', 'unchanged'], recall_per_class)),\n",
    "        'f1_per_class': dict(zip(['appreciation', 'depreciation', 'unchanged'], f1_per_class)),\n",
    "        'support_per_class': dict(zip(['appreciation', 'depreciation', 'unchanged'], support_per_class))\n",
    "    }\n",
    "\n",
    "    report = classification_report(all_actual, all_predictions)\n",
    "\n",
    "    print(stats)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a557df8e",
   "metadata": {},
   "source": [
    "# Loading Model for Downstream Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ece50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(base_model_id, adapter_dir):\n",
    "    # 2. Quantization (Recommended to match your training environment)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # 3. Load the Tokenizer (Load from base model, not adapter dir, unless you explicitly saved it there)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"    # for inference\n",
    "\n",
    "    # 4. Load the Base Model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # 5. Load and attach the Fine-Tuned Adapters\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "\n",
    "    # 6. Set mode for inference\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Tokenizer, Base Model, and Adapters loaded successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
