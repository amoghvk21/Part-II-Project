W0215 03:13:26.822000 2289 torch/distributed/run.py:793] 
W0215 03:13:26.822000 2289 torch/distributed/run.py:793] *****************************************
W0215 03:13:26.822000 2289 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0215 03:13:26.822000 2289 torch/distributed/run.py:793] *****************************************
imports done
DDP initialized: local_rank=0, rank=0, world_size=4
Reading from cache
imports done
imports done
imports done
Size of train set:  23840
Size of test set:  5960
Size of eval set:  200
new hyperparams
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.
DDP enabled | world_size=4
PID=2339 RANK=0 LOCAL_RANK=0 
new hyperparams
new hyperparams
new hyperparams
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.
PID=2341 RANK=2 LOCAL_RANK=2 
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.
PID=2340 RANK=1 LOCAL_RANK=1 
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.
PID=2342 RANK=3 LOCAL_RANK=3 
Set pad_token to Llama 3 reserved token (<|reserved_special_token_0|>)
DEBUG: Rank 0 is attempting to load model.
DEBUG: Rank 0 device_map is STRICTLY: {'': 0}
DEBUG: Rank 3 is attempting to load model.
DEBUG: Rank 3 device_map is STRICTLY: {'': 3}
DEBUG: Rank 2 is attempting to load model.
DEBUG: Rank 2 device_map is STRICTLY: {'': 2}
DEBUG: Rank 1 is attempting to load model.
DEBUG: Rank 1 device_map is STRICTLY: {'': 1}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:40<02:01, 40.48s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:40<02:01, 40.40s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:40<02:01, 40.53s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:40<02:01, 40.54s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:18<01:18, 39.09s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:18<01:18, 39.10s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:18<01:18, 39.10s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:18<01:18, 39.07s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:56<00:38, 38.36s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:56<00:38, 38.35s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:56<00:38, 38.38s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:56<00:38, 38.38s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:05<00:00, 26.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:05<00:00, 31.25s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:05<00:00, 26.73s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:05<00:00, 31.26s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:04<00:00, 26.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:04<00:00, 31.23s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:05<00:00, 26.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:05<00:00, 31.26s/it]
[Rank 1] Model loaded on device: cuda:1
[Rank 3] Model loaded on device: cuda:3
[Rank 0] Model loaded on device: cuda:0
[Rank 2] Model loaded on device: cuda:2
setup complete
Number of GPUs available: 4
GPU 0: NVIDIA L40S
GPU 1: NVIDIA L40S
GPU 2: NVIDIA L40S
GPU 3: NVIDIA L40S
finetuning model | learning_rate=5e-06 | save_dir=finetuned_llama_8b_lr5e6
Training with DDP | world_size=4
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from WANDB_API_KEY.
wandb: Currently logged in as: av670 (av670-university-of-cambridge) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.24.2
wandb: Run data is saved locally in /home/ubuntu/Part-II-Project/wandb/run-20260215_031546-d0vm0a05
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetuned_llama_8b_lr5e6
wandb: â­ï¸ View project at https://wandb.ai/av670-university-of-cambridge/part-ii-project
wandb: ðŸš€ View run at https://wandb.ai/av670-university-of-cambridge/part-ii-project/runs/d0vm0a05
Applying formatting function to train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Applying formatting function to train dataset:   1%|â–         | 344/23840 [00:00<00:06, 3381.85 examples/s]Applying formatting function to train dataset:   3%|â–Ž         | 752/23840 [00:00<00:06, 3783.82 examples/s]Applying formatting function to train dataset:   5%|â–Œ         | 1206/23840 [00:00<00:13, 1729.47 examples/s]Applying formatting function to train dataset:   7%|â–‹         | 1622/23840 [00:00<00:09, 2249.25 examples/s]Applying formatting function to train dataset:   8%|â–Š         | 2000/23840 [00:00<00:09, 2288.35 examples/s]Applying formatting function to train dataset:  10%|â–ˆ         | 2417/23840 [00:00<00:07, 2713.26 examples/s]Applying formatting function to train dataset:  12%|â–ˆâ–        | 2840/23840 [00:01<00:06, 3082.52 examples/s]Applying formatting function to train dataset:  13%|â–ˆâ–Ž        | 3206/23840 [00:01<00:07, 2878.64 examples/s]Applying formatting function to train dataset:  15%|â–ˆâ–Œ        | 3625/23840 [00:01<00:06, 3201.69 examples/s]Applying formatting function to train dataset:  17%|â–ˆâ–‹        | 4000/23840 [00:01<00:06, 2841.80 examples/s]Applying formatting function to train dataset:  19%|â–ˆâ–Š        | 4417/23840 [00:01<00:06, 3159.95 examples/s]Applying formatting function to train dataset:  20%|â–ˆâ–ˆ        | 4840/23840 [00:01<00:05, 3431.67 examples/s]Applying formatting function to train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5419/23840 [00:01<00:05, 3238.31 examples/s]Applying formatting function to train dataset:  25%|â–ˆâ–ˆâ–       | 5842/23840 [00:01<00:05, 3467.61 examples/s]Applying formatting function to train dataset:  27%|â–ˆâ–ˆâ–‹       | 6420/23840 [00:02<00:05, 3266.56 examples/s]Applying formatting function to train dataset:  29%|â–ˆâ–ˆâ–Š       | 6842/23840 [00:02<00:04, 3477.21 examples/s]Applying formatting function to train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7420/23840 [00:02<00:07, 2293.12 examples/s]Applying formatting function to train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7843/23840 [00:02<00:06, 2609.92 examples/s]Applying formatting function to train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8209/23840 [00:02<00:06, 2414.24 examples/s]Applying formatting function to train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8632/23840 [00:03<00:05, 2756.66 examples/s]Applying formatting function to train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9000/23840 [00:03<00:05, 2686.92 examples/s]Applying formatting function to train dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 9418/23840 [00:03<00:04, 3008.73 examples/s]Applying formatting function to train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9844/23840 [00:03<00:04, 3301.76 examples/s]Applying formatting function to train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10421/23840 [00:03<00:04, 3161.86 examples/s]Applying formatting function to train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10843/23840 [00:03<00:03, 3398.32 examples/s]Applying formatting function to train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11416/23840 [00:03<00:03, 3217.31 examples/s]Applying formatting function to train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11835/23840 [00:04<00:03, 3430.58 examples/s]Applying formatting function to train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12205/23840 [00:04<00:03, 3125.99 examples/s]Applying formatting function to train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12622/23840 [00:04<00:03, 3369.10 examples/s]Applying formatting function to train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13000/23840 [00:04<00:05, 2093.80 examples/s]Applying formatting function to train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13418/23840 [00:04<00:04, 2463.48 examples/s]Applying formatting function to train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13840/23840 [00:04<00:03, 2817.11 examples/s]Applying formatting function to train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14208/23840 [00:04<00:03, 2724.62 examples/s]Applying formatting function to train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14630/23840 [00:05<00:03, 3058.23 examples/s]Applying formatting function to train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15000/23840 [00:05<00:03, 2453.20 examples/s]Applying formatting function to train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15419/23840 [00:05<00:02, 2815.39 examples/s]Applying formatting function to train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15841/23840 [00:05<00:02, 3138.33 examples/s]Applying formatting function to train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16209/23840 [00:05<00:02, 2938.48 examples/s]Applying formatting function to train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16628/23840 [00:05<00:02, 3238.50 examples/s]Applying formatting function to train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17000/23840 [00:05<00:02, 3004.37 examples/s]Applying formatting function to train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17417/23840 [00:05<00:01, 3290.04 examples/s]Applying formatting function to train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17838/23840 [00:06<00:01, 3527.71 examples/s]Applying formatting function to train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18417/23840 [00:06<00:01, 3274.85 examples/s]Applying formatting function to train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18838/23840 [00:06<00:01, 3492.99 examples/s]Applying formatting function to train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19418/23840 [00:06<00:01, 2281.23 examples/s]Applying formatting function to train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19837/23840 [00:06<00:01, 2599.33 examples/s]Applying formatting function to train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20207/23840 [00:07<00:01, 2574.09 examples/s]Applying formatting function to train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20628/23840 [00:07<00:01, 2902.25 examples/s]Applying formatting function to train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21000/23840 [00:07<00:01, 2788.56 examples/s]Applying formatting function to train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21415/23840 [00:07<00:00, 3091.28 examples/s]Applying formatting function to train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21835/23840 [00:07<00:00, 3357.12 examples/s]Applying formatting function to train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22207/23840 [00:07<00:00, 3066.29 examples/s]Applying formatting function to train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22628/23840 [00:07<00:00, 3346.10 examples/s]Applying formatting function to train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23000/23840 [00:07<00:00, 3061.09 examples/s]Applying formatting function to train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23418/23840 [00:07<00:00, 3335.50 examples/s]Applying formatting function to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:08<00:00, 3565.16 examples/s]Applying formatting function to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:08<00:00, 2923.24 examples/s]
Adding EOS to train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Adding EOS to train dataset:   2%|â–         | 539/23840 [00:00<00:04, 5307.64 examples/s]Adding EOS to train dataset:   5%|â–Œ         | 1271/23840 [00:00<00:04, 4549.47 examples/s]Adding EOS to train dataset:   8%|â–Š         | 1814/23840 [00:00<00:04, 4869.23 examples/s]Adding EOS to train dataset:  11%|â–ˆ         | 2548/23840 [00:00<00:04, 4525.41 examples/s]Adding EOS to train dataset:  14%|â–ˆâ–Ž        | 3263/23840 [00:00<00:04, 4400.61 examples/s]Adding EOS to train dataset:  16%|â–ˆâ–Œ        | 3787/23840 [00:00<00:04, 4608.72 examples/s]Adding EOS to train dataset:  18%|â–ˆâ–Š        | 4264/23840 [00:00<00:04, 4164.79 examples/s]Adding EOS to train dataset:  20%|â–ˆâ–ˆ        | 4788/23840 [00:01<00:04, 4434.05 examples/s]Adding EOS to train dataset:  22%|â–ˆâ–ˆâ–       | 5264/23840 [00:01<00:04, 4252.54 examples/s]Adding EOS to train dataset:  24%|â–ˆâ–ˆâ–       | 5789/23840 [00:01<00:04, 4511.62 examples/s]Adding EOS to train dataset:  26%|â–ˆâ–ˆâ–‹       | 6263/23840 [00:01<00:04, 4310.42 examples/s]Adding EOS to train dataset:  28%|â–ˆâ–ˆâ–Š       | 6788/23840 [00:01<00:03, 4561.34 examples/s]Adding EOS to train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7264/23840 [00:01<00:03, 4345.44 examples/s]Adding EOS to train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7791/23840 [00:01<00:03, 4593.99 examples/s]Adding EOS to train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 8264/23840 [00:01<00:03, 3935.68 examples/s]Adding EOS to train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8790/23840 [00:01<00:03, 4269.48 examples/s]Adding EOS to train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9263/23840 [00:02<00:03, 4142.94 examples/s]Adding EOS to train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9784/23840 [00:02<00:03, 4420.76 examples/s]Adding EOS to train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10258/23840 [00:02<00:03, 4231.34 examples/s]Adding EOS to train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10776/23840 [00:02<00:02, 4484.56 examples/s]Adding EOS to train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11259/23840 [00:02<00:02, 4273.92 examples/s]Adding EOS to train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11778/23840 [00:02<00:02, 4518.70 examples/s]Adding EOS to train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12255/23840 [00:02<00:02, 4291.47 examples/s]Adding EOS to train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12771/23840 [00:02<00:02, 4523.42 examples/s]Adding EOS to train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13259/23840 [00:03<00:02, 4303.36 examples/s]Adding EOS to train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13780/23840 [00:03<00:02, 4546.50 examples/s]Adding EOS to train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14262/23840 [00:03<00:02, 4325.62 examples/s]Adding EOS to train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14781/23840 [00:03<00:01, 4558.04 examples/s]Adding EOS to train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15260/23840 [00:03<00:02, 3560.88 examples/s]Adding EOS to train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15780/23840 [00:03<00:02, 3944.76 examples/s]Adding EOS to train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16258/23840 [00:03<00:01, 3899.94 examples/s]Adding EOS to train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16777/23840 [00:03<00:01, 4222.78 examples/s]Adding EOS to train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17260/23840 [00:04<00:01, 4111.18 examples/s]Adding EOS to train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17782/23840 [00:04<00:01, 4398.80 examples/s]Adding EOS to train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18259/23840 [00:04<00:01, 4228.67 examples/s]Adding EOS to train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18780/23840 [00:04<00:01, 4488.72 examples/s]Adding EOS to train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19258/23840 [00:04<00:01, 4282.60 examples/s]Adding EOS to train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19782/23840 [00:04<00:00, 4538.48 examples/s]Adding EOS to train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20259/23840 [00:04<00:00, 4318.07 examples/s]Adding EOS to train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20777/23840 [00:04<00:00, 4551.58 examples/s]Adding EOS to train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21264/23840 [00:04<00:00, 4345.81 examples/s]Adding EOS to train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21791/23840 [00:05<00:00, 4594.54 examples/s]Adding EOS to train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22432/23840 [00:05<00:00, 2923.96 examples/s]Adding EOS to train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 22953/23840 [00:05<00:00, 3348.75 examples/s]Adding EOS to train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23575/23840 [00:05<00:00, 3574.38 examples/s]Adding EOS to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:05<00:00, 4180.18 examples/s]
Tokenizing train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 42/23840 [00:00<00:58, 405.27 examples/s]Tokenizing train dataset:   0%|          | 92/23840 [00:00<00:52, 456.17 examples/s]Tokenizing train dataset:   1%|          | 139/23840 [00:00<00:51, 458.42 examples/s]Tokenizing train dataset:   1%|          | 189/23840 [00:00<00:49, 473.87 examples/s]Tokenizing train dataset:   1%|          | 241/23840 [00:00<00:48, 486.68 examples/s]Tokenizing train dataset:   1%|          | 291/23840 [00:00<00:48, 486.73 examples/s]Tokenizing train dataset:   1%|â–         | 340/23840 [00:00<00:48, 484.99 examples/s]Tokenizing train dataset:   2%|â–         | 389/23840 [00:00<00:48, 484.90 examples/s]Tokenizing train dataset:   2%|â–         | 440/23840 [00:00<00:47, 490.03 examples/s]Tokenizing train dataset:   2%|â–         | 490/23840 [00:01<00:47, 492.56 examples/s]Tokenizing train dataset:   2%|â–         | 541/23840 [00:01<00:46, 497.44 examples/s]Tokenizing train dataset:   2%|â–         | 592/23840 [00:01<00:46, 499.33 examples/s]Tokenizing train dataset:   3%|â–Ž         | 646/23840 [00:01<00:45, 507.85 examples/s]Tokenizing train dataset:   3%|â–Ž         | 699/23840 [00:01<00:45, 510.43 examples/s]Tokenizing train dataset:   3%|â–Ž         | 752/23840 [00:01<00:44, 514.54 examples/s]Tokenizing train dataset:   3%|â–Ž         | 827/23840 [00:01<00:45, 505.39 examples/s]Tokenizing train dataset:   4%|â–Ž         | 878/23840 [00:01<00:45, 502.91 examples/s]Tokenizing train dataset:   4%|â–         | 930/23840 [00:01<00:45, 504.17 examples/s]Tokenizing train dataset:   4%|â–         | 1000/23840 [00:02<01:07, 340.71 examples/s]Tokenizing train dataset:   4%|â–         | 1045/23840 [00:02<01:03, 360.42 examples/s]Tokenizing train dataset:   5%|â–         | 1091/23840 [00:02<00:59, 380.35 examples/s]Tokenizing train dataset:   5%|â–         | 1139/23840 [00:02<00:56, 402.06 examples/s]Tokenizing train dataset:   5%|â–         | 1187/23840 [00:02<00:54, 419.28 examples/s]Tokenizing train dataset:   5%|â–Œ         | 1237/23840 [00:02<00:51, 437.80 examples/s]Tokenizing train dataset:   5%|â–Œ         | 1286/23840 [00:02<00:50, 450.05 examples/s]Tokenizing train dataset:   6%|â–Œ         | 1339/23840 [00:02<00:47, 468.92 examples/s]Tokenizing train dataset:   6%|â–Œ         | 1409/23840 [00:03<00:48, 463.59 examples/s]Tokenizing train dataset:   6%|â–Œ         | 1460/23840 [00:03<00:47, 473.59 examples/s]Tokenizing train dataset:   6%|â–‹         | 1533/23840 [00:03<00:47, 474.46 examples/s]Tokenizing train dataset:   7%|â–‹         | 1583/23840 [00:03<00:46, 477.26 examples/s]Tokenizing train dataset:   7%|â–‹         | 1636/23840 [00:03<00:45, 488.63 examples/s]Tokenizing train dataset:   7%|â–‹         | 1711/23840 [00:03<00:45, 489.48 examples/s]Tokenizing train dataset:   7%|â–‹         | 1765/23840 [00:03<00:44, 499.29 examples/s]Tokenizing train dataset:   8%|â–Š         | 1818/23840 [00:03<00:43, 505.48 examples/s]Tokenizing train dataset:   8%|â–Š         | 1893/23840 [00:04<00:43, 501.59 examples/s]Tokenizing train dataset:   8%|â–Š         | 1967/23840 [00:04<00:44, 495.43 examples/s]Tokenizing train dataset:   8%|â–Š         | 2017/23840 [00:04<01:05, 331.18 examples/s]Tokenizing train dataset:   9%|â–Š         | 2065/23840 [00:04<01:01, 356.90 examples/s]Tokenizing train dataset:   9%|â–‰         | 2114/23840 [00:04<00:56, 382.59 examples/s]Tokenizing train dataset:   9%|â–‰         | 2164/23840 [00:04<00:53, 408.84 examples/s]Tokenizing train dataset:   9%|â–‰         | 2210/23840 [00:04<00:51, 418.03 examples/s]Tokenizing train dataset:   9%|â–‰         | 2258/23840 [00:05<00:49, 433.09 examples/s]Tokenizing train dataset:  10%|â–‰         | 2306/23840 [00:05<00:48, 443.24 examples/s]Tokenizing train dataset:  10%|â–‰         | 2357/23840 [00:05<00:47, 454.52 examples/s]Tokenizing train dataset:  10%|â–ˆ         | 2428/23840 [00:05<00:46, 458.50 examples/s]Tokenizing train dataset:  10%|â–ˆ         | 2499/23840 [00:05<00:46, 459.36 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2549/23840 [00:05<00:45, 468.56 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2598/23840 [00:05<00:45, 470.23 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2648/23840 [00:05<00:44, 477.63 examples/s]Tokenizing train dataset:  11%|â–ˆâ–        | 2697/23840 [00:05<00:44, 477.23 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2748/23840 [00:06<00:43, 481.85 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2799/23840 [00:06<00:43, 486.25 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2850/23840 [00:06<00:42, 489.65 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2924/23840 [00:06<00:43, 485.45 examples/s]Tokenizing train dataset:  12%|â–ˆâ–Ž        | 2980/23840 [00:06<00:41, 501.92 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3043/23840 [00:06<01:05, 317.26 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3091/23840 [00:06<00:59, 346.50 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3142/23840 [00:07<00:54, 376.67 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3190/23840 [00:07<00:52, 397.05 examples/s]Tokenizing train dataset:  14%|â–ˆâ–Ž        | 3239/23840 [00:07<00:49, 419.05 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3290/23840 [00:07<00:46, 439.94 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3338/23840 [00:07<00:45, 449.64 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3389/23840 [00:07<00:44, 457.26 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3438/23840 [00:07<00:44, 462.46 examples/s]Tokenizing train dataset:  15%|â–ˆâ–        | 3486/23840 [00:07<00:43, 467.30 examples/s]Tokenizing train dataset:  15%|â–ˆâ–        | 3537/23840 [00:07<00:42, 477.63 examples/s]Tokenizing train dataset:  15%|â–ˆâ–Œ        | 3586/23840 [00:07<00:42, 477.34 examples/s]Tokenizing train dataset:  15%|â–ˆâ–Œ        | 3657/23840 [00:08<00:42, 472.97 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3706/23840 [00:08<00:42, 475.40 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3756/23840 [00:08<00:41, 479.76 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3807/23840 [00:08<00:41, 483.21 examples/s]Tokenizing train dataset:  16%|â–ˆâ–‹        | 3881/23840 [00:08<00:41, 482.09 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 3934/23840 [00:08<00:40, 492.51 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4000/23840 [00:09<00:59, 333.87 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4047/23840 [00:09<00:55, 358.66 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4095/23840 [00:09<00:51, 381.89 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4147/23840 [00:09<00:47, 411.84 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4194/23840 [00:09<00:46, 425.45 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4243/23840 [00:09<00:44, 439.87 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4294/23840 [00:09<00:43, 454.53 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4342/23840 [00:09<00:42, 459.35 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4391/23840 [00:09<00:41, 463.35 examples/s]Tokenizing train dataset:  19%|â–ˆâ–Š        | 4441/23840 [00:09<00:41, 472.24 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4514/23840 [00:10<00:40, 474.44 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4564/23840 [00:10<00:40, 479.91 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4637/23840 [00:10<00:39, 481.33 examples/s]Tokenizing train dataset:  20%|â–ˆâ–‰        | 4686/23840 [00:10<00:39, 480.64 examples/s]Tokenizing train dataset:  20%|â–ˆâ–‰        | 4735/23840 [00:10<00:39, 478.91 examples/s]Tokenizing train dataset:  20%|â–ˆâ–ˆ        | 4785/23840 [00:10<00:39, 478.86 examples/s]Tokenizing train dataset:  20%|â–ˆâ–ˆ        | 4837/23840 [00:10<00:38, 487.51 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 4891/23840 [00:10<00:38, 498.15 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 4964/23840 [00:11<00:38, 489.25 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 5021/23840 [00:11<01:01, 304.84 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 5064/23840 [00:11<00:57, 326.52 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆâ–       | 5116/23840 [00:11<00:51, 364.86 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5165/23840 [00:11<00:47, 392.18 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5215/23840 [00:11<00:44, 417.00 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5266/23840 [00:11<00:42, 439.38 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5316/23840 [00:11<00:41, 451.12 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5366/23840 [00:12<00:40, 461.03 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5433/23840 [00:12<01:01, 299.17 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5482/23840 [00:12<00:54, 333.93 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5531/23840 [00:12<00:50, 365.28 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5579/23840 [00:12<00:46, 389.42 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–Ž       | 5628/23840 [00:12<00:44, 413.03 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5679/23840 [00:12<00:41, 434.07 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5728/23840 [00:13<00:40, 445.77 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5779/23840 [00:13<00:39, 461.27 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5829/23840 [00:13<00:38, 469.06 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–       | 5881/23840 [00:13<00:37, 479.89 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–       | 5956/23840 [00:13<00:37, 482.66 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6022/23840 [00:13<00:53, 332.13 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6068/23840 [00:13<00:49, 355.93 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6116/23840 [00:14<00:46, 380.31 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6168/23840 [00:14<00:43, 408.15 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6215/23840 [00:14<00:41, 421.71 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–‹       | 6266/23840 [00:14<00:39, 441.45 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–‹       | 6314/23840 [00:14<00:38, 450.73 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6362/23840 [00:14<00:38, 456.41 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6436/23840 [00:14<00:37, 465.64 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6487/23840 [00:14<00:36, 475.06 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6539/23840 [00:14<00:35, 483.07 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6592/23840 [00:15<00:34, 493.40 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6643/23840 [00:15<00:34, 495.44 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6717/23840 [00:15<00:34, 491.75 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6790/23840 [00:15<00:34, 488.14 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–Š       | 6841/23840 [00:15<00:34, 490.78 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 6916/23840 [00:15<00:34, 490.54 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 6968/23840 [00:15<00:34, 493.35 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 7022/23840 [00:16<00:50, 330.80 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–‰       | 7070/23840 [00:16<00:46, 357.91 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–‰       | 7119/23840 [00:16<00:43, 385.05 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7170/23840 [00:16<00:40, 412.01 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7218/23840 [00:16<00:38, 427.54 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7271/23840 [00:16<00:36, 450.11 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7320/23840 [00:16<00:36, 456.57 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7370/23840 [00:16<00:35, 464.93 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7419/23840 [00:16<00:34, 469.88 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆâ–      | 7469/23840 [00:17<00:34, 475.35 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7518/23840 [00:17<00:34, 478.47 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7591/23840 [00:17<00:33, 478.71 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7642/23840 [00:17<00:33, 485.62 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7695/23840 [00:17<00:32, 495.22 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 7748/23840 [00:17<00:32, 501.87 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7799/23840 [00:17<00:32, 500.47 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7872/23840 [00:17<00:32, 492.51 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7922/23840 [00:17<00:32, 490.84 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7973/23840 [00:18<00:32, 493.91 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 8043/23840 [00:18<00:47, 332.19 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8092/23840 [00:18<00:43, 360.55 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8143/23840 [00:18<00:40, 391.67 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8193/23840 [00:18<00:37, 414.79 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 8245/23840 [00:18<00:35, 439.16 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 8295/23840 [00:18<00:34, 451.09 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 8344/23840 [00:18<00:33, 458.30 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 8394/23840 [00:19<00:33, 467.58 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 8444/23840 [00:19<00:32, 473.70 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8494/23840 [00:19<00:32, 478.33 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8544/23840 [00:19<00:31, 480.67 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8593/23840 [00:19<00:31, 481.95 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 8644/23840 [00:19<00:31, 489.89 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 8696/23840 [00:19<00:30, 495.29 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8747/23840 [00:19<00:30, 497.10 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8822/23840 [00:19<00:30, 494.77 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8892/23840 [00:20<00:30, 482.46 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 8945/23840 [00:20<00:30, 489.24 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9000/23840 [00:20<00:44, 334.44 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9043/23840 [00:20<00:42, 352.19 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9089/23840 [00:20<00:39, 375.07 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9138/23840 [00:20<00:36, 399.22 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 9187/23840 [00:20<00:35, 418.65 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 9237/23840 [00:21<00:33, 437.78 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9289/23840 [00:21<00:31, 456.28 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9337/23840 [00:21<00:31, 458.85 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9386/23840 [00:21<00:31, 465.22 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 9440/23840 [00:21<00:29, 483.01 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 9490/23840 [00:21<00:29, 485.51 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9540/23840 [00:21<00:29, 487.29 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9593/23840 [00:21<00:29, 487.47 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9644/23840 [00:21<00:28, 490.28 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9718/23840 [00:22<00:29, 485.93 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9769/23840 [00:22<00:28, 488.34 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9819/23840 [00:22<00:28, 487.63 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9873/23840 [00:22<00:27, 499.00 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9949/23840 [00:22<00:27, 498.46 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9999/23840 [00:22<00:27, 495.83 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10070/23840 [00:22<00:46, 296.45 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10115/23840 [00:23<00:42, 322.16 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10165/23840 [00:23<00:38, 355.62 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10213/23840 [00:23<00:35, 380.85 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10261/23840 [00:23<00:33, 402.59 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10310/23840 [00:23<00:32, 421.73 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10360/23840 [00:23<00:30, 438.79 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10429/23840 [00:23<00:30, 443.25 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10498/23840 [00:23<00:29, 447.05 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10549/23840 [00:24<00:28, 460.34 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10603/23840 [00:24<00:27, 477.91 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10678/23840 [00:24<00:27, 482.99 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10731/23840 [00:24<00:26, 490.50 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10806/23840 [00:24<00:26, 490.98 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10877/23840 [00:24<00:26, 482.12 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10932/23840 [00:24<00:26, 496.17 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10983/23840 [00:24<00:25, 497.87 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11047/23840 [00:25<00:52, 244.10 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11096/23840 [00:25<00:45, 279.45 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11146/23840 [00:25<00:40, 317.34 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11192/23840 [00:25<00:36, 344.24 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11238/23840 [00:25<00:34, 366.79 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11288/23840 [00:25<00:31, 396.16 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11340/23840 [00:26<00:29, 424.37 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11391/23840 [00:26<00:27, 445.12 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11441/23840 [00:26<00:27, 457.63 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11490/23840 [00:26<00:26, 460.58 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11542/23840 [00:26<00:25, 475.28 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11608/23840 [00:26<00:26, 454.49 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11659/23840 [00:26<00:26, 466.95 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11709/23840 [00:26<00:25, 473.58 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11778/23840 [00:26<00:26, 462.73 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11831/23840 [00:27<00:25, 478.58 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11880/23840 [00:27<00:25, 476.00 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 11935/23840 [00:27<00:24, 492.69 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 11988/23840 [00:27<00:23, 501.61 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12046/23840 [00:27<00:36, 325.34 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12094/23840 [00:27<00:33, 355.20 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12142/23840 [00:27<00:30, 379.27 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12192/23840 [00:28<00:28, 404.42 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12242/23840 [00:28<00:27, 424.96 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12289/23840 [00:28<00:26, 432.97 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12337/23840 [00:28<00:26, 442.30 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12387/23840 [00:28<00:25, 456.11 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12439/23840 [00:28<00:24, 473.01 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12490/23840 [00:28<00:23, 479.44 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12540/23840 [00:28<00:23, 482.43 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12591/23840 [00:28<00:23, 487.06 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12665/23840 [00:28<00:22, 487.54 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12718/23840 [00:29<00:22, 496.76 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12772/23840 [00:29<00:21, 504.90 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12824/23840 [00:29<00:21, 507.09 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12899/23840 [00:29<00:21, 501.61 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12974/23840 [00:29<00:21, 497.49 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13046/23840 [00:29<00:31, 345.03 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13094/23840 [00:30<00:29, 368.44 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13141/23840 [00:30<00:27, 388.63 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13192/23840 [00:30<00:25, 413.99 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13242/23840 [00:30<00:24, 433.02 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13289/23840 [00:30<00:24, 439.46 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13341/23840 [00:30<00:22, 459.60 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13390/23840 [00:30<00:22, 465.15 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13440/23840 [00:30<00:21, 473.62 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13491/23840 [00:30<00:21, 479.77 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13544/23840 [00:30<00:21, 489.10 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13619/23840 [00:31<00:20, 491.88 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13693/23840 [00:31<00:20, 489.57 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13768/23840 [00:31<00:20, 486.71 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13818/23840 [00:31<00:20, 486.34 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13889/23840 [00:31<00:20, 479.76 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13938/23840 [00:31<00:20, 481.19 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13990/23840 [00:31<00:20, 487.46 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14046/23840 [00:32<00:29, 327.63 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14094/23840 [00:32<00:27, 355.69 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14143/23840 [00:32<00:25, 381.25 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14193/23840 [00:32<00:23, 405.61 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14241/23840 [00:32<00:22, 420.66 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14287/23840 [00:32<00:22, 430.58 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14335/23840 [00:32<00:21, 441.95 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14383/23840 [00:32<00:21, 450.04 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14431/23840 [00:32<00:20, 456.50 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14484/23840 [00:33<00:19, 473.13 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14532/23840 [00:33<00:19, 471.59 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14581/23840 [00:33<00:19, 474.98 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14630/23840 [00:33<00:19, 475.90 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14680/23840 [00:33<00:19, 480.70 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14730/23840 [00:33<00:18, 484.05 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14780/23840 [00:33<00:18, 485.35 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14832/23840 [00:33<00:18, 493.67 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14883/23840 [00:33<00:18, 494.71 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 14935/23840 [00:34<00:17, 497.33 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 14987/23840 [00:34<00:17, 498.06 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15044/23840 [00:34<00:27, 318.38 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15090/23840 [00:34<00:25, 346.43 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15139/23840 [00:34<00:23, 376.26 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15188/23840 [00:34<00:21, 401.74 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15240/23840 [00:34<00:20, 428.84 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15292/23840 [00:34<00:19, 449.88 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15341/23840 [00:35<00:18, 456.05 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15393/23840 [00:35<00:17, 469.82 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15445/23840 [00:35<00:17, 478.42 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15499/23840 [00:35<00:16, 493.88 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15571/23840 [00:35<00:17, 483.75 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15624/23840 [00:35<00:16, 491.86 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15676/23840 [00:35<00:16, 495.45 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15730/23840 [00:35<00:16, 503.27 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15781/23840 [00:35<00:16, 503.01 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15835/23840 [00:36<00:15, 509.06 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15909/23840 [00:36<00:15, 499.39 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15984/23840 [00:36<00:15, 495.56 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16044/23840 [00:36<00:23, 338.02 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16094/23840 [00:36<00:21, 367.54 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16144/23840 [00:36<00:19, 393.53 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16189/23840 [00:36<00:18, 403.51 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16239/23840 [00:37<00:17, 424.00 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16289/23840 [00:37<00:17, 440.13 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16339/23840 [00:37<00:16, 449.84 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16387/23840 [00:37<00:16, 457.79 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16436/23840 [00:37<00:15, 465.22 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16486/23840 [00:37<00:15, 471.88 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16535/23840 [00:37<00:15, 472.41 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16586/23840 [00:37<00:15, 477.57 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16636/23840 [00:37<00:15, 479.61 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16697/23840 [00:38<00:23, 298.17 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16750/23840 [00:38<00:20, 341.63 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16805/23840 [00:38<00:18, 383.90 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16857/23840 [00:38<00:16, 413.23 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16906/23840 [00:38<00:16, 430.96 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16956/23840 [00:38<00:15, 445.82 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17023/23840 [00:39<00:21, 316.22 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17072/23840 [00:39<00:19, 348.08 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17120/23840 [00:39<00:17, 374.31 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17167/23840 [00:39<00:16, 395.84 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17215/23840 [00:39<00:15, 415.90 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17267/23840 [00:39<00:14, 441.33 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17316/23840 [00:39<00:14, 451.22 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17371/23840 [00:39<00:13, 475.19 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17446/23840 [00:39<00:13, 482.55 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17497/23840 [00:40<00:12, 488.64 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17549/23840 [00:40<00:12, 494.38 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17603/23840 [00:40<00:12, 502.28 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17678/23840 [00:40<00:12, 499.54 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17752/23840 [00:40<00:12, 493.70 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17804/23840 [00:40<00:12, 498.41 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17879/23840 [00:40<00:12, 494.21 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17931/23840 [00:40<00:11, 499.60 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17983/23840 [00:41<00:11, 502.06 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18048/23840 [00:41<00:16, 341.58 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18096/23840 [00:41<00:15, 365.87 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18145/23840 [00:41<00:14, 391.42 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18196/23840 [00:41<00:13, 417.66 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18244/23840 [00:41<00:12, 432.05 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18295/23840 [00:41<00:12, 449.22 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18345/23840 [00:41<00:11, 459.82 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18398/23840 [00:42<00:11, 473.15 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18447/23840 [00:42<00:11, 476.12 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18499/23840 [00:42<00:11, 485.09 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18551/23840 [00:42<00:10, 491.98 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18601/23840 [00:42<00:10, 491.19 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18654/23840 [00:42<00:10, 496.23 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18709/23840 [00:42<00:10, 509.50 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18762/23840 [00:42<00:09, 513.78 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18836/23840 [00:42<00:09, 502.59 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18887/23840 [00:43<00:09, 503.24 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18940/23840 [00:43<00:09, 505.00 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 19000/23840 [00:43<00:18, 263.03 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 19044/23840 [00:43<00:16, 291.18 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19091/23840 [00:43<00:14, 323.93 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19140/23840 [00:43<00:13, 356.40 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19190/23840 [00:43<00:11, 387.99 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19240/23840 [00:44<00:11, 413.07 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19288/23840 [00:44<00:10, 429.25 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19337/23840 [00:44<00:10, 441.04 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19384/23840 [00:44<00:10, 445.18 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19437/23840 [00:44<00:09, 467.20 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19489/23840 [00:44<00:09, 478.90 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19540/23840 [00:44<00:08, 485.02 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19614/23840 [00:44<00:08, 484.17 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19665/23840 [00:44<00:08, 487.19 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19738/23840 [00:45<00:08, 483.70 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19791/23840 [00:45<00:08, 491.42 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19859/23840 [00:45<00:08, 476.41 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19911/23840 [00:45<00:08, 483.79 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19961/23840 [00:45<00:08, 484.16 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20019/23840 [00:45<00:11, 321.96 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20065/23840 [00:45<00:10, 346.34 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20112/23840 [00:46<00:10, 371.17 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20161/23840 [00:46<00:09, 395.41 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20213/23840 [00:46<00:08, 425.09 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20263/23840 [00:46<00:08, 442.28 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20311/23840 [00:46<00:07, 448.48 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20359/23840 [00:46<00:07, 453.95 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20411/23840 [00:46<00:07, 470.97 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20460/23840 [00:46<00:07, 475.92 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20511/23840 [00:46<00:06, 482.40 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20585/23840 [00:47<00:06, 485.22 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20638/23840 [00:47<00:06, 493.75 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20689/23840 [00:47<00:06, 496.26 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20740/23840 [00:47<00:06, 498.16 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20791/23840 [00:47<00:06, 497.87 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20841/23840 [00:47<00:06, 496.16 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 20913/23840 [00:47<00:06, 487.82 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 20966/23840 [00:47<00:05, 496.55 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21021/23840 [00:48<00:08, 330.71 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21066/23840 [00:48<00:07, 352.98 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21112/23840 [00:48<00:07, 376.08 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21160/23840 [00:48<00:06, 399.93 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21210/23840 [00:48<00:06, 423.16 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21260/23840 [00:48<00:05, 440.24 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21310/23840 [00:48<00:05, 455.67 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21361/23840 [00:48<00:05, 468.48 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21435/23840 [00:48<00:05, 472.98 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21488/23840 [00:49<00:04, 484.87 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21538/23840 [00:49<00:04, 486.75 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21590/23840 [00:49<00:04, 494.36 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21641/23840 [00:49<00:04, 496.15 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21691/23840 [00:49<00:04, 494.39 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21764/23840 [00:49<00:04, 485.18 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21813/23840 [00:49<00:04, 483.23 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21864/23840 [00:49<00:04, 486.83 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21916/23840 [00:49<00:03, 492.12 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21969/23840 [00:50<00:03, 499.99 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22023/23840 [00:50<00:05, 328.37 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22068/23840 [00:50<00:05, 351.98 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22117/23840 [00:50<00:04, 380.23 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22168/23840 [00:50<00:04, 408.28 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22214/23840 [00:50<00:03, 418.41 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22264/23840 [00:51<00:05, 274.49 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22310/23840 [00:51<00:04, 309.44 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22357/23840 [00:51<00:04, 343.64 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22406/23840 [00:51<00:03, 375.57 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22454/23840 [00:51<00:03, 398.04 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22503/23840 [00:51<00:03, 418.32 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22549/23840 [00:51<00:03, 426.56 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22600/23840 [00:51<00:02, 447.63 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22648/23840 [00:51<00:02, 456.28 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22701/23840 [00:52<00:02, 475.16 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22751/23840 [00:52<00:02, 480.65 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22800/23840 [00:52<00:02, 480.47 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22850/23840 [00:52<00:02, 482.01 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22901/23840 [00:52<00:01, 485.51 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 22952/23840 [00:52<00:01, 491.11 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23018/23840 [00:52<00:02, 323.02 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23064/23840 [00:52<00:02, 349.57 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23111/23840 [00:53<00:01, 369.16 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23161/23840 [00:53<00:01, 395.08 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23210/23840 [00:53<00:01, 418.16 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23261/23840 [00:53<00:01, 441.20 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23308/23840 [00:53<00:01, 447.38 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23358/23840 [00:53<00:01, 459.38 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23406/23840 [00:53<00:00, 462.42 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23454/23840 [00:53<00:00, 466.44 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23502/23840 [00:53<00:00, 468.98 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23553/23840 [00:53<00:00, 476.82 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23605/23840 [00:54<00:00, 482.22 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23655/23840 [00:54<00:00, 482.73 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23707/23840 [00:54<00:00, 492.27 examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23758/23840 [00:54<00:00, 494.87 examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23810/23840 [00:54<00:00, 499.60 examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:54<00:00, 435.65 examples/s]
Truncating train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Truncating train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6000/23840 [00:00<00:00, 51357.46 examples/s]Truncating train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18000/23840 [00:00<00:00, 63155.13 examples/s]Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:00<00:00, 47565.60 examples/s]
Applying formatting function to eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Applying formatting function to train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Applying formatting function to train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Applying formatting function to train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Applying formatting function to eval dataset:   6%|â–‹         | 374/5960 [00:00<00:01, 3680.15 examples/s]Applying formatting function to train dataset:   1%|â–         | 325/23840 [00:00<00:07, 3191.99 examples/s]Applying formatting function to train dataset:   1%|â–         | 318/23840 [00:00<00:07, 3128.95 examples/s]Applying formatting function to train dataset:   1%|â–         | 333/23840 [00:00<00:07, 3272.91 examples/s]Applying formatting function to eval dataset:  13%|â–ˆâ–Ž        | 780/5960 [00:00<00:01, 3898.36 examples/s]Applying formatting function to train dataset:   3%|â–Ž         | 722/23840 [00:00<00:06, 3643.83 examples/s]Applying formatting function to train dataset:   3%|â–Ž         | 718/23840 [00:00<00:06, 3629.80 examples/s]Applying formatting function to train dataset:   3%|â–Ž         | 739/23840 [00:00<00:06, 3727.36 examples/s]Applying formatting function to eval dataset:  20%|â–ˆâ–ˆ        | 1198/5960 [00:00<00:01, 3116.70 examples/s]Applying formatting function to train dataset:   5%|â–Œ         | 1205/23840 [00:00<00:07, 2910.07 examples/s]Applying formatting function to train dataset:   5%|â–Œ         | 1197/23840 [00:00<00:07, 2840.39 examples/s]Applying formatting function to train dataset:   5%|â–Œ         | 1203/23840 [00:00<00:07, 2872.17 examples/s]Applying formatting function to eval dataset:  27%|â–ˆâ–ˆâ–‹       | 1603/5960 [00:00<00:01, 3426.80 examples/s]Applying formatting function to train dataset:   7%|â–‹         | 1615/23840 [00:00<00:06, 3277.88 examples/s]Applying formatting function to train dataset:   7%|â–‹         | 1598/23840 [00:00<00:06, 3198.98 examples/s]Applying formatting function to train dataset:   7%|â–‹         | 1613/23840 [00:00<00:06, 3250.42 examples/s]Applying formatting function to eval dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 2000/5960 [00:00<00:01, 2978.29 examples/s]Applying formatting function to train dataset:   8%|â–Š         | 2000/23840 [00:00<00:07, 2879.62 examples/s]Applying formatting function to train dataset:   8%|â–Š         | 2000/23840 [00:00<00:07, 2797.12 examples/s]Applying formatting function to train dataset:   8%|â–Š         | 2000/23840 [00:00<00:07, 2810.39 examples/s]Applying formatting function to eval dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2400/5960 [00:00<00:01, 3258.56 examples/s]Applying formatting function to train dataset:  10%|â–ˆ         | 2407/23840 [00:00<00:06, 3197.14 examples/s]Applying formatting function to train dataset:  10%|â–ˆ         | 2396/23840 [00:00<00:06, 3099.93 examples/s]Applying formatting function to train dataset:  10%|â–ˆ         | 2406/23840 [00:00<00:06, 3138.64 examples/s]Applying formatting function to eval dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2811/5960 [00:00<00:00, 3498.48 examples/s]Applying formatting function to train dataset:  12%|â–ˆâ–        | 2820/23840 [00:00<00:06, 3453.85 examples/s]Applying formatting function to train dataset:  12%|â–ˆâ–        | 2807/23840 [00:00<00:06, 3372.59 examples/s]Applying formatting function to train dataset:  12%|â–ˆâ–        | 2820/23840 [00:00<00:06, 3412.02 examples/s]Applying formatting function to eval dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 3196/5960 [00:00<00:00, 3076.17 examples/s]Applying formatting function to train dataset:  13%|â–ˆâ–Ž        | 3205/23840 [00:01<00:06, 3086.42 examples/s]Applying formatting function to train dataset:  13%|â–ˆâ–Ž        | 3200/23840 [00:01<00:06, 2990.97 examples/s]Applying formatting function to train dataset:  13%|â–ˆâ–Ž        | 3199/23840 [00:01<00:06, 2983.55 examples/s]Applying formatting function to eval dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3602/5960 [00:01<00:00, 3329.17 examples/s]Applying formatting function to train dataset:  15%|â–ˆâ–Œ        | 3614/23840 [00:01<00:06, 3343.48 examples/s]Applying formatting function to train dataset:  15%|â–ˆâ–Œ        | 3606/23840 [00:01<00:06, 3260.27 examples/s]Applying formatting function to train dataset:  15%|â–ˆâ–Œ        | 3599/23840 [00:01<00:06, 3237.48 examples/s]Applying formatting function to train dataset:  17%|â–ˆâ–‹        | 4000/23840 [00:01<00:06, 2888.42 examples/s]Applying formatting function to train dataset:  17%|â–ˆâ–‹        | 4000/23840 [00:01<00:07, 2808.00 examples/s]Applying formatting function to train dataset:  17%|â–ˆâ–‹        | 4000/23840 [00:01<00:07, 2792.42 examples/s]Applying formatting function to train dataset:  18%|â–ˆâ–Š        | 4410/23840 [00:01<00:06, 3179.61 examples/s]Applying formatting function to train dataset:  18%|â–ˆâ–Š        | 4404/23840 [00:01<00:06, 3098.22 examples/s]Applying formatting function to train dataset:  18%|â–ˆâ–Š        | 4402/23840 [00:01<00:06, 3078.86 examples/s]Applying formatting function to eval dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4000/5960 [00:01<00:01, 1948.01 examples/s]Applying formatting function to train dataset:  20%|â–ˆâ–ˆ        | 4824/23840 [00:01<00:05, 3424.44 examples/s]Applying formatting function to train dataset:  20%|â–ˆâ–ˆ        | 4818/23840 [00:01<00:05, 3358.51 examples/s]Applying formatting function to train dataset:  20%|â–ˆâ–ˆ        | 4814/23840 [00:01<00:05, 3337.40 examples/s]Applying formatting function to eval dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4402/5960 [00:01<00:00, 2313.23 examples/s]Applying formatting function to eval dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4811/5960 [00:01<00:00, 2670.92 examples/s]Applying formatting function to eval dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 5197/5960 [00:01<00:00, 2600.50 examples/s]Applying formatting function to train dataset:  22%|â–ˆâ–ˆâ–       | 5202/23840 [00:01<00:09, 2046.95 examples/s]Applying formatting function to train dataset:  22%|â–ˆâ–ˆâ–       | 5198/23840 [00:01<00:09, 1957.84 examples/s]Applying formatting function to train dataset:  22%|â–ˆâ–ˆâ–       | 5203/23840 [00:01<00:09, 1943.98 examples/s]Applying formatting function to eval dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5602/5960 [00:01<00:00, 2919.29 examples/s]Applying formatting function to train dataset:  24%|â–ˆâ–ˆâ–Ž       | 5607/23840 [00:01<00:07, 2411.80 examples/s]Applying formatting function to train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5599/23840 [00:02<00:07, 2316.49 examples/s]Applying formatting function to train dataset:  24%|â–ˆâ–ˆâ–Ž       | 5608/23840 [00:02<00:07, 2309.94 examples/s]Applying formatting function to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:02<00:00, 2770.99 examples/s]Applying formatting function to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:02<00:00, 2844.68 examples/s]
Adding EOS to eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Applying formatting function to train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6000/23840 [00:02<00:07, 2438.56 examples/s]Adding EOS to eval dataset:   9%|â–Š         | 521/5960 [00:00<00:01, 5139.76 examples/s]Applying formatting function to train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6000/23840 [00:02<00:07, 2345.07 examples/s]Applying formatting function to train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6000/23840 [00:02<00:07, 2336.65 examples/s]Applying formatting function to train dataset:  27%|â–ˆâ–ˆâ–‹       | 6406/23840 [00:02<00:06, 2776.54 examples/s]Applying formatting function to train dataset:  27%|â–ˆâ–ˆâ–‹       | 6397/23840 [00:02<00:06, 2671.81 examples/s]Applying formatting function to train dataset:  27%|â–ˆâ–ˆâ–‹       | 6404/23840 [00:02<00:06, 2678.66 examples/s]Applying formatting function to train dataset:  29%|â–ˆâ–ˆâ–Š       | 6815/23840 [00:02<00:05, 3077.51 examples/s]Adding EOS to eval dataset:  21%|â–ˆâ–ˆ        | 1262/5960 [00:00<00:01, 4537.74 examples/s]Applying formatting function to train dataset:  29%|â–ˆâ–ˆâ–Š       | 6807/23840 [00:02<00:05, 2987.78 examples/s]Applying formatting function to train dataset:  29%|â–ˆâ–ˆâ–Š       | 6818/23840 [00:02<00:05, 3004.06 examples/s]Adding EOS to eval dataset:  30%|â–ˆâ–ˆâ–ˆ       | 1792/5960 [00:00<00:00, 4813.49 examples/s]Applying formatting function to train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7202/23840 [00:02<00:05, 2873.09 examples/s]Applying formatting function to train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7198/23840 [00:02<00:05, 2778.51 examples/s]Applying formatting function to train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7200/23840 [00:02<00:05, 2775.03 examples/s]Applying formatting function to train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7609/23840 [00:02<00:05, 3153.14 examples/s]Adding EOS to eval dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2529/5960 [00:00<00:00, 4497.55 examples/s]Applying formatting function to train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7600/23840 [00:02<00:05, 3064.23 examples/s]Applying formatting function to train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7602/23840 [00:02<00:05, 3059.26 examples/s]Adding EOS to eval dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3000/5960 [00:00<00:00, 4308.16 examples/s]Applying formatting function to train dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 8000/23840 [00:02<00:05, 2684.30 examples/s]Adding EOS to eval dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3526/5960 [00:00<00:00, 4567.10 examples/s]Applying formatting function to train dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 8000/23840 [00:02<00:06, 2597.90 examples/s]Applying formatting function to train dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 8000/23840 [00:02<00:06, 2599.45 examples/s]Applying formatting function to train dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 8409/23840 [00:02<00:05, 2998.00 examples/s]Applying formatting function to train dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 8400/23840 [00:02<00:05, 2902.87 examples/s]Applying formatting function to train dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 8396/23840 [00:02<00:05, 2895.96 examples/s]Applying formatting function to train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8821/23840 [00:02<00:04, 3267.75 examples/s]Adding EOS to eval dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4000/5960 [00:00<00:00, 4121.40 examples/s]Applying formatting function to train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8813/23840 [00:03<00:04, 3193.93 examples/s]Applying formatting function to train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8802/23840 [00:03<00:04, 3171.31 examples/s]Adding EOS to eval dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4521/5960 [00:01<00:00, 4406.13 examples/s]Applying formatting function to train dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 9203/23840 [00:03<00:04, 2991.12 examples/s]Adding EOS to eval dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5000/5960 [00:01<00:00, 4266.68 examples/s]Applying formatting function to train dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 9201/23840 [00:03<00:05, 2900.84 examples/s]Applying formatting function to train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9609/23840 [00:03<00:04, 3250.62 examples/s]Applying formatting function to train dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 9200/23840 [00:03<00:05, 2886.45 examples/s]Adding EOS to eval dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 5526/5960 [00:01<00:00, 4532.92 examples/s]Applying formatting function to train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9605/23840 [00:03<00:04, 3169.52 examples/s]Applying formatting function to train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9599/23840 [00:03<00:04, 3143.76 examples/s]Applying formatting function to train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10000/23840 [00:03<00:04, 2983.43 examples/s]Adding EOS to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:01<00:00, 4400.83 examples/s]
Applying formatting function to train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10405/23840 [00:03<00:04, 3240.94 examples/s]Applying formatting function to train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10000/23840 [00:03<00:04, 2901.84 examples/s]Applying formatting function to train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10000/23840 [00:03<00:04, 2883.57 examples/s]Tokenizing eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Applying formatting function to train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10812/23840 [00:03<00:03, 3453.86 examples/s]Applying formatting function to train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10399/23840 [00:03<00:04, 3158.25 examples/s]Applying formatting function to train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10401/23840 [00:03<00:04, 3147.53 examples/s]Tokenizing eval dataset:   1%|          | 43/5960 [00:00<00:14, 414.43 examples/s]Applying formatting function to train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10809/23840 [00:03<00:03, 3395.43 examples/s]Applying formatting function to train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10807/23840 [00:03<00:03, 3377.12 examples/s]Tokenizing eval dataset:   2%|â–         | 94/5960 [00:00<00:12, 462.00 examples/s]Tokenizing eval dataset:   2%|â–         | 144/5960 [00:00<00:12, 471.40 examples/s]Applying formatting function to train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11203/23840 [00:03<00:06, 2073.05 examples/s]Tokenizing eval dataset:   3%|â–Ž         | 193/5960 [00:00<00:12, 476.92 examples/s]Applying formatting function to train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11608/23840 [00:04<00:05, 2432.65 examples/s]Tokenizing eval dataset:   4%|â–         | 241/5960 [00:00<00:12, 474.07 examples/s]Applying formatting function to train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11200/23840 [00:04<00:06, 1970.09 examples/s]Applying formatting function to train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11201/23840 [00:04<00:06, 1979.31 examples/s]Tokenizing eval dataset:   5%|â–         | 291/5960 [00:00<00:11, 482.06 examples/s]Applying formatting function to train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11603/23840 [00:04<00:05, 2329.49 examples/s]Applying formatting function to train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11602/23840 [00:04<00:05, 2333.58 examples/s]Applying formatting function to train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12000/23840 [00:04<00:04, 2450.94 examples/s]Applying formatting function to train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12406/23840 [00:04<00:04, 2785.15 examples/s]Tokenizing eval dataset:   6%|â–Œ         | 366/5960 [00:00<00:11, 484.73 examples/s]Applying formatting function to train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12000/23840 [00:04<00:05, 2352.81 examples/s]Applying formatting function to train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12000/23840 [00:04<00:05, 2355.71 examples/s]Applying formatting function to train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12818/23840 [00:04<00:03, 3089.56 examples/s]Tokenizing eval dataset:   7%|â–‹         | 419/5960 [00:00<00:11, 495.87 examples/s]Applying formatting function to train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12402/23840 [00:04<00:04, 2687.45 examples/s]Applying formatting function to train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12397/23840 [00:04<00:04, 2680.51 examples/s]Tokenizing eval dataset:   8%|â–Š         | 472/5960 [00:00<00:10, 502.27 examples/s]Applying formatting function to train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12814/23840 [00:04<00:03, 3006.98 examples/s]Applying formatting function to train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12806/23840 [00:04<00:03, 2992.18 examples/s]Applying formatting function to train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13202/23840 [00:04<00:03, 2875.03 examples/s]Tokenizing eval dataset:   9%|â–‰         | 526/5960 [00:01<00:10, 510.34 examples/s]Applying formatting function to train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13608/23840 [00:04<00:03, 3153.65 examples/s]Applying formatting function to train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13199/23840 [00:04<00:03, 2775.68 examples/s]Applying formatting function to train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13200/23840 [00:04<00:03, 2774.13 examples/s]Tokenizing eval dataset:  10%|â–ˆ         | 599/5960 [00:01<00:10, 497.93 examples/s]Applying formatting function to train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13603/23840 [00:04<00:03, 3064.31 examples/s]Applying formatting function to train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13601/23840 [00:04<00:03, 3056.07 examples/s]Applying formatting function to train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 14000/23840 [00:04<00:03, 2924.17 examples/s]Tokenizing eval dataset:  11%|â–ˆ         | 650/5960 [00:01<00:10, 496.99 examples/s]Applying formatting function to train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14403/23840 [00:04<00:02, 3187.91 examples/s]Applying formatting function to train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 14000/23840 [00:04<00:03, 2831.22 examples/s]Tokenizing eval dataset:  12%|â–ˆâ–        | 704/5960 [00:01<00:10, 503.47 examples/s]Applying formatting function to train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 14000/23840 [00:04<00:03, 2830.46 examples/s]Applying formatting function to train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14812/23840 [00:05<00:02, 3416.43 examples/s]Applying formatting function to train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14397/23840 [00:05<00:03, 3096.13 examples/s]Tokenizing eval dataset:  13%|â–ˆâ–Ž        | 757/5960 [00:01<00:10, 507.78 examples/s]Applying formatting function to train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14399/23840 [00:05<00:03, 3099.78 examples/s]Applying formatting function to train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14809/23840 [00:05<00:02, 3350.50 examples/s]Tokenizing eval dataset:  14%|â–ˆâ–Ž        | 810/5960 [00:01<00:10, 510.16 examples/s]Applying formatting function to train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14807/23840 [00:05<00:02, 3343.93 examples/s]Applying formatting function to train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15206/23840 [00:05<00:03, 2598.99 examples/s]Tokenizing eval dataset:  15%|â–ˆâ–        | 887/5960 [00:01<00:09, 508.23 examples/s]Applying formatting function to train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15613/23840 [00:05<00:02, 2918.41 examples/s]Applying formatting function to train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15202/23840 [00:05<00:03, 2527.13 examples/s]Applying formatting function to train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15201/23840 [00:05<00:03, 2518.00 examples/s]Tokenizing eval dataset:  16%|â–ˆâ–Œ        | 938/5960 [00:01<00:09, 505.26 examples/s]Applying formatting function to train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16000/23840 [00:05<00:02, 2783.53 examples/s]Applying formatting function to train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15601/23840 [00:05<00:02, 2837.78 examples/s]Applying formatting function to train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15601/23840 [00:05<00:02, 2830.99 examples/s]Tokenizing eval dataset:  17%|â–ˆâ–‹        | 990/5960 [00:01<00:09, 506.78 examples/s]Applying formatting function to train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16406/23840 [00:05<00:02, 3076.55 examples/s]Applying formatting function to train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16000/23840 [00:05<00:02, 2683.09 examples/s]Applying formatting function to train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16000/23840 [00:05<00:02, 2691.05 examples/s]Applying formatting function to train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16816/23840 [00:05<00:02, 3326.58 examples/s]Applying formatting function to train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16390/23840 [00:05<00:02, 2954.47 examples/s]Applying formatting function to train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16401/23840 [00:05<00:02, 2984.54 examples/s]Tokenizing eval dataset:  18%|â–ˆâ–Š        | 1049/5960 [00:02<00:14, 333.69 examples/s]Applying formatting function to train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16792/23840 [00:05<00:02, 3209.54 examples/s]Applying formatting function to train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16809/23840 [00:05<00:02, 3247.37 examples/s]Tokenizing eval dataset:  18%|â–ˆâ–Š        | 1097/5960 [00:02<00:13, 361.33 examples/s]Tokenizing eval dataset:  19%|â–ˆâ–‰        | 1145/5960 [00:02<00:12, 385.76 examples/s]Applying formatting function to train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17203/23840 [00:06<00:03, 2032.49 examples/s]Tokenizing eval dataset:  20%|â–ˆâ–ˆ        | 1194/5960 [00:02<00:11, 409.96 examples/s]Applying formatting function to train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17609/23840 [00:06<00:02, 2395.39 examples/s]Tokenizing eval dataset:  21%|â–ˆâ–ˆ        | 1242/5960 [00:02<00:11, 426.39 examples/s]Applying formatting function to train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17197/23840 [00:06<00:03, 1945.61 examples/s]Applying formatting function to train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17198/23840 [00:06<00:03, 1918.02 examples/s]Applying formatting function to train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18000/23840 [00:06<00:02, 2423.66 examples/s]Tokenizing eval dataset:  22%|â–ˆâ–ˆâ–       | 1295/5960 [00:02<00:10, 441.75 examples/s]Applying formatting function to train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17597/23840 [00:06<00:02, 2300.53 examples/s]Applying formatting function to train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17594/23840 [00:06<00:02, 2264.69 examples/s]Applying formatting function to train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18412/23840 [00:06<00:01, 2773.53 examples/s]Tokenizing eval dataset:  23%|â–ˆâ–ˆâ–Ž       | 1347/5960 [00:02<00:10, 460.14 examples/s]Applying formatting function to train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18824/23840 [00:06<00:01, 3077.53 examples/s]Applying formatting function to train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18000/23840 [00:06<00:02, 2333.40 examples/s]Applying formatting function to train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18000/23840 [00:06<00:02, 2295.59 examples/s]Tokenizing eval dataset:  24%|â–ˆâ–ˆâ–       | 1419/5960 [00:03<00:09, 464.05 examples/s]Applying formatting function to train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18396/23840 [00:06<00:02, 2657.26 examples/s]Applying formatting function to train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18395/23840 [00:06<00:02, 2620.27 examples/s]Applying formatting function to train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19202/23840 [00:06<00:01, 2868.52 examples/s]Tokenizing eval dataset:  25%|â–ˆâ–ˆâ–       | 1471/5960 [00:03<00:09, 476.46 examples/s]Applying formatting function to train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18804/23840 [00:06<00:01, 2971.40 examples/s]Applying formatting function to train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18797/23840 [00:06<00:01, 2926.56 examples/s]Applying formatting function to train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19609/23840 [00:06<00:01, 3149.86 examples/s]Tokenizing eval dataset:  26%|â–ˆâ–ˆâ–Œ       | 1522/5960 [00:03<00:09, 481.62 examples/s]Applying formatting function to train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19197/23840 [00:06<00:01, 2766.52 examples/s]Applying formatting function to train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19197/23840 [00:06<00:01, 2722.88 examples/s]Applying formatting function to train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20000/23840 [00:06<00:01, 2915.81 examples/s]Tokenizing eval dataset:  27%|â–ˆâ–ˆâ–‹       | 1593/5960 [00:03<00:09, 476.45 examples/s]Applying formatting function to train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19595/23840 [00:07<00:01, 3044.21 examples/s]Applying formatting function to train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19592/23840 [00:07<00:01, 2998.20 examples/s]Applying formatting function to train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20403/23840 [00:07<00:01, 3180.36 examples/s]Tokenizing eval dataset:  28%|â–ˆâ–ˆâ–Š       | 1664/5960 [00:03<00:09, 473.21 examples/s]Applying formatting function to train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19997/23840 [00:07<00:01, 3254.13 examples/s]Applying formatting function to train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20815/23840 [00:07<00:00, 3417.17 examples/s]Applying formatting function to train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20000/23840 [00:07<00:01, 2840.90 examples/s]Tokenizing eval dataset:  29%|â–ˆâ–ˆâ–‰       | 1716/5960 [00:03<00:08, 479.88 examples/s]Applying formatting function to train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20393/23840 [00:07<00:01, 3093.54 examples/s]Applying formatting function to train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21202/23840 [00:07<00:00, 3079.23 examples/s]Applying formatting function to train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20395/23840 [00:07<00:01, 2909.48 examples/s]Tokenizing eval dataset:  30%|â–ˆâ–ˆâ–ˆ       | 1789/5960 [00:03<00:08, 478.62 examples/s]Applying formatting function to train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20802/23840 [00:07<00:00, 3339.51 examples/s]Applying formatting function to train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21609/23840 [00:07<00:00, 3323.82 examples/s]Applying formatting function to train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20800/23840 [00:07<00:00, 3178.60 examples/s]Tokenizing eval dataset:  31%|â–ˆâ–ˆâ–ˆ       | 1838/5960 [00:03<00:08, 480.05 examples/s]Applying formatting function to train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21198/23840 [00:07<00:00, 2995.63 examples/s]Applying formatting function to train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22000/23840 [00:07<00:00, 3028.28 examples/s]Applying formatting function to train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21198/23840 [00:07<00:00, 2884.89 examples/s]Tokenizing eval dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 1894/5960 [00:04<00:08, 496.20 examples/s]Applying formatting function to train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21598/23840 [00:07<00:00, 3236.81 examples/s]Applying formatting function to train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22408/23840 [00:07<00:00, 3285.46 examples/s]Applying formatting function to train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21591/23840 [00:07<00:00, 3129.10 examples/s]Tokenizing eval dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1946/5960 [00:04<00:08, 500.34 examples/s]Applying formatting function to train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22819/23840 [00:07<00:00, 3497.03 examples/s]Applying formatting function to train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21995/23840 [00:07<00:00, 3356.44 examples/s]Tokenizing eval dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1998/5960 [00:04<00:07, 500.64 examples/s]Applying formatting function to train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22000/23840 [00:07<00:00, 2953.21 examples/s]Applying formatting function to train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22395/23840 [00:07<00:00, 3191.29 examples/s]Applying formatting function to train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22396/23840 [00:07<00:00, 2963.61 examples/s]Applying formatting function to train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22802/23840 [00:08<00:00, 3413.65 examples/s]Applying formatting function to train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22798/23840 [00:08<00:00, 3215.57 examples/s]Applying formatting function to train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23203/23840 [00:08<00:00, 2080.64 examples/s]Tokenizing eval dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 2066/5960 [00:04<00:11, 325.10 examples/s]Applying formatting function to train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23611/23840 [00:08<00:00, 2444.54 examples/s]Tokenizing eval dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 2116/5960 [00:04<00:10, 356.68 examples/s]Applying formatting function to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:08<00:00, 2852.41 examples/s]
Tokenizing eval dataset:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 2164/5960 [00:04<00:09, 381.91 examples/s]Adding EOS to train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Applying formatting function to train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23198/23840 [00:08<00:00, 1985.93 examples/s]Adding EOS to train dataset:   2%|â–         | 533/23840 [00:00<00:04, 5257.23 examples/s]Tokenizing eval dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 2211/5960 [00:04<00:09, 400.67 examples/s]Applying formatting function to train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23198/23840 [00:08<00:00, 1934.48 examples/s]Applying formatting function to train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23597/23840 [00:08<00:00, 2336.68 examples/s]Applying formatting function to train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23593/23840 [00:08<00:00, 2279.23 examples/s]Tokenizing eval dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 2258/5960 [00:05<00:08, 415.27 examples/s]Adding EOS to train dataset:   5%|â–Œ         | 1270/23840 [00:00<00:04, 4634.75 examples/s]Applying formatting function to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:08<00:00, 2757.12 examples/s]
Adding EOS to train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Tokenizing eval dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 2309/5960 [00:05<00:08, 437.38 examples/s]Applying formatting function to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:08<00:00, 2746.81 examples/s]
Adding EOS to train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Adding EOS to train dataset:   8%|â–Š         | 1805/23840 [00:00<00:04, 4899.65 examples/s]Adding EOS to train dataset:   2%|â–         | 523/23840 [00:00<00:04, 5152.24 examples/s]Tokenizing eval dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 2360/5960 [00:05<00:07, 454.17 examples/s]Adding EOS to train dataset:   2%|â–         | 514/23840 [00:00<00:04, 5065.05 examples/s]Tokenizing eval dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2413/5960 [00:05<00:07, 472.76 examples/s]Adding EOS to train dataset:  11%|â–ˆ         | 2541/23840 [00:00<00:04, 4524.68 examples/s]Adding EOS to train dataset:   5%|â–Œ         | 1262/23840 [00:00<00:04, 4533.96 examples/s]Adding EOS to train dataset:   5%|â–Œ         | 1260/23840 [00:00<00:05, 4418.39 examples/s]Tokenizing eval dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2464/5960 [00:05<00:07, 481.72 examples/s]Adding EOS to train dataset:   7%|â–‹         | 1785/23840 [00:00<00:04, 4787.47 examples/s]Adding EOS to train dataset:   7%|â–‹         | 1778/23840 [00:00<00:04, 4694.76 examples/s]Tokenizing eval dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2517/5960 [00:05<00:06, 492.09 examples/s]Adding EOS to train dataset:  14%|â–ˆâ–Ž        | 3270/23840 [00:00<00:04, 4450.78 examples/s]Adding EOS to train dataset:  16%|â–ˆâ–Œ        | 3804/23840 [00:00<00:04, 4672.56 examples/s]Adding EOS to train dataset:  11%|â–ˆ         | 2530/23840 [00:00<00:04, 4420.25 examples/s]Adding EOS to train dataset:   9%|â–‰         | 2263/23840 [00:00<00:05, 4183.09 examples/s]Tokenizing eval dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2593/5960 [00:05<00:06, 493.99 examples/s]Adding EOS to train dataset:  12%|â–ˆâ–        | 2784/23840 [00:00<00:04, 4490.03 examples/s]Tokenizing eval dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2644/5960 [00:05<00:06, 496.80 examples/s]Adding EOS to train dataset:  13%|â–ˆâ–Ž        | 3000/23840 [00:00<00:04, 4228.38 examples/s]Adding EOS to train dataset:  19%|â–ˆâ–‰        | 4538/23840 [00:01<00:04, 4343.11 examples/s]Adding EOS to train dataset:  15%|â–ˆâ–        | 3527/23840 [00:00<00:04, 4506.35 examples/s]Tokenizing eval dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2698/5960 [00:05<00:06, 504.69 examples/s]Adding EOS to train dataset:  14%|â–ˆâ–Ž        | 3263/23840 [00:00<00:04, 4211.83 examples/s]Adding EOS to train dataset:  21%|â–ˆâ–ˆ        | 5000/23840 [00:01<00:04, 4231.75 examples/s]Adding EOS to train dataset:  16%|â–ˆâ–Œ        | 3781/23840 [00:00<00:04, 4483.58 examples/s]Adding EOS to train dataset:  17%|â–ˆâ–‹        | 4000/23840 [00:00<00:04, 4054.16 examples/s]Adding EOS to train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5535/23840 [00:01<00:04, 4501.74 examples/s]Tokenizing eval dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2773/5960 [00:06<00:06, 498.08 examples/s]Adding EOS to train dataset:  19%|â–ˆâ–‰        | 4527/23840 [00:01<00:04, 4370.24 examples/s]Adding EOS to train dataset:  18%|â–ˆâ–Š        | 4263/23840 [00:00<00:04, 4004.50 examples/s]Adding EOS to train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6000/23840 [00:01<00:04, 4334.50 examples/s]Tokenizing eval dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2847/5960 [00:06<00:06, 494.71 examples/s]Adding EOS to train dataset:  20%|â–ˆâ–ˆ        | 4782/23840 [00:01<00:04, 4317.03 examples/s]Adding EOS to train dataset:  21%|â–ˆâ–ˆ        | 5000/23840 [00:01<00:04, 4214.21 examples/s]Adding EOS to train dataset:  27%|â–ˆâ–ˆâ–‹       | 6536/23840 [00:01<00:03, 4598.92 examples/s]Tokenizing eval dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2900/5960 [00:06<00:06, 501.80 examples/s]Adding EOS to train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5526/23840 [00:01<00:04, 4492.46 examples/s]Adding EOS to train dataset:  22%|â–ˆâ–ˆâ–       | 5263/23840 [00:01<00:04, 4146.27 examples/s]Tokenizing eval dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2951/5960 [00:06<00:05, 501.82 examples/s]Adding EOS to train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7269/23840 [00:01<00:03, 4503.99 examples/s]Adding EOS to train dataset:  24%|â–ˆâ–ˆâ–       | 5782/23840 [00:01<00:04, 4424.21 examples/s]Adding EOS to train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6000/23840 [00:01<00:04, 4274.96 examples/s]Adding EOS to train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7801/23840 [00:01<00:03, 4701.96 examples/s]Adding EOS to train dataset:  27%|â–ˆâ–ˆâ–‹       | 6528/23840 [00:01<00:03, 4545.02 examples/s]Adding EOS to train dataset:  26%|â–ˆâ–ˆâ–‹       | 6263/23840 [00:01<00:04, 4209.00 examples/s]Adding EOS to train dataset:  28%|â–ˆâ–ˆâ–Š       | 6784/23840 [00:01<00:03, 4473.96 examples/s]Adding EOS to train dataset:  29%|â–ˆâ–ˆâ–‰       | 7000/23840 [00:01<00:03, 4318.76 examples/s]Adding EOS to train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8539/23840 [00:01<00:03, 4197.20 examples/s]Tokenizing eval dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3022/5960 [00:06<00:08, 330.02 examples/s]Adding EOS to train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7527/23840 [00:01<00:03, 4574.50 examples/s]Adding EOS to train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7262/23840 [00:01<00:03, 4236.37 examples/s]Tokenizing eval dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3073/5960 [00:06<00:07, 362.03 examples/s]Adding EOS to train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9000/23840 [00:02<00:03, 4124.09 examples/s]Adding EOS to train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7777/23840 [00:01<00:03, 4478.68 examples/s]Adding EOS to train dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 9535/23840 [00:02<00:03, 4410.29 examples/s]Tokenizing eval dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3123/5960 [00:06<00:07, 388.67 examples/s]Adding EOS to train dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 8000/23840 [00:01<00:04, 3906.06 examples/s]Tokenizing eval dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 3173/5960 [00:07<00:06, 411.75 examples/s]Adding EOS to train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8524/23840 [00:01<00:03, 4242.16 examples/s]Adding EOS to train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10000/23840 [00:02<00:03, 4278.27 examples/s]Adding EOS to train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 8263/23840 [00:01<00:04, 3825.35 examples/s]Tokenizing eval dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3225/5960 [00:07<00:06, 435.18 examples/s]Adding EOS to train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10534/23840 [00:02<00:02, 4549.00 examples/s]Adding EOS to train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9000/23840 [00:02<00:03, 4118.64 examples/s]Adding EOS to train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8782/23840 [00:02<00:03, 4163.00 examples/s]Tokenizing eval dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3277/5960 [00:07<00:05, 453.59 examples/s]Adding EOS to train dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 9531/23840 [00:02<00:03, 4428.07 examples/s]Adding EOS to train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9263/23840 [00:02<00:03, 4039.12 examples/s]Adding EOS to train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11269/23840 [00:02<00:02, 4461.21 examples/s]Tokenizing eval dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 3326/5960 [00:07<00:05, 461.45 examples/s]Adding EOS to train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10000/23840 [00:02<00:03, 4240.15 examples/s]Adding EOS to train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9782/23840 [00:02<00:03, 4334.79 examples/s]Adding EOS to train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11802/23840 [00:02<00:02, 4670.84 examples/s]Tokenizing eval dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 3378/5960 [00:07<00:05, 474.26 examples/s]Adding EOS to train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10532/23840 [00:02<00:02, 4526.85 examples/s]Adding EOS to train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10263/23840 [00:02<00:03, 4166.70 examples/s]Adding EOS to train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12537/23840 [00:02<00:02, 4551.19 examples/s]Tokenizing eval dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3450/5960 [00:07<00:05, 471.94 examples/s]Adding EOS to train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 11000/23840 [00:02<00:02, 4291.90 examples/s]Adding EOS to train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10780/23840 [00:02<00:02, 4431.20 examples/s]Tokenizing eval dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3500/5960 [00:07<00:05, 477.18 examples/s]Adding EOS to train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11528/23840 [00:02<00:02, 4557.68 examples/s]Adding EOS to train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13270/23840 [00:02<00:02, 4475.54 examples/s]Adding EOS to train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11261/23840 [00:02<00:02, 4209.99 examples/s]Tokenizing eval dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3550/5960 [00:07<00:05, 481.66 examples/s]Adding EOS to train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12000/23840 [00:02<00:02, 4318.93 examples/s]Adding EOS to train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11776/23840 [00:02<00:02, 4458.18 examples/s]Adding EOS to train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13805/23840 [00:03<00:02, 4670.05 examples/s]Adding EOS to train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12525/23840 [00:02<00:02, 4570.41 examples/s]Adding EOS to train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12263/23840 [00:02<00:02, 4239.38 examples/s]Adding EOS to train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14540/23840 [00:03<00:02, 4558.72 examples/s]Adding EOS to train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13000/23840 [00:02<00:02, 4327.68 examples/s]Adding EOS to train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12780/23840 [00:02<00:02, 4486.44 examples/s]Tokenizing eval dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3601/5960 [00:08<00:07, 300.42 examples/s]Adding EOS to train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13528/23840 [00:03<00:02, 4584.50 examples/s]Adding EOS to train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13261/23840 [00:03<00:02, 4248.76 examples/s]Tokenizing eval dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3656/5960 [00:08<00:06, 348.82 examples/s]Adding EOS to train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15277/23840 [00:03<00:02, 3878.45 examples/s]Adding EOS to train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 14000/23840 [00:03<00:02, 4328.89 examples/s]Adding EOS to train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13780/23840 [00:03<00:02, 4497.62 examples/s]Tokenizing eval dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3709/5960 [00:08<00:05, 386.21 examples/s]Adding EOS to train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15817/23840 [00:03<00:01, 4175.94 examples/s]Adding EOS to train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14524/23840 [00:03<00:02, 4575.77 examples/s]Tokenizing eval dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3757/5960 [00:08<00:05, 408.02 examples/s]Adding EOS to train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14263/23840 [00:03<00:02, 4265.92 examples/s]Adding EOS to train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16270/23840 [00:03<00:01, 4106.77 examples/s]Tokenizing eval dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3809/5960 [00:08<00:04, 434.93 examples/s]Adding EOS to train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14781/23840 [00:03<00:02, 4509.79 examples/s]Adding EOS to train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15000/23840 [00:03<00:02, 3578.62 examples/s]Adding EOS to train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16807/23840 [00:03<00:01, 4397.04 examples/s]Tokenizing eval dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3859/5960 [00:08<00:04, 451.51 examples/s]Adding EOS to train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15529/23840 [00:03<00:02, 3981.55 examples/s]Tokenizing eval dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3912/5960 [00:08<00:04, 472.27 examples/s]Adding EOS to train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15265/23840 [00:03<00:02, 3523.79 examples/s]Adding EOS to train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17539/23840 [00:03<00:01, 4386.53 examples/s]Adding EOS to train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16000/23840 [00:03<00:01, 3936.87 examples/s]Tokenizing eval dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3967/5960 [00:08<00:04, 489.43 examples/s]Adding EOS to train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15786/23840 [00:03<00:02, 3914.89 examples/s]Adding EOS to train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18000/23840 [00:04<00:01, 4261.30 examples/s]Adding EOS to train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16529/23840 [00:03<00:01, 4279.01 examples/s]Adding EOS to train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16262/23840 [00:03<00:01, 3883.12 examples/s]Adding EOS to train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18538/23840 [00:04<00:01, 4530.85 examples/s]Adding EOS to train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17000/23840 [00:03<00:01, 4133.15 examples/s]Adding EOS to train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16782/23840 [00:03<00:01, 4211.39 examples/s]Adding EOS to train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17529/23840 [00:04<00:01, 4435.30 examples/s]Tokenizing eval dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4026/5960 [00:09<00:05, 327.98 examples/s]Adding EOS to train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19270/23840 [00:04<00:01, 4471.38 examples/s]Adding EOS to train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17262/23840 [00:04<00:01, 4081.40 examples/s]Tokenizing eval dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4075/5960 [00:09<00:05, 359.47 examples/s]Adding EOS to train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19804/23840 [00:04<00:00, 4676.05 examples/s]Adding EOS to train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18000/23840 [00:04<00:01, 4236.84 examples/s]Adding EOS to train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17780/23840 [00:04<00:01, 4365.34 examples/s]Tokenizing eval dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 4127/5960 [00:09<00:04, 394.64 examples/s]Adding EOS to train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18530/23840 [00:04<00:01, 4518.86 examples/s]Adding EOS to train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20539/23840 [00:04<00:00, 4564.57 examples/s]Adding EOS to train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18263/23840 [00:04<00:01, 4189.41 examples/s]Tokenizing eval dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 4177/5960 [00:09<00:04, 418.56 examples/s]Adding EOS to train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 19000/23840 [00:04<00:01, 4290.19 examples/s]Adding EOS to train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18783/23840 [00:04<00:01, 4453.36 examples/s]Tokenizing eval dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 4226/5960 [00:09<00:03, 435.54 examples/s]Adding EOS to train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21270/23840 [00:04<00:00, 4490.67 examples/s]Adding EOS to train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19534/23840 [00:04<00:00, 4572.78 examples/s]Tokenizing eval dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4275/5960 [00:09<00:03, 448.76 examples/s]Adding EOS to train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19264/23840 [00:04<00:01, 4247.24 examples/s]Adding EOS to train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21805/23840 [00:04<00:00, 4682.95 examples/s]Tokenizing eval dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 4323/5960 [00:09<00:03, 455.15 examples/s]Adding EOS to train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19782/23840 [00:04<00:00, 4495.03 examples/s]Adding EOS to train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20265/23840 [00:04<00:00, 4428.90 examples/s]Adding EOS to train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22539/23840 [00:05<00:00, 4569.39 examples/s]Tokenizing eval dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 4376/5960 [00:09<00:03, 473.76 examples/s]Adding EOS to train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20793/23840 [00:04<00:00, 4636.51 examples/s]Adding EOS to train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20262/23840 [00:04<00:00, 4279.98 examples/s]Tokenizing eval dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4427/5960 [00:09<00:03, 481.50 examples/s]Adding EOS to train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20782/23840 [00:04<00:00, 4525.12 examples/s]Adding EOS to train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23269/23840 [00:05<00:00, 4494.73 examples/s]Adding EOS to train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21526/23840 [00:04<00:00, 4479.29 examples/s]Tokenizing eval dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4479/5960 [00:10<00:03, 488.31 examples/s]Adding EOS to train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21264/23840 [00:04<00:00, 4295.45 examples/s]Adding EOS to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23804/23840 [00:05<00:00, 4682.21 examples/s]Adding EOS to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:05<00:00, 4443.53 examples/s]
Adding EOS to train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22000/23840 [00:05<00:00, 4287.63 examples/s]Tokenizing eval dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4530/5960 [00:10<00:02, 490.70 examples/s]Adding EOS to train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21784/23840 [00:05<00:00, 4536.84 examples/s]Adding EOS to train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22528/23840 [00:05<00:00, 4527.21 examples/s]Tokenizing eval dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4584/5960 [00:10<00:02, 501.22 examples/s]Tokenizing train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Adding EOS to train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22263/23840 [00:05<00:00, 4298.42 examples/s]Tokenizing train dataset:   0%|          | 44/23840 [00:00<00:55, 427.00 examples/s]Adding EOS to train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23000/23840 [00:05<00:00, 4306.70 examples/s]Tokenizing eval dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4659/5960 [00:10<00:02, 496.09 examples/s]Adding EOS to train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22783/23840 [00:05<00:00, 4539.93 examples/s]Tokenizing train dataset:   0%|          | 94/23840 [00:00<00:51, 461.39 examples/s]Adding EOS to train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23527/23840 [00:05<00:00, 4552.14 examples/s]Tokenizing eval dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4711/5960 [00:10<00:02, 500.81 examples/s]Adding EOS to train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23263/23840 [00:05<00:00, 4301.93 examples/s]Adding EOS to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:05<00:00, 4338.39 examples/s]
Tokenizing eval dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4765/5960 [00:10<00:02, 509.01 examples/s]Tokenizing train dataset:   1%|          | 164/23840 [00:00<00:51, 460.90 examples/s]Adding EOS to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23780/23840 [00:05<00:00, 4534.32 examples/s]Tokenizing train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Adding EOS to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:05<00:00, 4265.60 examples/s]
Tokenizing train dataset:   1%|          | 214/23840 [00:00<00:49, 472.96 examples/s]Tokenizing eval dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4840/5960 [00:10<00:02, 502.68 examples/s]Tokenizing train dataset:   0%|          | 43/23840 [00:00<00:57, 413.40 examples/s]Tokenizing train dataset:   1%|          | 262/23840 [00:00<00:49, 472.95 examples/s]Tokenizing train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Tokenizing eval dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4892/5960 [00:10<00:02, 503.37 examples/s]Tokenizing train dataset:   0%|          | 90/23840 [00:00<00:53, 444.11 examples/s]Tokenizing train dataset:   1%|â–         | 312/23840 [00:00<00:49, 479.03 examples/s]Tokenizing train dataset:   0%|          | 43/23840 [00:00<00:57, 416.71 examples/s]Tokenizing eval dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4943/5960 [00:10<00:02, 504.26 examples/s]Tokenizing train dataset:   1%|          | 135/23840 [00:00<00:53, 441.53 examples/s]Tokenizing train dataset:   0%|          | 91/23840 [00:00<00:53, 447.97 examples/s]Tokenizing train dataset:   2%|â–         | 383/23840 [00:00<00:49, 474.52 examples/s]Tokenizing train dataset:   1%|          | 182/23840 [00:00<00:52, 448.97 examples/s]Tokenizing train dataset:   1%|          | 136/23840 [00:00<00:53, 445.06 examples/s]Tokenizing train dataset:   2%|â–         | 434/23840 [00:00<00:48, 482.94 examples/s]Tokenizing train dataset:   1%|          | 231/23840 [00:00<00:51, 458.64 examples/s]Tokenizing train dataset:   1%|          | 184/23840 [00:00<00:52, 454.51 examples/s]Tokenizing train dataset:   2%|â–         | 484/23840 [00:01<00:48, 485.22 examples/s]Tokenizing train dataset:   1%|          | 279/23840 [00:00<00:50, 463.52 examples/s]Tokenizing eval dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5000/5960 [00:11<00:03, 313.29 examples/s]Tokenizing train dataset:   1%|          | 233/23840 [00:00<00:50, 463.43 examples/s]Tokenizing train dataset:   2%|â–         | 537/23840 [00:01<00:47, 494.03 examples/s]Tokenizing train dataset:   1%|â–         | 327/23840 [00:00<00:50, 467.27 examples/s]Tokenizing eval dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5048/5960 [00:11<00:02, 344.42 examples/s]Tokenizing train dataset:   1%|          | 281/23840 [00:00<00:50, 465.88 examples/s]Tokenizing train dataset:   2%|â–         | 589/23840 [00:01<00:46, 498.41 examples/s]Tokenizing eval dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 5095/5960 [00:11<00:02, 369.70 examples/s]Tokenizing train dataset:   1%|â–         | 330/23840 [00:00<00:50, 468.29 examples/s]Tokenizing train dataset:   2%|â–         | 397/23840 [00:00<00:50, 462.00 examples/s]Tokenizing train dataset:   3%|â–Ž         | 641/23840 [00:01<00:46, 503.81 examples/s]Tokenizing eval dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 5142/5960 [00:11<00:02, 391.29 examples/s]Tokenizing train dataset:   2%|â–         | 448/23840 [00:00<00:49, 472.86 examples/s]Tokenizing train dataset:   2%|â–         | 400/23840 [00:00<00:50, 463.58 examples/s]Tokenizing eval dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 5192/5960 [00:11<00:01, 415.33 examples/s]Tokenizing train dataset:   3%|â–Ž         | 720/23840 [00:01<00:45, 509.49 examples/s]Tokenizing train dataset:   2%|â–         | 499/23840 [00:01<00:48, 479.11 examples/s]Tokenizing train dataset:   2%|â–         | 452/23840 [00:00<00:48, 477.82 examples/s]Tokenizing eval dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 5242/5960 [00:11<00:01, 434.36 examples/s]Tokenizing train dataset:   3%|â–Ž         | 771/23840 [00:01<00:45, 505.40 examples/s]Tokenizing train dataset:   2%|â–         | 550/23840 [00:01<00:48, 483.53 examples/s]Tokenizing train dataset:   2%|â–         | 502/23840 [00:01<00:48, 481.39 examples/s]Tokenizing eval dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 5291/5960 [00:11<00:01, 446.66 examples/s]Tokenizing train dataset:   3%|â–Ž         | 822/23840 [00:01<00:45, 504.74 examples/s]Tokenizing train dataset:   3%|â–Ž         | 602/23840 [00:01<00:47, 489.00 examples/s]Tokenizing train dataset:   2%|â–         | 552/23840 [00:01<00:48, 484.86 examples/s]Tokenizing eval dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 5343/5960 [00:12<00:01, 463.17 examples/s]Tokenizing train dataset:   4%|â–Ž         | 875/23840 [00:01<00:45, 509.77 examples/s]Tokenizing train dataset:   3%|â–Ž         | 654/23840 [00:01<00:47, 492.00 examples/s]Tokenizing train dataset:   3%|â–Ž         | 604/23840 [00:01<00:47, 492.08 examples/s]Tokenizing eval dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 5393/5960 [00:12<00:01, 469.93 examples/s]Tokenizing train dataset:   3%|â–Ž         | 706/23840 [00:01<00:46, 495.58 examples/s]Tokenizing train dataset:   4%|â–         | 951/23840 [00:01<00:45, 502.58 examples/s]Tokenizing train dataset:   3%|â–Ž         | 655/23840 [00:01<00:46, 494.43 examples/s]Tokenizing eval dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5445/5960 [00:12<00:01, 481.32 examples/s]Tokenizing train dataset:   3%|â–Ž         | 759/23840 [00:01<00:45, 502.31 examples/s]Tokenizing train dataset:   3%|â–Ž         | 708/23840 [00:01<00:46, 498.61 examples/s]Tokenizing eval dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5498/5960 [00:12<00:00, 491.30 examples/s]Tokenizing train dataset:   3%|â–Ž         | 761/23840 [00:01<00:45, 505.43 examples/s]Tokenizing train dataset:   3%|â–Ž         | 833/23840 [00:01<00:46, 496.53 examples/s]Tokenizing eval dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 5548/5960 [00:12<00:00, 491.57 examples/s]Tokenizing train dataset:   4%|â–Ž         | 883/23840 [00:01<00:46, 497.24 examples/s]Tokenizing train dataset:   4%|â–         | 1021/23840 [00:02<01:05, 346.12 examples/s]Tokenizing eval dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5601/5960 [00:12<00:00, 497.84 examples/s]Tokenizing train dataset:   4%|â–Ž         | 836/23840 [00:01<00:46, 498.79 examples/s]Tokenizing train dataset:   4%|â–         | 1067/23840 [00:02<01:02, 366.36 examples/s]Tokenizing train dataset:   4%|â–         | 935/23840 [00:01<00:45, 499.74 examples/s]Tokenizing train dataset:   4%|â–Ž         | 887/23840 [00:01<00:45, 499.24 examples/s]Tokenizing eval dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5675/5960 [00:12<00:00, 491.82 examples/s]Tokenizing train dataset:   5%|â–         | 1113/23840 [00:02<00:59, 384.12 examples/s]Tokenizing train dataset:   4%|â–         | 939/23840 [00:01<00:45, 502.11 examples/s]Tokenizing eval dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5726/5960 [00:12<00:00, 493.67 examples/s]Tokenizing train dataset:   5%|â–         | 1161/23840 [00:02<00:56, 403.63 examples/s]Tokenizing eval dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5777/5960 [00:12<00:00, 494.08 examples/s]Tokenizing train dataset:   5%|â–Œ         | 1209/23840 [00:02<00:53, 419.89 examples/s]Tokenizing train dataset:   4%|â–         | 1000/23840 [00:02<01:08, 334.02 examples/s]Tokenizing eval dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5829/5960 [00:13<00:00, 500.27 examples/s]Tokenizing train dataset:   5%|â–Œ         | 1257/23840 [00:02<00:52, 433.98 examples/s]Tokenizing train dataset:   4%|â–         | 1043/23840 [00:02<01:04, 351.14 examples/s]Tokenizing train dataset:   4%|â–         | 1000/23840 [00:02<01:09, 329.35 examples/s]Tokenizing eval dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5885/5960 [00:13<00:00, 512.26 examples/s]Tokenizing train dataset:   5%|â–Œ         | 1309/23840 [00:02<00:49, 455.23 examples/s]Tokenizing train dataset:   5%|â–         | 1087/23840 [00:02<01:01, 368.32 examples/s]Tokenizing train dataset:   4%|â–         | 1044/23840 [00:02<01:05, 349.91 examples/s]Tokenizing eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5939/5960 [00:13<00:00, 517.48 examples/s]Tokenizing train dataset:   6%|â–Œ         | 1358/23840 [00:02<00:48, 460.82 examples/s]Tokenizing train dataset:   5%|â–         | 1133/23840 [00:02<00:58, 388.69 examples/s]Tokenizing train dataset:   5%|â–         | 1089/23840 [00:02<01:01, 370.81 examples/s]Tokenizing train dataset:   5%|â–         | 1179/23840 [00:02<00:56, 402.09 examples/s]Tokenizing train dataset:   6%|â–Œ         | 1431/23840 [00:03<00:48, 466.58 examples/s]Tokenizing train dataset:   5%|â–         | 1137/23840 [00:02<00:57, 393.65 examples/s]Tokenizing eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:13<00:00, 443.06 examples/s]
Truncating eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Tokenizing train dataset:   5%|â–Œ         | 1224/23840 [00:02<00:54, 414.06 examples/s]Tokenizing train dataset:   5%|â–         | 1183/23840 [00:02<00:55, 407.35 examples/s]Truncating eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:00<00:00, 52852.57 examples/s]Tokenizing train dataset:   6%|â–‹         | 1502/23840 [00:03<00:47, 466.81 examples/s]Truncating eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:00<00:00, 50780.46 examples/s]
Tokenizing train dataset:   5%|â–Œ         | 1272/23840 [00:02<00:52, 428.65 examples/s]Tokenizing train dataset:   5%|â–Œ         | 1230/23840 [00:02<00:53, 421.89 examples/s]Tokenizing train dataset:   7%|â–‹         | 1553/23840 [00:03<00:47, 473.79 examples/s]Tokenizing train dataset:   6%|â–Œ         | 1322/23840 [00:02<00:50, 444.19 examples/s]Tokenizing train dataset:   5%|â–Œ         | 1280/23840 [00:02<00:51, 438.06 examples/s]Tokenizing train dataset:   7%|â–‹         | 1604/23840 [00:03<00:46, 480.84 examples/s]Tokenizing train dataset:   6%|â–Œ         | 1368/23840 [00:03<00:50, 445.52 examples/s]Tokenizing train dataset:   6%|â–Œ         | 1331/23840 [00:02<00:49, 455.48 examples/s]Tokenizing train dataset:   7%|â–‹         | 1656/23840 [00:03<00:45, 489.44 examples/s]Tokenizing train dataset:   6%|â–Œ         | 1437/23840 [00:03<00:50, 446.67 examples/s]Tokenizing train dataset:   7%|â–‹         | 1709/23840 [00:03<00:44, 498.04 examples/s]Tokenizing train dataset:   6%|â–Œ         | 1399/23840 [00:03<00:49, 451.87 examples/s]Tokenizing train dataset:   6%|â–Œ         | 1483/23840 [00:03<00:50, 446.19 examples/s]Tokenizing train dataset:   7%|â–‹         | 1762/23840 [00:03<00:43, 504.90 examples/s]Tokenizing train dataset:   6%|â–Œ         | 1448/23840 [00:03<00:48, 459.18 examples/s]Tokenizing train dataset:   6%|â–‹         | 1530/23840 [00:03<00:49, 451.31 examples/s]Tokenizing train dataset:   8%|â–Š         | 1815/23840 [00:03<00:43, 508.09 examples/s]Tokenizing train dataset:   6%|â–‹         | 1496/23840 [00:03<00:48, 462.50 examples/s]Tokenizing train dataset:   7%|â–‹         | 1578/23840 [00:03<00:48, 456.44 examples/s]Tokenizing train dataset:   8%|â–Š         | 1867/23840 [00:04<00:43, 506.42 examples/s]Tokenizing train dataset:   6%|â–‹         | 1546/23840 [00:03<00:47, 470.42 examples/s]Tokenizing train dataset:   7%|â–‹         | 1630/23840 [00:03<00:47, 470.35 examples/s]Tokenizing train dataset:   7%|â–‹         | 1595/23840 [00:03<00:47, 472.45 examples/s]Tokenizing train dataset:   8%|â–Š         | 1940/23840 [00:04<00:44, 497.41 examples/s]Tokenizing train dataset:   7%|â–‹         | 1679/23840 [00:03<00:46, 472.60 examples/s]Tokenizing train dataset:   7%|â–‹         | 1647/23840 [00:03<00:45, 485.36 examples/s]Tokenizing train dataset:   8%|â–Š         | 1993/23840 [00:04<00:43, 502.68 examples/s]Tokenizing train dataset:   7%|â–‹         | 1730/23840 [00:03<00:45, 481.07 examples/s]Tokenizing train dataset:   7%|â–‹         | 1700/23840 [00:03<00:45, 489.08 examples/s]Tokenizing train dataset:   7%|â–‹         | 1782/23840 [00:03<00:45, 489.94 examples/s]Tokenizing train dataset:   7%|â–‹         | 1754/23840 [00:03<00:44, 497.68 examples/s]Tokenizing train dataset:   8%|â–Š         | 1835/23840 [00:04<00:44, 497.09 examples/s]Tokenizing train dataset:   8%|â–Š         | 1806/23840 [00:03<00:43, 502.77 examples/s]Tokenizing train dataset:   9%|â–Š         | 2066/23840 [00:04<01:05, 330.15 examples/s]Tokenizing train dataset:   8%|â–Š         | 1907/23840 [00:04<00:44, 489.55 examples/s]Tokenizing train dataset:   8%|â–Š         | 1880/23840 [00:04<00:44, 495.18 examples/s]Tokenizing train dataset:   9%|â–‰         | 2115/23840 [00:04<01:00, 358.48 examples/s]Tokenizing train dataset:   8%|â–Š         | 1959/23840 [00:04<00:44, 492.58 examples/s]Tokenizing train dataset:   9%|â–‰         | 2167/23840 [00:04<00:55, 389.20 examples/s]Tokenizing train dataset:   8%|â–Š         | 1954/23840 [00:04<00:44, 492.36 examples/s]Tokenizing train dataset:   9%|â–‰         | 2214/23840 [00:04<00:53, 404.86 examples/s]Tokenizing train dataset:   9%|â–‰         | 2261/23840 [00:05<00:51, 417.27 examples/s]Tokenizing train dataset:   8%|â–Š         | 2015/23840 [00:04<01:09, 315.85 examples/s]Tokenizing train dataset:  10%|â–‰         | 2310/23840 [00:05<00:49, 432.40 examples/s]Tokenizing train dataset:   9%|â–Š         | 2061/23840 [00:04<01:03, 342.71 examples/s]Tokenizing train dataset:   8%|â–Š         | 2019/23840 [00:04<01:06, 330.22 examples/s]Tokenizing train dataset:  10%|â–‰         | 2359/23840 [00:05<00:48, 441.02 examples/s]Tokenizing train dataset:   9%|â–‰         | 2107/23840 [00:04<00:59, 366.97 examples/s]Tokenizing train dataset:   9%|â–Š         | 2065/23840 [00:04<01:01, 353.15 examples/s]Tokenizing train dataset:  10%|â–ˆ         | 2407/23840 [00:05<00:47, 446.75 examples/s]Tokenizing train dataset:   9%|â–‰         | 2156/23840 [00:04<00:54, 394.39 examples/s]Tokenizing train dataset:   9%|â–‰         | 2112/23840 [00:04<00:57, 375.25 examples/s]Tokenizing train dataset:  10%|â–ˆ         | 2457/23840 [00:05<00:46, 457.92 examples/s]Tokenizing train dataset:   9%|â–‰         | 2161/23840 [00:04<00:54, 400.03 examples/s]Tokenizing train dataset:   9%|â–‰         | 2223/23840 [00:05<00:52, 409.73 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2505/23840 [00:05<00:46, 459.49 examples/s]Tokenizing train dataset:  10%|â–‰         | 2268/23840 [00:05<00:51, 417.76 examples/s]Tokenizing train dataset:   9%|â–‰         | 2228/23840 [00:05<00:52, 412.00 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2556/23840 [00:05<00:45, 469.70 examples/s]Tokenizing train dataset:  10%|â–‰         | 2314/23840 [00:05<00:50, 427.89 examples/s]Tokenizing train dataset:  10%|â–‰         | 2274/23840 [00:05<00:51, 419.41 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2605/23840 [00:05<00:44, 474.33 examples/s]Tokenizing train dataset:  10%|â–‰         | 2359/23840 [00:05<00:49, 430.64 examples/s]Tokenizing train dataset:  10%|â–‰         | 2322/23840 [00:05<00:49, 432.18 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2656/23840 [00:05<00:43, 482.92 examples/s]Tokenizing train dataset:  10%|â–ˆ         | 2404/23840 [00:05<00:49, 433.08 examples/s]Tokenizing train dataset:  10%|â–‰         | 2368/23840 [00:05<00:49, 437.13 examples/s]Tokenizing train dataset:  11%|â–ˆâ–        | 2705/23840 [00:05<00:43, 483.09 examples/s]Tokenizing train dataset:  10%|â–ˆ         | 2451/23840 [00:05<00:48, 441.76 examples/s]Tokenizing train dataset:  10%|â–ˆ         | 2414/23840 [00:05<00:48, 440.06 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2757/23840 [00:06<00:42, 491.64 examples/s]Tokenizing train dataset:  10%|â–ˆ         | 2499/23840 [00:05<00:47, 448.13 examples/s]Tokenizing train dataset:  10%|â–ˆ         | 2463/23840 [00:05<00:47, 447.93 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2809/23840 [00:06<00:42, 498.26 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2548/23840 [00:05<00:46, 457.46 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2509/23840 [00:05<00:47, 448.65 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2860/23840 [00:06<00:42, 499.42 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2596/23840 [00:05<00:45, 461.90 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2559/23840 [00:05<00:46, 459.08 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2936/23840 [00:06<00:42, 497.52 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2647/23840 [00:06<00:44, 473.33 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2608/23840 [00:05<00:45, 465.74 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 2992/23840 [00:06<00:40, 509.38 examples/s]Tokenizing train dataset:  11%|â–ˆâ–        | 2695/23840 [00:06<00:45, 468.06 examples/s]Tokenizing train dataset:  11%|â–ˆ         | 2658/23840 [00:05<00:44, 474.81 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2745/23840 [00:06<00:44, 475.88 examples/s]Tokenizing train dataset:  11%|â–ˆâ–        | 2706/23840 [00:06<00:44, 473.02 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2795/23840 [00:06<00:43, 479.50 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2757/23840 [00:06<00:43, 482.03 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3044/23840 [00:06<01:05, 315.71 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2848/23840 [00:06<00:42, 490.56 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2807/23840 [00:06<00:43, 486.45 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3093/23840 [00:06<00:59, 348.31 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2858/23840 [00:06<00:42, 490.17 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2920/23840 [00:06<00:43, 480.81 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3143/23840 [00:07<00:54, 378.98 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2977/23840 [00:06<00:41, 502.26 examples/s]Tokenizing train dataset:  12%|â–ˆâ–        | 2931/23840 [00:06<00:42, 487.39 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3191/23840 [00:07<00:51, 399.66 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 2986/23840 [00:06<00:41, 500.81 examples/s]Tokenizing train dataset:  14%|â–ˆâ–Ž        | 3242/23840 [00:07<00:48, 424.11 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3293/23840 [00:07<00:46, 444.29 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3040/23840 [00:07<01:06, 311.04 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3366/23840 [00:07<00:44, 457.71 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3043/23840 [00:06<01:07, 307.09 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3087/23840 [00:07<01:01, 339.15 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3415/23840 [00:07<00:44, 463.39 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3089/23840 [00:07<01:01, 335.36 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3135/23840 [00:07<00:56, 366.85 examples/s]Tokenizing train dataset:  15%|â–ˆâ–        | 3464/23840 [00:07<00:43, 466.77 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3137/23840 [00:07<00:56, 365.03 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3179/23840 [00:07<00:54, 381.76 examples/s]Tokenizing train dataset:  15%|â–ˆâ–        | 3515/23840 [00:07<00:42, 474.47 examples/s]Tokenizing train dataset:  13%|â–ˆâ–Ž        | 3181/23840 [00:07<00:54, 381.72 examples/s]Tokenizing train dataset:  14%|â–ˆâ–Ž        | 3227/23840 [00:07<00:51, 402.41 examples/s]Tokenizing train dataset:  15%|â–ˆâ–        | 3564/23840 [00:07<00:42, 475.88 examples/s]Tokenizing train dataset:  14%|â–ˆâ–Ž        | 3228/23840 [00:07<00:51, 401.48 examples/s]Tokenizing train dataset:  14%|â–ˆâ–Ž        | 3277/23840 [00:07<00:48, 424.85 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3278/23840 [00:07<00:48, 425.33 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3323/23840 [00:07<00:47, 432.73 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3325/23840 [00:07<00:47, 435.75 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3371/23840 [00:07<00:46, 443.04 examples/s]Tokenizing train dataset:  15%|â–ˆâ–Œ        | 3624/23840 [00:08<01:06, 301.97 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3374/23840 [00:07<00:45, 448.09 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3438/23840 [00:07<00:46, 442.49 examples/s]Tokenizing train dataset:  15%|â–ˆâ–Œ        | 3674/23840 [00:08<00:59, 339.01 examples/s]Tokenizing train dataset:  14%|â–ˆâ–        | 3422/23840 [00:07<00:44, 454.37 examples/s]Tokenizing train dataset:  15%|â–ˆâ–        | 3486/23840 [00:08<00:45, 448.58 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3722/23840 [00:08<00:54, 366.78 examples/s]Tokenizing train dataset:  15%|â–ˆâ–        | 3469/23840 [00:07<00:44, 456.01 examples/s]Tokenizing train dataset:  15%|â–ˆâ–        | 3538/23840 [00:08<00:43, 462.90 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3772/23840 [00:08<00:50, 395.76 examples/s]Tokenizing train dataset:  15%|â–ˆâ–        | 3520/23840 [00:08<00:43, 468.29 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3824/23840 [00:08<00:47, 424.47 examples/s]Tokenizing train dataset:  15%|â–ˆâ–        | 3569/23840 [00:08<00:42, 473.00 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3872/23840 [00:08<00:45, 435.12 examples/s]Tokenizing train dataset:  16%|â–ˆâ–‹        | 3925/23840 [00:08<00:43, 459.03 examples/s]Tokenizing train dataset:  15%|â–ˆâ–Œ        | 3601/23840 [00:08<01:10, 288.86 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 3975/23840 [00:08<00:42, 465.81 examples/s]Tokenizing train dataset:  15%|â–ˆâ–Œ        | 3649/23840 [00:08<01:02, 321.12 examples/s]Tokenizing train dataset:  15%|â–ˆâ–Œ        | 3624/23840 [00:08<01:11, 284.31 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3697/23840 [00:08<00:57, 350.71 examples/s]Tokenizing train dataset:  15%|â–ˆâ–Œ        | 3674/23840 [00:08<01:01, 325.39 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3745/23840 [00:08<00:53, 377.91 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3722/23840 [00:08<00:56, 357.42 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4046/23840 [00:09<01:01, 323.76 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3795/23840 [00:08<00:49, 402.81 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3772/23840 [00:08<00:51, 389.81 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4093/23840 [00:09<00:56, 351.09 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3846/23840 [00:09<00:46, 427.51 examples/s]Tokenizing train dataset:  16%|â–ˆâ–Œ        | 3825/23840 [00:08<00:47, 423.14 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4144/23840 [00:09<00:51, 384.82 examples/s]Tokenizing train dataset:  16%|â–ˆâ–‹        | 3896/23840 [00:09<00:44, 444.07 examples/s]Tokenizing train dataset:  16%|â–ˆâ–‹        | 3898/23840 [00:09<00:45, 441.14 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4191/23840 [00:09<00:48, 403.60 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 3948/23840 [00:09<00:43, 462.50 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 3951/23840 [00:09<00:43, 459.42 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4238/23840 [00:09<00:46, 419.82 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 3997/23840 [00:09<00:42, 466.76 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4287/23840 [00:09<00:44, 436.36 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4336/23840 [00:09<00:43, 447.85 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4000/23840 [00:09<01:03, 311.81 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4385/23840 [00:10<00:43, 451.77 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4045/23840 [00:09<00:58, 338.96 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4065/23840 [00:09<01:03, 310.96 examples/s]Tokenizing train dataset:  19%|â–ˆâ–Š        | 4436/23840 [00:10<00:41, 464.16 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4091/23840 [00:09<00:54, 363.78 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4115/23840 [00:09<00:57, 344.57 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4484/23840 [00:10<00:41, 466.41 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4142/23840 [00:09<00:49, 395.10 examples/s]Tokenizing train dataset:  17%|â–ˆâ–‹        | 4166/23840 [00:09<00:52, 376.82 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4536/23840 [00:10<00:40, 476.74 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4190/23840 [00:09<00:47, 413.99 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4211/23840 [00:09<00:50, 391.68 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4587/23840 [00:10<00:39, 482.90 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4237/23840 [00:09<00:46, 425.14 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4258/23840 [00:10<00:47, 409.61 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4639/23840 [00:10<00:38, 492.59 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4286/23840 [00:10<00:44, 438.62 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4307/23840 [00:10<00:45, 427.46 examples/s]Tokenizing train dataset:  20%|â–ˆâ–‰        | 4714/23840 [00:10<00:38, 491.73 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4333/23840 [00:10<00:43, 446.01 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4355/23840 [00:10<00:44, 439.86 examples/s]Tokenizing train dataset:  20%|â–ˆâ–‰        | 4765/23840 [00:10<00:38, 492.34 examples/s]Tokenizing train dataset:  18%|â–ˆâ–Š        | 4380/23840 [00:10<00:43, 447.60 examples/s]Tokenizing train dataset:  19%|â–ˆâ–Š        | 4425/23840 [00:10<00:43, 448.38 examples/s]Tokenizing train dataset:  19%|â–ˆâ–Š        | 4429/23840 [00:10<00:42, 458.51 examples/s]Tokenizing train dataset:  20%|â–ˆâ–ˆ        | 4843/23840 [00:10<00:38, 499.00 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4495/23840 [00:10<00:43, 449.41 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4476/23840 [00:10<00:42, 457.20 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 4898/23840 [00:11<00:37, 510.45 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4544/23840 [00:10<00:42, 456.77 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4528/23840 [00:10<00:40, 471.34 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 4973/23840 [00:11<00:37, 501.90 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4577/23840 [00:10<00:40, 475.26 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4593/23840 [00:10<00:41, 461.35 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4642/23840 [00:10<00:41, 467.66 examples/s]Tokenizing train dataset:  19%|â–ˆâ–‰        | 4630/23840 [00:10<00:39, 487.38 examples/s]Tokenizing train dataset:  20%|â–ˆâ–‰        | 4691/23840 [00:11<00:40, 470.80 examples/s]Tokenizing train dataset:  20%|â–ˆâ–‰        | 4702/23840 [00:10<00:39, 480.91 examples/s]Tokenizing train dataset:  20%|â–ˆâ–‰        | 4740/23840 [00:11<00:40, 472.50 examples/s]Tokenizing train dataset:  20%|â–ˆâ–‰        | 4752/23840 [00:11<00:39, 481.89 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 5044/23840 [00:11<00:58, 323.49 examples/s]Tokenizing train dataset:  20%|â–ˆâ–ˆ        | 4790/23840 [00:11<00:40, 474.25 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆâ–       | 5091/23840 [00:11<00:53, 347.96 examples/s]Tokenizing train dataset:  20%|â–ˆâ–ˆ        | 4827/23840 [00:11<00:39, 485.16 examples/s]Tokenizing train dataset:  20%|â–ˆâ–ˆ        | 4845/23840 [00:11<00:38, 492.45 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5142/23840 [00:11<00:49, 378.29 examples/s]Tokenizing train dataset:  20%|â–ˆâ–ˆ        | 4883/23840 [00:11<00:37, 501.22 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 4899/23840 [00:11<00:37, 503.22 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5191/23840 [00:11<00:46, 401.27 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 4934/23840 [00:11<00:37, 501.53 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 4950/23840 [00:11<00:37, 501.99 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5241/23840 [00:11<00:43, 424.13 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5291/23840 [00:12<00:41, 441.91 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5341/23840 [00:12<00:40, 453.49 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5389/23840 [00:12<00:40, 457.04 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 5000/23840 [00:11<01:00, 311.95 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 5021/23840 [00:11<01:01, 304.83 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5439/23840 [00:12<00:39, 466.44 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 5063/23840 [00:12<00:57, 324.65 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆ        | 5061/23840 [00:11<00:56, 332.15 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5489/23840 [00:12<00:38, 473.69 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆâ–       | 5114/23840 [00:12<00:51, 362.27 examples/s]Tokenizing train dataset:  21%|â–ˆâ–ˆâ–       | 5111/23840 [00:11<00:51, 362.92 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5540/23840 [00:12<00:38, 480.92 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5161/23840 [00:12<00:48, 385.55 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5158/23840 [00:12<00:48, 382.65 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5589/23840 [00:12<00:38, 479.40 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5209/23840 [00:12<00:46, 404.75 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5207/23840 [00:12<00:45, 406.20 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–Ž       | 5639/23840 [00:12<00:37, 479.09 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5258/23840 [00:12<00:43, 425.65 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5255/23840 [00:12<00:43, 423.89 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5691/23840 [00:12<00:37, 488.15 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5302/23840 [00:12<00:42, 433.97 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5328/23840 [00:12<00:42, 436.85 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5766/23840 [00:13<00:36, 489.44 examples/s]Tokenizing train dataset:  22%|â–ˆâ–ˆâ–       | 5350/23840 [00:12<00:41, 442.68 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5376/23840 [00:12<00:41, 445.88 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5818/23840 [00:13<00:36, 495.07 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5397/23840 [00:12<00:41, 444.55 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5424/23840 [00:12<00:40, 453.10 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–       | 5871/23840 [00:13<00:35, 501.89 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5445/23840 [00:12<00:40, 450.72 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5493/23840 [00:12<00:40, 453.44 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5492/23840 [00:12<00:40, 454.52 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–       | 5946/23840 [00:13<00:36, 496.34 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5541/23840 [00:13<00:39, 458.75 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5541/23840 [00:12<00:39, 461.22 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–Œ       | 5999/23840 [00:13<00:35, 501.06 examples/s]Tokenizing train dataset:  23%|â–ˆâ–ˆâ–Ž       | 5590/23840 [00:13<00:39, 461.39 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–Ž       | 5612/23840 [00:13<00:39, 461.62 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–Ž       | 5639/23840 [00:13<00:39, 463.05 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5683/23840 [00:13<00:39, 462.31 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5690/23840 [00:13<00:38, 472.97 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5732/23840 [00:13<00:38, 467.06 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6070/23840 [00:13<00:52, 338.78 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5763/23840 [00:13<00:38, 474.68 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6118/23840 [00:13<00:48, 364.16 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5802/23840 [00:13<00:38, 466.34 examples/s]Tokenizing train dataset:  24%|â–ˆâ–ˆâ–       | 5814/23840 [00:13<00:37, 481.47 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6168/23840 [00:14<00:45, 390.47 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–       | 5855/23840 [00:13<00:37, 477.67 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–       | 5866/23840 [00:13<00:36, 489.37 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6215/23840 [00:14<00:43, 407.69 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–       | 5926/23840 [00:13<00:37, 474.86 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–‹       | 6264/23840 [00:14<00:41, 426.25 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–       | 5940/23840 [00:13<00:36, 485.97 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–Œ       | 5981/23840 [00:13<00:36, 491.16 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–‹       | 6311/23840 [00:14<00:40, 436.38 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–Œ       | 5992/23840 [00:13<00:36, 490.09 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6358/23840 [00:14<00:39, 444.24 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6428/23840 [00:14<00:38, 449.18 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6044/23840 [00:14<00:54, 328.28 examples/s]Tokenizing train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6045/23840 [00:14<00:55, 321.68 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6481/23840 [00:14<00:37, 466.40 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6090/23840 [00:14<00:50, 351.85 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6091/23840 [00:14<00:51, 347.09 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6532/23840 [00:14<00:36, 474.81 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6139/23840 [00:14<00:46, 379.54 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6140/23840 [00:14<00:46, 377.17 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6585/23840 [00:14<00:35, 487.59 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6186/23840 [00:14<00:44, 397.08 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6187/23840 [00:14<00:44, 397.38 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6637/23840 [00:15<00:34, 493.87 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6232/23840 [00:14<00:42, 411.15 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–Œ       | 6233/23840 [00:14<00:42, 412.57 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6711/23840 [00:15<00:34, 491.11 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–‹       | 6278/23840 [00:14<00:41, 422.80 examples/s]Tokenizing train dataset:  26%|â–ˆâ–ˆâ–‹       | 6283/23840 [00:14<00:40, 432.54 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6763/23840 [00:15<00:34, 496.59 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6325/23840 [00:14<00:40, 435.14 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6332/23840 [00:14<00:39, 445.53 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–Š       | 6837/23840 [00:15<00:34, 493.89 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6391/23840 [00:15<00:40, 435.38 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6402/23840 [00:14<00:38, 448.93 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6439/23840 [00:15<00:39, 444.33 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6455/23840 [00:15<00:37, 465.18 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 6913/23840 [00:15<00:34, 494.41 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6490/23840 [00:15<00:37, 460.00 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6505/23840 [00:15<00:36, 470.55 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 6966/23840 [00:15<00:33, 498.58 examples/s]Tokenizing train dataset:  27%|â–ˆâ–ˆâ–‹       | 6540/23840 [00:15<00:36, 467.96 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6557/23840 [00:15<00:35, 481.38 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6593/23840 [00:15<00:35, 481.20 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6610/23840 [00:15<00:35, 491.06 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6643/23840 [00:15<00:35, 484.36 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 7020/23840 [00:16<00:50, 333.81 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6684/23840 [00:15<00:35, 486.39 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–‰       | 7066/23840 [00:16<00:46, 357.64 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6715/23840 [00:15<00:35, 478.87 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6734/23840 [00:15<00:35, 487.39 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–‰       | 7114/23840 [00:16<00:43, 383.01 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6764/23840 [00:15<00:35, 477.09 examples/s]Tokenizing train dataset:  28%|â–ˆâ–ˆâ–Š       | 6785/23840 [00:15<00:34, 489.58 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7165/23840 [00:16<00:40, 411.50 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–Š       | 6814/23840 [00:15<00:35, 479.85 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 6857/23840 [00:15<00:35, 483.42 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7214/23840 [00:16<00:38, 428.47 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 6885/23840 [00:16<00:35, 475.35 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 6908/23840 [00:15<00:34, 488.70 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7267/23840 [00:16<00:36, 454.20 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 6937/23840 [00:16<00:34, 486.39 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 6960/23840 [00:16<00:34, 495.96 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7335/23840 [00:16<00:36, 450.66 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7385/23840 [00:16<00:35, 457.50 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7434/23840 [00:16<00:35, 463.78 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 7000/23840 [00:16<00:51, 327.16 examples/s]Tokenizing train dataset:  29%|â–ˆâ–ˆâ–‰       | 7021/23840 [00:16<00:52, 321.37 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆâ–      | 7483/23840 [00:16<00:35, 467.21 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–‰       | 7045/23840 [00:16<00:48, 349.55 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–‰       | 7067/23840 [00:16<00:48, 346.06 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7531/23840 [00:17<00:34, 466.67 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–‰       | 7090/23840 [00:16<00:45, 370.36 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–‰       | 7115/23840 [00:16<00:44, 373.34 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7581/23840 [00:17<00:34, 470.89 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–‰       | 7136/23840 [00:16<00:42, 389.87 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7166/23840 [00:16<00:41, 402.36 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7630/23840 [00:17<00:34, 474.26 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7185/23840 [00:16<00:40, 413.72 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7214/23840 [00:16<00:39, 419.57 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7682/23840 [00:17<00:33, 483.33 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7232/23840 [00:16<00:38, 426.96 examples/s]Tokenizing train dataset:  30%|â–ˆâ–ˆâ–ˆ       | 7267/23840 [00:16<00:37, 445.39 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7736/23840 [00:17<00:32, 497.10 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7285/23840 [00:17<00:36, 453.78 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7788/23840 [00:17<00:32, 501.20 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7334/23840 [00:17<00:37, 444.79 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7352/23840 [00:17<00:36, 446.76 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7383/23840 [00:17<00:36, 453.64 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7862/23840 [00:17<00:32, 495.76 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7401/23840 [00:17<00:36, 455.53 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7433/23840 [00:17<00:35, 461.29 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 7449/23840 [00:17<00:35, 460.33 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7937/23840 [00:17<00:32, 494.00 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆâ–      | 7482/23840 [00:17<00:35, 466.57 examples/s]Tokenizing train dataset:  31%|â–ˆâ–ˆâ–ˆâ–      | 7499/23840 [00:17<00:35, 466.72 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 7988/23840 [00:18<00:31, 495.49 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7530/23840 [00:17<00:34, 466.36 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7547/23840 [00:17<00:34, 466.83 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7579/23840 [00:17<00:34, 469.16 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7617/23840 [00:17<00:34, 465.69 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7627/23840 [00:17<00:34, 470.50 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 8042/23840 [00:18<00:47, 331.04 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7667/23840 [00:17<00:34, 471.99 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7676/23840 [00:17<00:34, 475.32 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8091/23840 [00:18<00:43, 359.86 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7731/23840 [00:17<00:32, 494.33 examples/s]Tokenizing train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 7721/23840 [00:18<00:33, 486.44 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8142/23840 [00:18<00:40, 391.34 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7771/23840 [00:18<00:32, 489.38 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7781/23840 [00:17<00:32, 493.83 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8192/23840 [00:18<00:37, 413.54 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7831/23840 [00:18<00:32, 494.15 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7825/23840 [00:18<00:32, 495.65 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 8242/23840 [00:18<00:35, 434.62 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7902/23840 [00:18<00:33, 482.19 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7894/23840 [00:18<00:33, 480.94 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 8291/23840 [00:18<00:34, 445.90 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7954/23840 [00:18<00:32, 489.02 examples/s]Tokenizing train dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7945/23840 [00:18<00:32, 487.22 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 8340/23840 [00:18<00:34, 453.64 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 7995/23840 [00:18<00:32, 487.69 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 8388/23840 [00:19<00:33, 460.07 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 8436/23840 [00:19<00:33, 462.01 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8484/23840 [00:19<00:33, 463.63 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 8020/23840 [00:18<00:48, 324.02 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8532/23840 [00:19<00:32, 465.95 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8068/23840 [00:18<00:44, 351.56 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8066/23840 [00:18<00:48, 325.40 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8581/23840 [00:19<00:32, 469.99 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8117/23840 [00:18<00:41, 378.39 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8112/23840 [00:19<00:44, 350.46 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8633/23840 [00:19<00:31, 482.81 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8166/23840 [00:18<00:38, 403.33 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8160/23840 [00:19<00:41, 376.89 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 8683/23840 [00:19<00:31, 484.57 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8215/23840 [00:19<00:36, 422.90 examples/s]Tokenizing train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 8210/23840 [00:19<00:38, 403.26 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8733/23840 [00:19<00:31, 486.87 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 8264/23840 [00:19<00:35, 438.72 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 8259/23840 [00:19<00:36, 423.33 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8783/23840 [00:19<00:30, 488.08 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 8335/23840 [00:19<00:34, 448.55 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 8330/23840 [00:19<00:35, 439.34 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8836/23840 [00:19<00:30, 495.99 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 8383/23840 [00:19<00:34, 451.83 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 8379/23840 [00:19<00:34, 450.78 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8909/23840 [00:20<00:30, 490.06 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 8430/23840 [00:19<00:33, 455.56 examples/s]Tokenizing train dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 8448/23840 [00:19<00:34, 450.89 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 8959/23840 [00:20<00:30, 489.31 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8477/23840 [00:19<00:33, 457.43 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8497/23840 [00:19<00:33, 458.33 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8524/23840 [00:19<00:33, 458.93 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8545/23840 [00:19<00:33, 462.32 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8572/23840 [00:19<00:33, 460.80 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8594/23840 [00:20<00:32, 466.98 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8624/23840 [00:19<00:32, 473.04 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 8647/23840 [00:20<00:31, 480.79 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 8696/23840 [00:20<00:32, 470.60 examples/s]Tokenizing train dataset:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 8697/23840 [00:20<00:31, 478.06 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9020/23840 [00:20<01:02, 238.61 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8746/23840 [00:20<00:31, 473.87 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8749/23840 [00:20<00:30, 488.60 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9065/23840 [00:20<00:54, 269.56 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8819/23840 [00:20<00:31, 476.07 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8825/23840 [00:20<00:30, 491.14 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9111/23840 [00:20<00:48, 303.40 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9161/23840 [00:21<00:42, 341.62 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8891/23840 [00:20<00:31, 473.21 examples/s]Tokenizing train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 8897/23840 [00:20<00:30, 483.06 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 9209/23840 [00:21<00:39, 369.79 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 8942/23840 [00:20<00:31, 479.87 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 8949/23840 [00:20<00:30, 489.85 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9259/23840 [00:21<00:36, 400.08 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9306/23840 [00:21<00:34, 416.12 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9354/23840 [00:21<00:33, 430.90 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9403/23840 [00:21<00:32, 441.68 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 9454/23840 [00:21<00:31, 456.72 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9000/23840 [00:21<01:04, 231.51 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9000/23840 [00:21<01:03, 234.85 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 9503/23840 [00:21<00:30, 464.63 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9044/23840 [00:21<00:56, 261.23 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9042/23840 [00:21<00:56, 260.93 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9552/23840 [00:21<00:30, 470.06 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9089/23840 [00:21<00:50, 293.13 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9086/23840 [00:21<00:50, 290.58 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9600/23840 [00:21<00:30, 469.59 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9137/23840 [00:21<00:44, 328.93 examples/s]Tokenizing train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 9132/23840 [00:21<00:45, 322.73 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9648/23840 [00:22<00:30, 471.34 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 9184/23840 [00:21<00:40, 359.52 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 9179/23840 [00:21<00:41, 351.99 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9696/23840 [00:22<00:29, 472.30 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 9231/23840 [00:21<00:37, 384.52 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 9225/23840 [00:21<00:38, 374.92 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9745/23840 [00:22<00:29, 475.96 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9280/23840 [00:21<00:35, 410.30 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9274/23840 [00:21<00:36, 402.05 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9794/23840 [00:22<00:29, 478.06 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9327/23840 [00:22<00:34, 424.96 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9320/23840 [00:21<00:34, 415.68 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9845/23840 [00:22<00:28, 484.00 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9373/23840 [00:22<00:33, 432.45 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9367/23840 [00:21<00:33, 428.75 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9898/23840 [00:22<00:28, 492.78 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 9423/23840 [00:22<00:32, 449.80 examples/s]Tokenizing train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9416/23840 [00:22<00:32, 443.30 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9949/23840 [00:22<00:28, 495.09 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 9472/23840 [00:22<00:31, 458.60 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 9466/23840 [00:22<00:31, 455.66 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 9516/23840 [00:22<00:30, 466.26 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9542/23840 [00:22<00:31, 458.88 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9565/23840 [00:22<00:30, 471.00 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9593/23840 [00:22<00:30, 461.94 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10000/23840 [00:23<00:49, 279.28 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9614/23840 [00:22<00:29, 474.76 examples/s]Tokenizing train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9642/23840 [00:22<00:30, 466.02 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10046/23840 [00:23<00:44, 313.04 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9662/23840 [00:22<00:30, 472.25 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10092/23840 [00:23<00:40, 342.04 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9714/23840 [00:22<00:30, 468.46 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9713/23840 [00:22<00:29, 478.82 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10141/23840 [00:23<00:36, 374.00 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9762/23840 [00:22<00:29, 469.90 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9787/23840 [00:22<00:29, 480.57 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10189/23840 [00:23<00:34, 399.36 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9834/23840 [00:23<00:29, 469.65 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9837/23840 [00:22<00:29, 482.58 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10239/23840 [00:23<00:32, 424.48 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9887/23840 [00:23<00:28, 483.67 examples/s]Tokenizing train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9889/23840 [00:23<00:28, 492.74 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10311/23840 [00:23<00:30, 441.74 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9936/23840 [00:23<00:28, 481.36 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9963/23840 [00:23<00:28, 490.42 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10360/23840 [00:23<00:29, 452.34 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9988/23840 [00:23<00:28, 488.84 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10429/23840 [00:23<00:29, 449.98 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10500/23840 [00:24<00:29, 455.29 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10020/23840 [00:23<00:48, 284.59 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10549/23840 [00:24<00:28, 463.19 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10044/23840 [00:23<00:49, 276.09 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10067/23840 [00:23<00:43, 314.88 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10603/23840 [00:24<00:27, 480.53 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10087/23840 [00:23<00:45, 302.75 examples/s]Tokenizing train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10111/23840 [00:23<00:40, 337.49 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10133/23840 [00:23<00:41, 333.51 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10678/23840 [00:24<00:27, 485.61 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10159/23840 [00:23<00:37, 367.96 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10181/23840 [00:24<00:37, 364.72 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10731/23840 [00:24<00:26, 493.97 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10207/23840 [00:24<00:34, 391.58 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10230/23840 [00:24<00:34, 392.78 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10783/23840 [00:24<00:26, 499.14 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10255/23840 [00:24<00:32, 412.10 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10275/23840 [00:24<00:33, 403.47 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10301/23840 [00:24<00:32, 422.05 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10857/23840 [00:24<00:26, 492.13 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10324/23840 [00:24<00:31, 424.81 examples/s]Tokenizing train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10350/23840 [00:24<00:30, 438.26 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10907/23840 [00:24<00:26, 493.89 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10372/23840 [00:24<00:30, 438.01 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10960/23840 [00:25<00:25, 503.25 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10418/23840 [00:24<00:30, 440.46 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10439/23840 [00:24<00:30, 438.07 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10464/23840 [00:24<00:30, 443.60 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10487/23840 [00:24<00:29, 445.75 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10512/23840 [00:24<00:29, 451.19 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10536/23840 [00:24<00:29, 454.67 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 11022/23840 [00:25<00:38, 336.89 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10561/23840 [00:24<00:28, 459.16 examples/s]Tokenizing train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10587/23840 [00:24<00:28, 468.71 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11071/23840 [00:25<00:34, 365.99 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10614/23840 [00:24<00:27, 478.34 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10635/23840 [00:25<00:28, 469.70 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11120/23840 [00:25<00:32, 391.12 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10663/23840 [00:24<00:27, 479.22 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10687/23840 [00:25<00:27, 481.73 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11169/23840 [00:25<00:30, 412.35 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10715/23840 [00:25<00:26, 486.84 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10739/23840 [00:25<00:26, 490.90 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10767/23840 [00:25<00:26, 492.36 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10789/23840 [00:25<00:26, 491.23 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11237/23840 [00:25<00:29, 421.56 examples/s]Tokenizing train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10819/23840 [00:25<00:26, 497.30 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11288/23840 [00:25<00:28, 439.92 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10861/23840 [00:25<00:26, 486.17 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11339/23840 [00:25<00:27, 452.43 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10891/23840 [00:25<00:26, 482.77 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10911/23840 [00:25<00:26, 487.56 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11390/23840 [00:26<00:26, 465.72 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10947/23840 [00:25<00:25, 499.47 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10965/23840 [00:25<00:25, 498.54 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11440/23840 [00:26<00:26, 472.54 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10998/23840 [00:25<00:25, 498.69 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11512/23840 [00:26<00:26, 471.95 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11560/23840 [00:26<00:25, 473.43 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 11022/23840 [00:26<00:39, 322.19 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11070/23840 [00:26<00:36, 352.08 examples/s]Tokenizing train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11070/23840 [00:26<00:39, 327.15 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11628/23840 [00:26<00:26, 462.91 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11117/23840 [00:26<00:33, 377.85 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11117/23840 [00:26<00:36, 352.30 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11681/23840 [00:26<00:25, 476.49 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11165/23840 [00:26<00:31, 401.57 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11164/23840 [00:26<00:33, 376.32 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11730/23840 [00:26<00:25, 476.58 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11208/23840 [00:26<00:32, 387.18 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11229/23840 [00:26<00:30, 407.14 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11805/23840 [00:26<00:25, 479.03 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11252/23840 [00:26<00:31, 399.14 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11277/23840 [00:26<00:29, 422.83 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11858/23840 [00:27<00:24, 489.61 examples/s]Tokenizing train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11302/23840 [00:26<00:29, 423.76 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11327/23840 [00:26<00:28, 439.26 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11910/23840 [00:27<00:24, 494.86 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11349/23840 [00:26<00:28, 433.24 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11374/23840 [00:26<00:27, 445.30 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 11965/23840 [00:27<00:23, 507.79 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11399/23840 [00:26<00:27, 447.54 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11422/23840 [00:26<00:27, 454.07 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11447/23840 [00:26<00:27, 453.30 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11469/23840 [00:27<00:27, 452.59 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11494/23840 [00:26<00:27, 452.23 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11517/23840 [00:27<00:26, 456.72 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12022/23840 [00:27<00:35, 332.97 examples/s]Tokenizing train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11543/23840 [00:27<00:26, 457.83 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11566/23840 [00:27<00:27, 454.26 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12069/23840 [00:27<00:32, 358.95 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11590/23840 [00:27<00:26, 457.98 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12117/23840 [00:27<00:30, 385.35 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11635/23840 [00:27<00:27, 451.83 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11661/23840 [00:27<00:26, 458.21 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12163/23840 [00:27<00:29, 400.84 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11687/23840 [00:27<00:26, 467.06 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11711/23840 [00:27<00:25, 466.68 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12215/23840 [00:27<00:27, 428.00 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11758/23840 [00:27<00:25, 466.50 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12262/23840 [00:28<00:26, 438.15 examples/s]Tokenizing train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11781/23840 [00:27<00:26, 462.37 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11809/23840 [00:27<00:25, 474.85 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12309/23840 [00:28<00:26, 442.53 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11837/23840 [00:27<00:25, 477.83 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11862/23840 [00:27<00:24, 482.05 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12357/23840 [00:28<00:25, 452.25 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11890/23840 [00:27<00:24, 487.05 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11914/23840 [00:27<00:24, 487.65 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12406/23840 [00:28<00:24, 460.97 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 11944/23840 [00:27<00:23, 497.46 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 11969/23840 [00:28<00:23, 500.82 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12459/23840 [00:28<00:23, 476.19 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12509/23840 [00:28<00:23, 481.51 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12560/23840 [00:28<00:23, 486.26 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12000/23840 [00:28<00:36, 326.45 examples/s]Tokenizing train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12020/23840 [00:28<00:37, 318.86 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12045/23840 [00:28<00:33, 349.38 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12635/23840 [00:28<00:22, 488.82 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12065/23840 [00:28<00:34, 343.71 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12091/23840 [00:28<00:31, 373.35 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12685/23840 [00:28<00:22, 490.25 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12113/23840 [00:28<00:31, 369.66 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12137/23840 [00:28<00:30, 389.79 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12738/23840 [00:29<00:22, 498.27 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12157/23840 [00:28<00:30, 383.01 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12185/23840 [00:28<00:28, 410.71 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12792/23840 [00:29<00:21, 508.47 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 12207/23840 [00:28<00:28, 410.59 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12233/23840 [00:28<00:27, 427.11 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12846/23840 [00:29<00:21, 514.80 examples/s]Tokenizing train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12253/23840 [00:28<00:27, 420.70 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12299/23840 [00:28<00:26, 430.40 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12922/23840 [00:29<00:21, 509.23 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12320/23840 [00:29<00:26, 426.83 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12347/23840 [00:28<00:26, 440.39 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12368/23840 [00:29<00:26, 440.03 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12997/23840 [00:29<00:21, 502.91 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12394/23840 [00:29<00:25, 446.37 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12417/23840 [00:29<00:25, 451.22 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12446/23840 [00:29<00:24, 463.22 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12468/23840 [00:29<00:24, 464.78 examples/s]Tokenizing train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12496/23840 [00:29<00:24, 469.46 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12518/23840 [00:29<00:23, 473.30 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13070/23840 [00:29<00:30, 349.26 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12546/23840 [00:29<00:23, 474.36 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12567/23840 [00:29<00:23, 477.03 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13119/23840 [00:30<00:28, 373.88 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12594/23840 [00:29<00:23, 473.16 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12616/23840 [00:29<00:23, 474.50 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13167/23840 [00:30<00:27, 393.80 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12645/23840 [00:29<00:23, 481.17 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12666/23840 [00:29<00:23, 479.68 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13217/23840 [00:30<00:25, 416.20 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12696/23840 [00:29<00:23, 483.81 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12717/23840 [00:29<00:22, 486.02 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13265/23840 [00:30<00:24, 430.38 examples/s]Tokenizing train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12748/23840 [00:29<00:22, 492.38 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12769/23840 [00:29<00:22, 494.94 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13315/23840 [00:30<00:23, 446.43 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12802/23840 [00:29<00:21, 502.98 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12822/23840 [00:30<00:22, 498.94 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13367/23840 [00:30<00:22, 464.46 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12853/23840 [00:29<00:21, 503.78 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12872/23840 [00:30<00:22, 497.52 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13416/23840 [00:30<00:22, 468.10 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12904/23840 [00:30<00:21, 501.56 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12922/23840 [00:30<00:22, 495.62 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12973/23840 [00:30<00:21, 495.38 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13490/23840 [00:30<00:21, 474.48 examples/s]Tokenizing train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12979/23840 [00:30<00:21, 496.57 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13541/23840 [00:30<00:21, 482.39 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13615/23840 [00:31<00:21, 482.35 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13040/23840 [00:30<00:33, 326.36 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13044/23840 [00:30<00:33, 321.60 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13664/23840 [00:31<00:21, 483.34 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13087/23840 [00:30<00:30, 352.74 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13090/23840 [00:30<00:30, 347.99 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13713/23840 [00:31<00:20, 484.69 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13133/23840 [00:30<00:28, 374.00 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13137/23840 [00:30<00:28, 370.90 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13784/23840 [00:31<00:21, 478.70 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13180/23840 [00:30<00:27, 392.89 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13186/23840 [00:31<00:26, 396.25 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13834/23840 [00:31<00:20, 477.04 examples/s]Tokenizing train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13228/23840 [00:30<00:25, 412.16 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13232/23840 [00:31<00:25, 411.00 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13883/23840 [00:31<00:20, 477.09 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13273/23840 [00:31<00:25, 419.10 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13277/23840 [00:31<00:25, 419.24 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13932/23840 [00:31<00:20, 478.05 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13323/23840 [00:31<00:23, 438.97 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13326/23840 [00:31<00:24, 436.59 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13985/23840 [00:31<00:20, 490.49 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13372/23840 [00:31<00:23, 449.15 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13375/23840 [00:31<00:23, 447.92 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13419/23840 [00:31<00:23, 452.42 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13422/23840 [00:31<00:23, 452.68 examples/s]Tokenizing train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13466/23840 [00:31<00:22, 454.43 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13470/23840 [00:31<00:22, 456.95 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14046/23840 [00:32<00:30, 325.43 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13516/23840 [00:31<00:22, 464.57 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13518/23840 [00:31<00:22, 460.78 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14094/23840 [00:32<00:27, 354.79 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13568/23840 [00:31<00:21, 478.12 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13569/23840 [00:31<00:21, 473.82 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14143/23840 [00:32<00:25, 381.12 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13638/23840 [00:31<00:21, 470.55 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13638/23840 [00:31<00:21, 466.32 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14193/23840 [00:32<00:23, 407.43 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13688/23840 [00:31<00:21, 476.07 examples/s]Tokenizing train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13688/23840 [00:32<00:21, 472.29 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14241/23840 [00:32<00:22, 419.82 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13738/23840 [00:32<00:21, 479.81 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13739/23840 [00:32<00:21, 471.44 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14287/23840 [00:32<00:22, 428.82 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13789/23840 [00:32<00:21, 475.47 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14335/23840 [00:32<00:21, 440.46 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13807/23840 [00:32<00:21, 470.39 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13837/23840 [00:32<00:21, 474.90 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14383/23840 [00:32<00:21, 447.86 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13877/23840 [00:32<00:21, 467.70 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13885/23840 [00:32<00:21, 473.18 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14430/23840 [00:32<00:20, 453.54 examples/s]Tokenizing train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13926/23840 [00:32<00:21, 469.43 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14484/23840 [00:33<00:19, 474.33 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13958/23840 [00:32<00:20, 477.87 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13978/23840 [00:32<00:20, 479.69 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14021/23840 [00:32<00:30, 324.89 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14553/23840 [00:33<00:30, 305.33 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14045/23840 [00:32<00:30, 321.81 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14066/23840 [00:33<00:28, 347.84 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14604/23840 [00:33<00:26, 342.31 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14091/23840 [00:32<00:28, 347.05 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14113/23840 [00:33<00:26, 371.91 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14654/23840 [00:33<00:24, 374.65 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14137/23840 [00:33<00:26, 370.00 examples/s]Tokenizing train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14158/23840 [00:33<00:24, 387.92 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14707/23840 [00:33<00:22, 408.46 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14185/23840 [00:33<00:24, 392.52 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14208/23840 [00:33<00:23, 411.84 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14760/23840 [00:33<00:20, 436.93 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14232/23840 [00:33<00:23, 409.25 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14255/23840 [00:33<00:22, 423.68 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14813/23840 [00:33<00:19, 457.86 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14301/23840 [00:33<00:22, 429.35 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 14303/23840 [00:33<00:22, 427.23 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14865/23840 [00:34<00:19, 468.24 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14349/23840 [00:33<00:21, 438.67 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14350/23840 [00:33<00:21, 435.33 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 14917/23840 [00:34<00:18, 481.90 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14397/23840 [00:33<00:21, 449.34 examples/s]Tokenizing train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14398/23840 [00:33<00:21, 446.68 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 14992/23840 [00:34<00:18, 487.34 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14446/23840 [00:33<00:20, 458.51 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14447/23840 [00:33<00:20, 456.12 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14499/23840 [00:33<00:19, 476.70 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14501/23840 [00:33<00:19, 475.63 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14569/23840 [00:34<00:19, 470.62 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15045/23840 [00:34<00:27, 324.43 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15092/23840 [00:34<00:24, 350.95 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14553/23840 [00:34<00:32, 284.16 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15140/23840 [00:34<00:23, 376.95 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14603/23840 [00:34<00:28, 324.01 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15188/23840 [00:34<00:21, 400.06 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14626/23840 [00:34<00:30, 299.59 examples/s]Tokenizing train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14652/23840 [00:34<00:25, 358.33 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15240/23840 [00:34<00:20, 426.29 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14677/23840 [00:34<00:27, 337.76 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14703/23840 [00:34<00:23, 392.98 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15292/23840 [00:35<00:19, 448.42 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14728/23840 [00:34<00:24, 372.65 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14756/23840 [00:34<00:21, 425.55 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14780/23840 [00:34<00:22, 404.12 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15361/23840 [00:35<00:18, 449.67 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14808/23840 [00:34<00:20, 446.30 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14832/23840 [00:34<00:20, 430.80 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15411/23840 [00:35<00:18, 460.06 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14859/23840 [00:34<00:19, 460.39 examples/s]Tokenizing train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14880/23840 [00:34<00:20, 441.48 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15462/23840 [00:35<00:17, 471.52 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 14911/23840 [00:34<00:18, 473.97 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 14931/23840 [00:35<00:19, 457.35 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15513/23840 [00:35<00:17, 478.91 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 14961/23840 [00:35<00:18, 474.14 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 14981/23840 [00:35<00:18, 468.15 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15563/23840 [00:35<00:17, 481.06 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15613/23840 [00:35<00:17, 483.20 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15666/23840 [00:35<00:16, 492.02 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15018/23840 [00:35<00:28, 309.50 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15720/23840 [00:35<00:16, 502.72 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15043/23840 [00:35<00:28, 307.35 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15062/23840 [00:35<00:26, 334.37 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15772/23840 [00:36<00:16, 503.39 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15087/23840 [00:35<00:26, 332.32 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15108/23840 [00:35<00:24, 361.14 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15825/23840 [00:36<00:15, 508.18 examples/s]Tokenizing train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15133/23840 [00:35<00:24, 358.90 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15155/23840 [00:35<00:22, 386.65 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15181/23840 [00:35<00:22, 385.61 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15900/23840 [00:36<00:15, 500.50 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15205/23840 [00:35<00:20, 411.53 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15232/23840 [00:35<00:20, 412.29 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15256/23840 [00:35<00:19, 435.14 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15975/23840 [00:36<00:15, 495.84 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15283/23840 [00:36<00:19, 435.69 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15307/23840 [00:35<00:18, 452.98 examples/s]Tokenizing train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15331/23840 [00:36<00:19, 442.23 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15380/23840 [00:36<00:18, 459.05 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15378/23840 [00:36<00:18, 446.50 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15428/23840 [00:36<00:18, 463.55 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16044/23840 [00:36<00:22, 343.28 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15450/23840 [00:36<00:18, 457.20 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15480/23840 [00:36<00:17, 477.00 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16094/23840 [00:36<00:20, 370.38 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15502/23840 [00:36<00:17, 467.48 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16143/23840 [00:37<00:19, 394.12 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15551/23840 [00:36<00:17, 473.16 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15574/23840 [00:36<00:17, 468.43 examples/s]Tokenizing train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15602/23840 [00:36<00:17, 478.62 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16211/23840 [00:37<00:18, 409.34 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15625/23840 [00:36<00:17, 476.17 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15653/23840 [00:36<00:16, 484.71 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16261/23840 [00:37<00:17, 426.96 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15676/23840 [00:36<00:16, 481.44 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15706/23840 [00:36<00:16, 492.53 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16312/23840 [00:37<00:16, 443.81 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15730/23840 [00:36<00:16, 491.42 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15760/23840 [00:36<00:16, 503.60 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16363/23840 [00:37<00:16, 451.92 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15781/23840 [00:37<00:16, 493.29 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16411/23840 [00:37<00:16, 456.65 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15836/23840 [00:37<00:15, 501.75 examples/s]Tokenizing train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15835/23840 [00:37<00:15, 502.53 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16461/23840 [00:37<00:15, 464.52 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15910/23840 [00:37<00:16, 494.56 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15910/23840 [00:37<00:15, 496.41 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16510/23840 [00:37<00:15, 467.86 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16558/23840 [00:37<00:15, 468.49 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15984/23840 [00:37<00:16, 490.83 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15982/23840 [00:37<00:16, 488.25 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16609/23840 [00:37<00:15, 475.87 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16659/23840 [00:38<00:15, 478.14 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16711/23840 [00:38<00:14, 486.46 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16043/23840 [00:37<00:23, 332.45 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16045/23840 [00:37<00:23, 331.68 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16765/23840 [00:38<00:14, 497.28 examples/s]Tokenizing train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 16090/23840 [00:37<00:21, 355.30 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16093/23840 [00:37<00:21, 358.27 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16817/23840 [00:38<00:14, 498.99 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16139/23840 [00:37<00:20, 380.99 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16142/23840 [00:38<00:20, 382.71 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16872/23840 [00:38<00:13, 510.13 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16206/23840 [00:38<00:19, 398.14 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16210/23840 [00:38<00:19, 400.23 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16947/23840 [00:38<00:13, 503.43 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16254/23840 [00:38<00:18, 414.55 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16259/23840 [00:38<00:18, 417.77 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 16999/23840 [00:38<00:13, 506.59 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16304/23840 [00:38<00:17, 433.51 examples/s]Tokenizing train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16309/23840 [00:38<00:17, 436.07 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16356/23840 [00:38<00:16, 442.44 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16375/23840 [00:38<00:16, 444.08 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16404/23840 [00:38<00:16, 448.87 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16422/23840 [00:38<00:16, 448.29 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17072/23840 [00:39<00:19, 339.38 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16471/23840 [00:38<00:16, 455.30 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16475/23840 [00:38<00:16, 456.69 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17119/23840 [00:39<00:18, 362.98 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16520/23840 [00:38<00:15, 460.66 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16523/23840 [00:38<00:15, 459.16 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17165/23840 [00:39<00:17, 381.62 examples/s]Tokenizing train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16567/23840 [00:38<00:15, 460.88 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16572/23840 [00:38<00:15, 463.08 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17211/23840 [00:39<00:16, 398.62 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16617/23840 [00:38<00:15, 470.40 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16622/23840 [00:39<00:15, 471.10 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17262/23840 [00:39<00:15, 424.98 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16666/23840 [00:38<00:15, 473.71 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16671/23840 [00:39<00:15, 472.60 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17335/23840 [00:39<00:14, 443.78 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16717/23840 [00:39<00:14, 478.55 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16722/23840 [00:39<00:14, 477.25 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17388/23840 [00:39<00:13, 463.31 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16770/23840 [00:39<00:14, 489.32 examples/s]Tokenizing train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16775/23840 [00:39<00:14, 491.25 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17439/23840 [00:39<00:13, 473.63 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16820/23840 [00:39<00:14, 491.19 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16826/23840 [00:39<00:14, 493.78 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17490/23840 [00:39<00:13, 477.11 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16874/23840 [00:39<00:13, 500.75 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16879/23840 [00:39<00:13, 499.49 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17542/23840 [00:40<00:12, 486.39 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16949/23840 [00:39<00:13, 496.98 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16954/23840 [00:39<00:13, 494.98 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17594/23840 [00:40<00:12, 492.91 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17646/23840 [00:40<00:12, 496.26 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17000/23840 [00:39<00:20, 329.81 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17722/23840 [00:40<00:12, 494.54 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17022/23840 [00:40<00:20, 337.06 examples/s]Tokenizing train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17044/23840 [00:39<00:19, 349.26 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17773/23840 [00:40<00:12, 495.97 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17070/23840 [00:40<00:18, 361.64 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17091/23840 [00:40<00:18, 374.33 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17116/23840 [00:40<00:17, 381.09 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17849/23840 [00:40<00:12, 495.69 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17134/23840 [00:40<00:17, 386.82 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17161/23840 [00:40<00:16, 394.55 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17900/23840 [00:40<00:11, 497.70 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17200/23840 [00:40<00:16, 401.84 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17207/23840 [00:40<00:16, 409.23 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17956/23840 [00:40<00:11, 511.27 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17246/23840 [00:40<00:16, 411.92 examples/s]Tokenizing train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17255/23840 [00:40<00:15, 425.10 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17290/23840 [00:40<00:15, 417.71 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17301/23840 [00:40<00:15, 432.21 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17338/23840 [00:40<00:14, 433.57 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17352/23840 [00:40<00:14, 450.03 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18025/23840 [00:41<00:16, 348.99 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17387/23840 [00:40<00:14, 447.10 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17401/23840 [00:40<00:14, 458.87 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18072/23840 [00:41<00:15, 369.99 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17434/23840 [00:40<00:14, 452.86 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17449/23840 [00:40<00:13, 463.69 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18119/23840 [00:41<00:14, 390.84 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17481/23840 [00:40<00:14, 453.15 examples/s]Tokenizing train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17497/23840 [00:41<00:13, 467.01 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18169/23840 [00:41<00:13, 414.91 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17529/23840 [00:41<00:13, 458.13 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 17547/23840 [00:41<00:13, 472.52 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18219/23840 [00:41<00:12, 435.41 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17598/23840 [00:41<00:13, 479.83 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18268/23840 [00:41<00:12, 446.83 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17601/23840 [00:41<00:13, 461.46 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17647/23840 [00:41<00:12, 477.39 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18317/23840 [00:41<00:12, 456.24 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17672/23840 [00:41<00:13, 460.25 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17696/23840 [00:41<00:12, 478.41 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18368/23840 [00:41<00:11, 468.13 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17747/23840 [00:41<00:12, 484.77 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18418/23840 [00:42<00:11, 473.83 examples/s]Tokenizing train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17740/23840 [00:41<00:13, 454.29 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17797/23840 [00:41<00:12, 487.33 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18468/23840 [00:42<00:11, 479.99 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17787/23840 [00:41<00:13, 455.20 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18520/23840 [00:42<00:10, 489.83 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17871/23840 [00:41<00:12, 487.84 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17836/23840 [00:41<00:13, 459.94 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18571/23840 [00:42<00:10, 493.05 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17923/23840 [00:41<00:11, 495.97 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17886/23840 [00:41<00:12, 468.34 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17977/23840 [00:42<00:11, 505.50 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17940/23840 [00:41<00:12, 485.36 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18646/23840 [00:42<00:10, 492.74 examples/s]Tokenizing train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17993/23840 [00:41<00:11, 494.77 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18701/23840 [00:42<00:10, 503.97 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18753/23840 [00:42<00:10, 507.09 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18804/23840 [00:42<00:09, 504.78 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18045/23840 [00:42<00:17, 331.76 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18047/23840 [00:42<00:18, 316.21 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18090/23840 [00:42<00:16, 353.71 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18879/23840 [00:42<00:09, 500.02 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18091/23840 [00:42<00:16, 339.43 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18137/23840 [00:42<00:15, 377.16 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18931/23840 [00:43<00:09, 501.71 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18139/23840 [00:42<00:15, 367.68 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18186/23840 [00:42<00:14, 401.84 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18188/23840 [00:42<00:14, 394.28 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18233/23840 [00:42<00:13, 415.29 examples/s]Tokenizing train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18234/23840 [00:42<00:13, 409.82 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18279/23840 [00:42<00:13, 424.46 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18281/23840 [00:42<00:13, 422.54 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18327/23840 [00:42<00:12, 434.87 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18331/23840 [00:42<00:12, 439.95 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18377/23840 [00:43<00:12, 449.85 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 19000/23840 [00:43<00:17, 269.77 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18381/23840 [00:43<00:11, 456.32 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18424/23840 [00:43<00:12, 448.24 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 19046/23840 [00:43<00:16, 298.96 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18475/23840 [00:43<00:11, 462.60 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19095/23840 [00:43<00:14, 332.86 examples/s]Tokenizing train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 18453/23840 [00:43<00:11, 463.28 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18526/23840 [00:43<00:11, 474.39 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19143/23840 [00:43<00:12, 361.97 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18505/23840 [00:43<00:11, 475.63 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18575/23840 [00:43<00:11, 476.68 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19194/23840 [00:43<00:11, 393.63 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18578/23840 [00:43<00:11, 477.32 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19245/23840 [00:44<00:10, 417.80 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18649/23840 [00:43<00:10, 480.14 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18629/23840 [00:43<00:10, 481.46 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19295/23840 [00:44<00:10, 433.82 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18702/23840 [00:43<00:10, 491.62 examples/s]Tokenizing train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18681/23840 [00:43<00:10, 488.80 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18752/23840 [00:43<00:10, 491.84 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19366/23840 [00:44<00:10, 441.82 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18733/23840 [00:43<00:10, 496.54 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19418/23840 [00:44<00:09, 459.31 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18784/23840 [00:43<00:10, 496.82 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18825/23840 [00:44<00:10, 485.94 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19470/23840 [00:44<00:09, 472.46 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18874/23840 [00:44<00:10, 484.44 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18860/23840 [00:43<00:10, 497.47 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19521/23840 [00:44<00:09, 477.63 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18925/23840 [00:44<00:10, 487.71 examples/s]Tokenizing train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18910/23840 [00:44<00:09, 495.82 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19571/23840 [00:44<00:08, 480.22 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18977/23840 [00:44<00:09, 491.07 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 18962/23840 [00:44<00:09, 498.63 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19621/23840 [00:44<00:08, 483.63 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19671/23840 [00:44<00:08, 483.56 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19744/23840 [00:45<00:08, 481.34 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19797/23840 [00:45<00:08, 490.81 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 19042/23840 [00:44<00:18, 254.00 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 19019/23840 [00:44<00:19, 251.44 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19869/23840 [00:45<00:08, 481.90 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19088/23840 [00:44<00:16, 285.12 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 19064/23840 [00:44<00:16, 282.93 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19136/23840 [00:45<00:14, 318.53 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19109/23840 [00:44<00:15, 312.24 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19183/23840 [00:45<00:13, 349.73 examples/s]Tokenizing train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19156/23840 [00:44<00:13, 343.52 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19233/23840 [00:45<00:12, 382.10 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19206/23840 [00:45<00:12, 376.49 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19937/23840 [00:45<00:12, 321.91 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19282/23840 [00:45<00:11, 405.66 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19257/23840 [00:45<00:11, 406.29 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19988/23840 [00:45<00:10, 353.99 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19332/23840 [00:45<00:10, 427.93 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 19307/23840 [00:45<00:10, 426.15 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19400/23840 [00:45<00:10, 433.08 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19374/23840 [00:45<00:10, 428.55 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20044/23840 [00:46<00:13, 276.10 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19452/23840 [00:45<00:09, 452.19 examples/s]Tokenizing train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19427/23840 [00:45<00:09, 450.89 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20089/23840 [00:46<00:12, 304.49 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19503/23840 [00:45<00:09, 464.59 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19478/23840 [00:45<00:09, 465.16 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20137/23840 [00:46<00:10, 336.64 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19527/23840 [00:45<00:09, 470.69 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19576/23840 [00:45<00:09, 470.53 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20188/23840 [00:46<00:09, 373.05 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19578/23840 [00:45<00:08, 478.09 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19625/23840 [00:46<00:08, 474.77 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20239/23840 [00:46<00:08, 404.03 examples/s]Tokenizing train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19627/23840 [00:45<00:08, 479.40 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19675/23840 [00:46<00:08, 478.50 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20288/23840 [00:46<00:08, 423.86 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19677/23840 [00:46<00:08, 482.69 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19750/23840 [00:46<00:08, 481.53 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20359/23840 [00:46<00:07, 439.41 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19753/23840 [00:46<00:08, 485.38 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19803/23840 [00:46<00:08, 490.54 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20410/23840 [00:46<00:07, 456.03 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19806/23840 [00:46<00:08, 493.88 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20458/23840 [00:46<00:07, 460.13 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19874/23840 [00:46<00:08, 480.37 examples/s]Tokenizing train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19878/23840 [00:46<00:08, 485.95 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20509/23840 [00:47<00:07, 470.25 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19923/23840 [00:46<00:08, 480.17 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20558/23840 [00:47<00:06, 472.95 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20612/23840 [00:47<00:06, 486.95 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20666/23840 [00:47<00:06, 499.75 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19936/23840 [00:46<00:12, 305.23 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19986/23840 [00:47<00:12, 305.27 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19987/23840 [00:46<00:11, 340.37 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20741/23840 [00:47<00:06, 495.67 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20792/23840 [00:47<00:06, 496.87 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20843/23840 [00:47<00:06, 498.43 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20040/23840 [00:47<00:15, 249.95 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20041/23840 [00:47<00:14, 264.03 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20083/23840 [00:47<00:13, 277.09 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 20918/23840 [00:47<00:05, 492.02 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20082/23840 [00:47<00:13, 288.16 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20130/23840 [00:47<00:11, 311.28 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 20973/23840 [00:47<00:05, 505.74 examples/s]Tokenizing train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20127/23840 [00:47<00:11, 317.98 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20178/23840 [00:47<00:10, 344.61 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20176/23840 [00:47<00:10, 353.76 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20228/23840 [00:47<00:09, 377.60 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 20227/23840 [00:47<00:09, 388.20 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20276/23840 [00:47<00:08, 400.92 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21042/23840 [00:48<00:08, 339.78 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20277/23840 [00:47<00:08, 412.76 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20321/23840 [00:47<00:08, 408.17 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21086/23840 [00:48<00:07, 356.18 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20323/23840 [00:47<00:08, 420.57 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20369/23840 [00:48<00:08, 424.05 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21135/23840 [00:48<00:07, 381.58 examples/s]Tokenizing train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20372/23840 [00:47<00:07, 437.36 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20419/23840 [00:48<00:07, 443.07 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21186/23840 [00:48<00:06, 410.72 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20421/23840 [00:48<00:07, 446.68 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20467/23840 [00:48<00:07, 450.73 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21233/23840 [00:48<00:06, 425.03 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20471/23840 [00:48<00:07, 456.83 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20516/23840 [00:48<00:07, 459.31 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21283/23840 [00:48<00:05, 442.35 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 20521/23840 [00:48<00:07, 465.68 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20565/23840 [00:48<00:07, 464.64 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21334/23840 [00:48<00:05, 457.70 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20569/23840 [00:48<00:07, 466.74 examples/s]Tokenizing train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20617/23840 [00:48<00:06, 479.96 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21386/23840 [00:49<00:05, 473.19 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20622/23840 [00:48<00:06, 484.27 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20670/23840 [00:48<00:06, 492.95 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20675/23840 [00:48<00:06, 496.15 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21459/23840 [00:49<00:05, 474.66 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20720/23840 [00:48<00:06, 492.06 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21510/23840 [00:49<00:04, 481.65 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20748/23840 [00:48<00:06, 485.59 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20794/23840 [00:48<00:06, 490.73 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21563/23840 [00:49<00:04, 491.85 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20800/23840 [00:48<00:06, 489.90 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20845/23840 [00:49<00:06, 493.28 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21615/23840 [00:49<00:04, 498.93 examples/s]Tokenizing train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20850/23840 [00:48<00:06, 490.07 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 20918/23840 [00:49<00:05, 488.06 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21689/23840 [00:49<00:04, 494.88 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 20925/23840 [00:49<00:05, 488.43 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 20973/23840 [00:49<00:05, 502.20 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21740/23840 [00:49<00:04, 493.31 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 20980/23840 [00:49<00:05, 503.18 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21813/23840 [00:49<00:04, 488.38 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21864/23840 [00:50<00:04, 492.82 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21041/23840 [00:49<00:08, 331.46 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21916/23840 [00:50<00:03, 497.44 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21039/23840 [00:49<00:08, 322.23 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21083/23840 [00:49<00:07, 346.64 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21969/23840 [00:50<00:03, 503.98 examples/s]Tokenizing train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21081/23840 [00:49<00:08, 338.82 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21129/23840 [00:49<00:07, 368.80 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21124/23840 [00:49<00:07, 356.69 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21177/23840 [00:49<00:06, 393.01 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21171/23840 [00:49<00:06, 382.67 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21223/23840 [00:50<00:06, 407.44 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22021/23840 [00:50<00:05, 333.06 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21218/23840 [00:49<00:06, 403.87 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21270/23840 [00:50<00:06, 422.36 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22066/23840 [00:50<00:04, 354.89 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21266/23840 [00:50<00:06, 422.32 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21319/23840 [00:50<00:05, 438.48 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22115/23840 [00:50<00:04, 384.19 examples/s]Tokenizing train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21315/23840 [00:50<00:05, 438.03 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21369/23840 [00:50<00:05, 452.76 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22164/23840 [00:50<00:04, 409.39 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21365/23840 [00:50<00:05, 454.99 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21418/23840 [00:50<00:05, 458.99 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22212/23840 [00:50<00:03, 424.10 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21467/23840 [00:50<00:05, 466.34 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 21437/23840 [00:50<00:05, 462.73 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22262/23840 [00:51<00:03, 441.75 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21516/23840 [00:50<00:04, 471.62 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21486/23840 [00:50<00:05, 468.57 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22309/23840 [00:51<00:03, 444.58 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21567/23840 [00:50<00:04, 480.64 examples/s]Tokenizing train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21535/23840 [00:50<00:04, 473.13 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22358/23840 [00:51<00:03, 455.74 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21620/23840 [00:50<00:04, 488.96 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21587/23840 [00:50<00:04, 483.66 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22408/23840 [00:51<00:03, 464.93 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21637/23840 [00:50<00:04, 486.52 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22456/23840 [00:51<00:02, 463.94 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21694/23840 [00:51<00:04, 485.00 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22506/23840 [00:51<00:02, 471.31 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 21711/23840 [00:50<00:04, 488.69 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21768/23840 [00:51<00:04, 482.58 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22576/23840 [00:51<00:02, 466.92 examples/s]Tokenizing train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21781/23840 [00:51<00:04, 476.12 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21842/23840 [00:51<00:04, 483.38 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22627/23840 [00:51<00:02, 475.55 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21832/23840 [00:51<00:04, 480.64 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21894/23840 [00:51<00:03, 489.65 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22678/23840 [00:51<00:02, 483.25 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21883/23840 [00:51<00:04, 485.69 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21945/23840 [00:51<00:03, 492.05 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22729/23840 [00:51<00:02, 488.59 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21934/23840 [00:51<00:03, 490.42 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21988/23840 [00:51<00:03, 500.70 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22802/23840 [00:52<00:02, 483.20 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22000/23840 [00:51<00:05, 335.40 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22874/23840 [00:52<00:02, 480.44 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22043/23840 [00:51<00:05, 351.47 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22927/23840 [00:52<00:01, 489.56 examples/s]Tokenizing train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22043/23840 [00:51<00:05, 319.56 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22090/23840 [00:52<00:04, 373.76 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 22978/23840 [00:52<00:01, 492.05 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22087/23840 [00:51<00:05, 343.29 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22137/23840 [00:52<00:04, 394.73 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22133/23840 [00:52<00:04, 366.81 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22186/23840 [00:52<00:03, 417.62 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22183/23840 [00:52<00:04, 395.87 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22253/23840 [00:52<00:03, 426.56 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23041/23840 [00:52<00:02, 329.84 examples/s]Tokenizing train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22250/23840 [00:52<00:03, 411.05 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23090/23840 [00:52<00:02, 354.28 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22322/23840 [00:52<00:03, 434.48 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 22318/23840 [00:52<00:03, 423.39 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23136/23840 [00:53<00:01, 374.14 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22368/23840 [00:52<00:03, 437.27 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22364/23840 [00:52<00:03, 431.12 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23186/23840 [00:53<00:01, 402.04 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22419/23840 [00:52<00:03, 451.82 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22414/23840 [00:52<00:03, 446.30 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23237/23840 [00:53<00:01, 427.30 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22467/23840 [00:52<00:02, 458.57 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22463/23840 [00:52<00:03, 453.52 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23286/23840 [00:53<00:01, 440.76 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22516/23840 [00:52<00:02, 464.99 examples/s]Tokenizing train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22513/23840 [00:52<00:02, 463.96 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23334/23840 [00:53<00:01, 449.25 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22589/23840 [00:53<00:02, 468.48 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22587/23840 [00:53<00:02, 470.00 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23402/23840 [00:53<00:00, 448.55 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22638/23840 [00:53<00:02, 471.02 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 22638/23840 [00:53<00:02, 477.06 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23451/23840 [00:53<00:00, 456.24 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22689/23840 [00:53<00:02, 478.56 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22690/23840 [00:53<00:02, 487.32 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22741/23840 [00:53<00:02, 487.95 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23521/23840 [00:53<00:00, 456.20 examples/s]Tokenizing train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22742/23840 [00:53<00:02, 493.21 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23573/23840 [00:53<00:00, 468.69 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22817/23840 [00:53<00:02, 490.90 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22792/23840 [00:53<00:02, 491.31 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23622/23840 [00:54<00:00, 472.80 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22889/23840 [00:53<00:01, 483.16 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22866/23840 [00:53<00:02, 486.98 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23693/23840 [00:54<00:00, 471.63 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22941/23840 [00:53<00:01, 490.19 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22916/23840 [00:53<00:01, 488.87 examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23745/23840 [00:54<00:00, 479.49 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 22993/23840 [00:53<00:01, 494.57 examples/s]Tokenizing train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 22968/23840 [00:53<00:01, 494.70 examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23798/23840 [00:54<00:00, 490.62 examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:54<00:00, 436.29 examples/s]
Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23019/23840 [00:54<00:02, 316.40 examples/s]Truncating train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23063/23840 [00:54<00:02, 328.07 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23065/23840 [00:54<00:02, 341.73 examples/s]Truncating train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6000/23840 [00:00<00:00, 53137.19 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23107/23840 [00:54<00:02, 347.58 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23106/23840 [00:54<00:02, 354.97 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23154/23840 [00:54<00:01, 370.55 examples/s]Truncating train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18000/23840 [00:00<00:00, 64854.88 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23148/23840 [00:54<00:01, 368.97 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23204/23840 [00:54<00:01, 397.80 examples/s]Tokenizing train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 23197/23840 [00:54<00:01, 396.93 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23256/23840 [00:54<00:01, 425.83 examples/s]Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:00<00:00, 48393.13 examples/s]
Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23248/23840 [00:54<00:01, 424.99 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23304/23840 [00:54<00:01, 437.62 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23294/23840 [00:54<00:01, 433.95 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23353/23840 [00:54<00:01, 449.61 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23340/23840 [00:54<00:01, 439.18 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23423/23840 [00:55<00:00, 454.69 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23386/23840 [00:54<00:01, 440.64 examples/s]Tokenizing train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23434/23840 [00:55<00:00, 450.59 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23493/23840 [00:55<00:00, 455.69 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23545/23840 [00:55<00:00, 467.38 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 23502/23840 [00:55<00:00, 449.14 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23594/23840 [00:55<00:00, 472.85 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23553/23840 [00:55<00:00, 462.22 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23644/23840 [00:55<00:00, 478.24 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23603/23840 [00:55<00:00, 469.36 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23693/23840 [00:55<00:00, 477.79 examples/s]Tokenizing train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23675/23840 [00:55<00:00, 467.20 examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23745/23840 [00:55<00:00, 484.70 examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23728/23840 [00:55<00:00, 480.27 examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23797/23840 [00:55<00:00, 493.37 examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23779/23840 [00:55<00:00, 483.74 examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 23831/23840 [00:55<00:00, 491.18 examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:56<00:00, 425.46 examples/s]
Truncating train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:56<00:00, 425.62 examples/s]
Truncating train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6000/23840 [00:00<00:00, 51832.94 examples/s]Truncating train dataset:   0%|          | 0/23840 [00:00<?, ? examples/s]Truncating train dataset:  25%|â–ˆâ–ˆâ–Œ       | 6000/23840 [00:00<00:00, 50991.37 examples/s]Truncating train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18000/23840 [00:00<00:00, 62698.10 examples/s]Truncating train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18000/23840 [00:00<00:00, 62387.92 examples/s]Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:00<00:00, 47453.43 examples/s]
Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23840/23840 [00:00<00:00, 47106.39 examples/s]
No checkpoint found. Starting training from scratch...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128002}.
Applying formatting function to eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Applying formatting function to eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Applying formatting function to eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Applying formatting function to eval dataset:   7%|â–‹         | 395/5960 [00:00<00:01, 3892.20 examples/s]Applying formatting function to eval dataset:   7%|â–‹         | 388/5960 [00:00<00:01, 3819.27 examples/s]Applying formatting function to eval dataset:   6%|â–‹         | 377/5960 [00:00<00:01, 3711.59 examples/s]Applying formatting function to eval dataset:  14%|â–ˆâ–Ž        | 809/5960 [00:00<00:01, 4028.48 examples/s]Applying formatting function to eval dataset:  13%|â–ˆâ–Ž        | 790/5960 [00:00<00:01, 3933.30 examples/s]Applying formatting function to eval dataset:  13%|â–ˆâ–Ž        | 774/5960 [00:00<00:01, 3846.13 examples/s]Applying formatting function to eval dataset:  20%|â–ˆâ–‰        | 1188/5960 [00:00<00:01, 2977.96 examples/s]Applying formatting function to eval dataset:  20%|â–ˆâ–‰        | 1187/5960 [00:00<00:01, 2932.77 examples/s]Applying formatting function to eval dataset:  24%|â–ˆâ–ˆâ–Ž       | 1404/5960 [00:00<00:01, 3289.09 examples/s]Applying formatting function to eval dataset:  27%|â–ˆâ–ˆâ–‹       | 1581/5960 [00:00<00:01, 3286.45 examples/s]Applying formatting function to eval dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 1907/5960 [00:00<00:01, 2251.69 examples/s]Applying formatting function to eval dataset:  28%|â–ˆâ–ˆâ–Š       | 1687/5960 [00:00<00:02, 1941.30 examples/s]Applying formatting function to eval dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 2197/5960 [00:00<00:01, 2195.16 examples/s]Applying formatting function to eval dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 2000/5960 [00:00<00:02, 1822.87 examples/s]Applying formatting function to eval dataset:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 2000/5960 [00:00<00:02, 1965.36 examples/s]Applying formatting function to eval dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2606/5960 [00:00<00:01, 2593.78 examples/s]Applying formatting function to eval dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 2377/5960 [00:00<00:01, 2188.91 examples/s]Applying formatting function to eval dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 2378/5960 [00:00<00:01, 2335.45 examples/s]Applying formatting function to eval dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2774/5960 [00:01<00:01, 2565.53 examples/s]Applying formatting function to eval dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2772/5960 [00:01<00:01, 2696.73 examples/s]Applying formatting function to eval dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3000/5960 [00:01<00:01, 2562.56 examples/s]Applying formatting function to eval dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 3400/5960 [00:01<00:00, 2885.96 examples/s]Applying formatting function to eval dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 3187/5960 [00:01<00:01, 2501.94 examples/s]Applying formatting function to eval dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 3188/5960 [00:01<00:01, 2567.77 examples/s]Applying formatting function to eval dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3812/5960 [00:01<00:00, 3186.39 examples/s]Applying formatting function to eval dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3576/5960 [00:01<00:00, 2807.32 examples/s]Applying formatting function to eval dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3573/5960 [00:01<00:00, 2854.77 examples/s]Applying formatting function to eval dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3981/5960 [00:01<00:00, 3102.88 examples/s]Applying formatting function to eval dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3974/5960 [00:01<00:00, 3134.77 examples/s]Applying formatting function to eval dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 4198/5960 [00:01<00:00, 2790.65 examples/s]Applying formatting function to eval dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4605/5960 [00:01<00:00, 3089.28 examples/s]Applying formatting function to eval dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 4384/5960 [00:01<00:00, 2670.78 examples/s]Applying formatting function to eval dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 4381/5960 [00:01<00:00, 2679.62 examples/s]Applying formatting function to eval dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4782/5960 [00:01<00:00, 2964.88 examples/s]Applying formatting function to eval dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5000/5960 [00:01<00:00, 2882.51 examples/s]Applying formatting function to eval dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4776/5960 [00:01<00:00, 2965.84 examples/s]Applying formatting function to eval dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 5397/5960 [00:01<00:00, 3139.67 examples/s]Applying formatting function to eval dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 5187/5960 [00:01<00:00, 2756.23 examples/s]Applying formatting function to eval dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 5188/5960 [00:01<00:00, 2736.58 examples/s]Applying formatting function to eval dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5809/5960 [00:01<00:00, 3386.36 examples/s]Applying formatting function to eval dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 5578/5960 [00:02<00:00, 3018.17 examples/s]Applying formatting function to eval dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 5575/5960 [00:02<00:00, 2991.52 examples/s]Applying formatting function to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:02<00:00, 2866.00 examples/s]
Adding EOS to eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Adding EOS to eval dataset:   9%|â–Š         | 518/5960 [00:00<00:01, 5105.20 examples/s]Applying formatting function to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:02<00:00, 2792.19 examples/s]Applying formatting function to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:02<00:00, 2727.53 examples/s]
Adding EOS to eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Applying formatting function to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:02<00:00, 2751.63 examples/s]Applying formatting function to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:02<00:00, 2676.16 examples/s]
Adding EOS to eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Adding EOS to eval dataset:   8%|â–Š         | 504/5960 [00:00<00:01, 4963.75 examples/s]Adding EOS to eval dataset:   8%|â–Š         | 501/5960 [00:00<00:01, 4935.43 examples/s]Adding EOS to eval dataset:  21%|â–ˆâ–ˆ        | 1260/5960 [00:00<00:01, 4498.62 examples/s]Adding EOS to eval dataset:  30%|â–ˆâ–ˆâ–‰       | 1783/5960 [00:00<00:00, 4762.51 examples/s]Adding EOS to eval dataset:  17%|â–ˆâ–‹        | 1000/5960 [00:00<00:01, 4123.34 examples/s]Adding EOS to eval dataset:  21%|â–ˆâ–ˆ        | 1251/5960 [00:00<00:01, 4375.09 examples/s]Adding EOS to eval dataset:  25%|â–ˆâ–ˆâ–Œ       | 1499/5960 [00:00<00:00, 4474.89 examples/s]Adding EOS to eval dataset:  30%|â–ˆâ–ˆâ–‰       | 1765/5960 [00:00<00:00, 4649.25 examples/s]Adding EOS to eval dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2519/5960 [00:00<00:00, 4446.38 examples/s]Adding EOS to eval dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1994/5960 [00:00<00:00, 4646.73 examples/s]Adding EOS to eval dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 2252/5960 [00:00<00:00, 4216.53 examples/s]Adding EOS to eval dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3000/5960 [00:00<00:00, 4260.79 examples/s]Adding EOS to eval dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2767/5960 [00:00<00:00, 4497.49 examples/s]Adding EOS to eval dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2541/5960 [00:00<00:00, 4199.09 examples/s]Adding EOS to eval dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3516/5960 [00:00<00:00, 4503.14 examples/s]Adding EOS to eval dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3256/5960 [00:00<00:00, 4244.61 examples/s]Adding EOS to eval dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3000/5960 [00:00<00:00, 4001.15 examples/s]Adding EOS to eval dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4000/5960 [00:00<00:00, 4070.73 examples/s]Adding EOS to eval dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3770/5960 [00:00<00:00, 4495.87 examples/s]Adding EOS to eval dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3502/5960 [00:00<00:00, 4285.52 examples/s]Adding EOS to eval dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4519/5960 [00:01<00:00, 4359.32 examples/s]Adding EOS to eval dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4253/5960 [00:00<00:00, 4010.72 examples/s]Adding EOS to eval dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4000/5960 [00:00<00:00, 3859.43 examples/s]Adding EOS to eval dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5000/5960 [00:01<00:00, 4221.23 examples/s]Adding EOS to eval dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4767/5960 [00:01<00:00, 4307.18 examples/s]Adding EOS to eval dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4505/5960 [00:01<00:00, 4167.77 examples/s]Adding EOS to eval dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 5526/5960 [00:01<00:00, 4496.70 examples/s]Adding EOS to eval dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 5255/5960 [00:01<00:00, 4147.44 examples/s]Adding EOS to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:01<00:00, 4357.22 examples/s]
Adding EOS to eval dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5000/5960 [00:01<00:00, 4027.02 examples/s]Adding EOS to eval dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5768/5960 [00:01<00:00, 4406.89 examples/s]Adding EOS to eval dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5506/5960 [00:01<00:00, 4295.83 examples/s]Tokenizing eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Adding EOS to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:01<00:00, 4256.56 examples/s]
Tokenizing eval dataset:   1%|          | 41/5960 [00:00<00:14, 399.40 examples/s]Adding EOS to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:01<00:00, 4060.34 examples/s]Adding EOS to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:01<00:00, 4154.30 examples/s]
Tokenizing eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Tokenizing eval dataset:   2%|â–         | 90/5960 [00:00<00:13, 445.56 examples/s]Tokenizing eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Tokenizing eval dataset:   1%|          | 38/5960 [00:00<00:16, 369.72 examples/s]Tokenizing eval dataset:   2%|â–         | 138/5960 [00:00<00:12, 458.32 examples/s]Tokenizing eval dataset:   1%|          | 39/5960 [00:00<00:15, 380.05 examples/s]Tokenizing eval dataset:   1%|â–         | 86/5960 [00:00<00:13, 428.45 examples/s]Tokenizing eval dataset:   3%|â–Ž         | 186/5960 [00:00<00:12, 463.52 examples/s]Tokenizing eval dataset:   1%|â–         | 87/5960 [00:00<00:13, 430.63 examples/s]Tokenizing eval dataset:   2%|â–         | 132/5960 [00:00<00:13, 438.44 examples/s]Tokenizing eval dataset:   4%|â–         | 234/5960 [00:00<00:12, 466.67 examples/s]Tokenizing eval dataset:   2%|â–         | 133/5960 [00:00<00:13, 442.01 examples/s]Tokenizing eval dataset:   3%|â–Ž         | 178/5960 [00:00<00:13, 441.20 examples/s]Tokenizing eval dataset:   5%|â–         | 282/5960 [00:00<00:12, 467.95 examples/s]Tokenizing eval dataset:   3%|â–Ž         | 178/5960 [00:00<00:13, 441.52 examples/s]Tokenizing eval dataset:   4%|â–         | 225/5960 [00:00<00:12, 448.07 examples/s]Tokenizing eval dataset:   6%|â–Œ         | 329/5960 [00:00<00:12, 467.34 examples/s]Tokenizing eval dataset:   4%|â–         | 225/5960 [00:00<00:12, 446.39 examples/s]Tokenizing eval dataset:   5%|â–         | 270/5960 [00:00<00:12, 444.81 examples/s]Tokenizing eval dataset:   6%|â–‹         | 379/5960 [00:00<00:11, 473.50 examples/s]Tokenizing eval dataset:   5%|â–         | 270/5960 [00:00<00:12, 442.96 examples/s]Tokenizing eval dataset:   5%|â–Œ         | 319/5960 [00:00<00:12, 455.26 examples/s]Tokenizing eval dataset:   7%|â–‹         | 431/5960 [00:00<00:11, 483.96 examples/s]Tokenizing eval dataset:   5%|â–Œ         | 318/5960 [00:00<00:12, 452.19 examples/s]Tokenizing eval dataset:   6%|â–Œ         | 367/5960 [00:00<00:12, 461.43 examples/s]Tokenizing eval dataset:   8%|â–Š         | 481/5960 [00:01<00:11, 485.67 examples/s]Tokenizing eval dataset:   6%|â–Œ         | 367/5960 [00:00<00:12, 458.06 examples/s]Tokenizing eval dataset:   7%|â–‹         | 418/5960 [00:00<00:11, 467.95 examples/s]Tokenizing eval dataset:   9%|â–‰         | 533/5960 [00:01<00:11, 492.96 examples/s]Tokenizing eval dataset:   7%|â–‹         | 416/5960 [00:00<00:11, 466.39 examples/s]Tokenizing eval dataset:   8%|â–Š         | 467/5960 [00:01<00:11, 471.06 examples/s]Tokenizing eval dataset:  10%|â–ˆ         | 604/5960 [00:01<00:11, 482.88 examples/s]Tokenizing eval dataset:   8%|â–Š         | 463/5960 [00:01<00:11, 466.76 examples/s]Tokenizing eval dataset:   9%|â–Š         | 519/5960 [00:01<00:11, 482.20 examples/s]Tokenizing eval dataset:   9%|â–Š         | 516/5960 [00:01<00:11, 478.51 examples/s]Tokenizing eval dataset:  11%|â–ˆâ–        | 677/5960 [00:01<00:10, 480.76 examples/s]Tokenizing eval dataset:  10%|â–‰         | 590/5960 [00:01<00:11, 473.97 examples/s]Tokenizing eval dataset:  10%|â–‰         | 587/5960 [00:01<00:11, 473.40 examples/s]Tokenizing eval dataset:  11%|â–ˆ         | 640/5960 [00:01<00:11, 477.65 examples/s]Tokenizing eval dataset:  13%|â–ˆâ–Ž        | 751/5960 [00:01<00:10, 483.03 examples/s]Tokenizing eval dataset:  11%|â–ˆ         | 637/5960 [00:01<00:11, 479.59 examples/s]Tokenizing eval dataset:  13%|â–ˆâ–Ž        | 804/5960 [00:01<00:10, 493.04 examples/s]Tokenizing eval dataset:  12%|â–ˆâ–        | 710/5960 [00:01<00:11, 469.89 examples/s]Tokenizing eval dataset:  12%|â–ˆâ–        | 708/5960 [00:01<00:11, 474.26 examples/s]Tokenizing eval dataset:  13%|â–ˆâ–Ž        | 763/5960 [00:01<00:10, 481.69 examples/s]Tokenizing eval dataset:  15%|â–ˆâ–        | 879/5960 [00:01<00:10, 490.94 examples/s]Tokenizing eval dataset:  13%|â–ˆâ–Ž        | 760/5960 [00:01<00:10, 482.37 examples/s]Tokenizing eval dataset:  14%|â–ˆâ–Ž        | 813/5960 [00:01<00:10, 486.05 examples/s]Tokenizing eval dataset:  16%|â–ˆâ–Œ        | 931/5960 [00:01<00:10, 494.71 examples/s]Tokenizing eval dataset:  14%|â–ˆâ–Ž        | 811/5960 [00:01<00:10, 487.42 examples/s]Tokenizing eval dataset:  14%|â–ˆâ–        | 862/5960 [00:01<00:10, 486.26 examples/s]Tokenizing eval dataset:  14%|â–ˆâ–        | 860/5960 [00:01<00:10, 485.95 examples/s]Tokenizing eval dataset:  15%|â–ˆâ–Œ        | 914/5960 [00:01<00:10, 492.61 examples/s]Tokenizing eval dataset:  15%|â–ˆâ–Œ        | 912/5960 [00:01<00:10, 492.38 examples/s]Tokenizing eval dataset:  16%|â–ˆâ–Œ        | 964/5960 [00:02<00:10, 491.61 examples/s]Tokenizing eval dataset:  17%|â–ˆâ–‹        | 992/5960 [00:02<00:15, 321.16 examples/s]Tokenizing eval dataset:  17%|â–ˆâ–‹        | 1022/5960 [00:02<00:15, 316.38 examples/s]Tokenizing eval dataset:  16%|â–ˆâ–Œ        | 968/5960 [00:02<00:16, 294.27 examples/s]Tokenizing eval dataset:  18%|â–ˆâ–Š        | 1048/5960 [00:02<00:18, 263.54 examples/s]Tokenizing eval dataset:  18%|â–ˆâ–Š        | 1068/5960 [00:02<00:14, 342.36 examples/s]Tokenizing eval dataset:  18%|â–ˆâ–Š        | 1095/5960 [00:02<00:16, 295.69 examples/s]Tokenizing eval dataset:  19%|â–ˆâ–Š        | 1112/5960 [00:02<00:13, 361.84 examples/s]Tokenizing eval dataset:  19%|â–ˆâ–‰        | 1142/5960 [00:02<00:14, 326.65 examples/s]Tokenizing eval dataset:  19%|â–ˆâ–‰        | 1158/5960 [00:02<00:12, 383.98 examples/s]Tokenizing eval dataset:  17%|â–ˆâ–‹        | 1022/5960 [00:02<00:20, 243.57 examples/s]Tokenizing eval dataset:  20%|â–ˆâ–‰        | 1190/5960 [00:02<00:13, 356.99 examples/s]Tokenizing eval dataset:  20%|â–ˆâ–ˆ        | 1204/5960 [00:02<00:11, 399.25 examples/s]Tokenizing eval dataset:  18%|â–ˆâ–Š        | 1067/5960 [00:02<00:17, 276.70 examples/s]Tokenizing eval dataset:  21%|â–ˆâ–ˆ        | 1237/5960 [00:03<00:12, 380.81 examples/s]Tokenizing eval dataset:  21%|â–ˆâ–ˆ        | 1249/5960 [00:02<00:11, 411.16 examples/s]Tokenizing eval dataset:  19%|â–ˆâ–Š        | 1111/5960 [00:02<00:15, 305.54 examples/s]Tokenizing eval dataset:  22%|â–ˆâ–ˆâ–       | 1284/5960 [00:03<00:11, 401.16 examples/s]Tokenizing eval dataset:  22%|â–ˆâ–ˆâ–       | 1295/5960 [00:02<00:11, 421.18 examples/s]Tokenizing eval dataset:  19%|â–ˆâ–‰        | 1158/5960 [00:02<00:14, 338.07 examples/s]Tokenizing eval dataset:  22%|â–ˆâ–ˆâ–       | 1335/5960 [00:03<00:10, 425.79 examples/s]Tokenizing eval dataset:  23%|â–ˆâ–ˆâ–Ž       | 1343/5960 [00:03<00:10, 435.77 examples/s]Tokenizing eval dataset:  20%|â–ˆâ–ˆ        | 1204/5960 [00:03<00:13, 363.78 examples/s]Tokenizing eval dataset:  23%|â–ˆâ–ˆâ–Ž       | 1383/5960 [00:03<00:10, 438.74 examples/s]Tokenizing eval dataset:  23%|â–ˆâ–ˆâ–Ž       | 1389/5960 [00:03<00:10, 438.55 examples/s]Tokenizing eval dataset:  21%|â–ˆâ–ˆ        | 1251/5960 [00:03<00:12, 386.39 examples/s]Tokenizing eval dataset:  24%|â–ˆâ–ˆâ–       | 1433/5960 [00:03<00:09, 452.77 examples/s]Tokenizing eval dataset:  24%|â–ˆâ–ˆâ–       | 1437/5960 [00:03<00:10, 446.52 examples/s]Tokenizing eval dataset:  22%|â–ˆâ–ˆâ–       | 1297/5960 [00:03<00:11, 402.48 examples/s]Tokenizing eval dataset:  25%|â–ˆâ–ˆâ–       | 1483/5960 [00:03<00:09, 463.72 examples/s]Tokenizing eval dataset:  25%|â–ˆâ–ˆâ–       | 1485/5960 [00:03<00:09, 452.61 examples/s]Tokenizing eval dataset:  23%|â–ˆâ–ˆâ–Ž       | 1345/5960 [00:03<00:10, 422.70 examples/s]Tokenizing eval dataset:  26%|â–ˆâ–ˆâ–Œ       | 1536/5960 [00:03<00:09, 477.18 examples/s]Tokenizing eval dataset:  27%|â–ˆâ–ˆâ–‹       | 1585/5960 [00:03<00:09, 476.34 examples/s]Tokenizing eval dataset:  24%|â–ˆâ–ˆâ–Ž       | 1413/5960 [00:03<00:10, 431.59 examples/s]Tokenizing eval dataset:  24%|â–ˆâ–ˆâ–       | 1459/5960 [00:03<00:10, 437.98 examples/s]Tokenizing eval dataset:  28%|â–ˆâ–ˆâ–Š       | 1656/5960 [00:03<00:09, 473.40 examples/s]Tokenizing eval dataset:  26%|â–ˆâ–ˆâ–Œ       | 1531/5960 [00:03<00:15, 277.43 examples/s]Tokenizing eval dataset:  25%|â–ˆâ–ˆâ–Œ       | 1509/5960 [00:03<00:09, 451.29 examples/s]Tokenizing eval dataset:  29%|â–ˆâ–ˆâ–Š       | 1709/5960 [00:03<00:08, 483.93 examples/s]Tokenizing eval dataset:  26%|â–ˆâ–ˆâ–‹       | 1579/5960 [00:03<00:13, 315.91 examples/s]Tokenizing eval dataset:  26%|â–ˆâ–ˆâ–Œ       | 1556/5960 [00:03<00:09, 455.39 examples/s]Tokenizing eval dataset:  27%|â–ˆâ–ˆâ–‹       | 1623/5960 [00:03<00:12, 343.23 examples/s]Tokenizing eval dataset:  30%|â–ˆâ–ˆâ–‰       | 1781/5960 [00:04<00:08, 476.59 examples/s]Tokenizing eval dataset:  27%|â–ˆâ–ˆâ–‹       | 1603/5960 [00:03<00:09, 455.92 examples/s]Tokenizing eval dataset:  28%|â–ˆâ–ˆâ–Š       | 1674/5960 [00:04<00:11, 381.17 examples/s]Tokenizing eval dataset:  31%|â–ˆâ–ˆâ–ˆ       | 1830/5960 [00:04<00:08, 477.21 examples/s]Tokenizing eval dataset:  28%|â–ˆâ–ˆâ–Š       | 1650/5960 [00:03<00:09, 456.49 examples/s]Tokenizing eval dataset:  29%|â–ˆâ–ˆâ–‰       | 1723/5960 [00:04<00:10, 406.14 examples/s]Tokenizing eval dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 1885/5960 [00:04<00:08, 492.85 examples/s]Tokenizing eval dataset:  29%|â–ˆâ–ˆâ–Š       | 1700/5960 [00:04<00:09, 467.86 examples/s]Tokenizing eval dataset:  30%|â–ˆâ–ˆâ–‰       | 1773/5960 [00:04<00:09, 429.69 examples/s]Tokenizing eval dataset:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 1937/5960 [00:04<00:08, 495.78 examples/s]Tokenizing eval dataset:  31%|â–ˆâ–ˆâ–ˆ       | 1820/5960 [00:04<00:09, 437.10 examples/s]Tokenizing eval dataset:  30%|â–ˆâ–ˆâ–‰       | 1770/5960 [00:04<00:09, 458.22 examples/s]Tokenizing eval dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1990/5960 [00:04<00:07, 501.84 examples/s]Tokenizing eval dataset:  30%|â–ˆâ–ˆâ–ˆ       | 1817/5960 [00:04<00:08, 460.47 examples/s]Tokenizing eval dataset:  31%|â–ˆâ–ˆâ–ˆâ–      | 1873/5960 [00:04<00:08, 460.45 examples/s]Tokenizing eval dataset:  31%|â–ˆâ–ˆâ–ˆâ–      | 1869/5960 [00:04<00:08, 473.14 examples/s]Tokenizing eval dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 1924/5960 [00:04<00:08, 470.87 examples/s]Tokenizing eval dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 1918/5960 [00:04<00:08, 476.46 examples/s]Tokenizing eval dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1977/5960 [00:04<00:08, 484.57 examples/s]Tokenizing eval dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 2041/5960 [00:04<00:12, 313.29 examples/s]Tokenizing eval dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1972/5960 [00:04<00:08, 489.71 examples/s]Tokenizing eval dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 2088/5960 [00:04<00:11, 344.03 examples/s]Tokenizing eval dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 2135/5960 [00:05<00:10, 371.70 examples/s]Tokenizing eval dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 2179/5960 [00:05<00:09, 388.01 examples/s]Tokenizing eval dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 2040/5960 [00:04<00:12, 306.42 examples/s]Tokenizing eval dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 2225/5960 [00:05<00:09, 405.00 examples/s]Tokenizing eval dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 2086/5960 [00:05<00:11, 334.67 examples/s]Tokenizing eval dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 2041/5960 [00:05<00:12, 312.92 examples/s]Tokenizing eval dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 2271/5960 [00:05<00:08, 417.63 examples/s]Tokenizing eval dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 2133/5960 [00:05<00:10, 362.63 examples/s]Tokenizing eval dataset:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 2087/5960 [00:05<00:11, 339.24 examples/s]Tokenizing eval dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 2320/5960 [00:05<00:08, 434.58 examples/s]Tokenizing eval dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 2177/5960 [00:05<00:10, 378.20 examples/s]Tokenizing eval dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 2135/5960 [00:05<00:10, 365.73 examples/s]Tokenizing eval dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 2369/5960 [00:05<00:08, 447.06 examples/s]Tokenizing eval dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 2223/5960 [00:05<00:09, 396.86 examples/s]Tokenizing eval dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 2179/5960 [00:05<00:09, 381.32 examples/s]Tokenizing eval dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2421/5960 [00:05<00:07, 463.51 examples/s]Tokenizing eval dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 2268/5960 [00:05<00:09, 407.55 examples/s]Tokenizing eval dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 2224/5960 [00:05<00:09, 396.90 examples/s]Tokenizing eval dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2471/5960 [00:05<00:07, 470.07 examples/s]Tokenizing eval dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 2315/5960 [00:05<00:08, 420.80 examples/s]Tokenizing eval dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 2269/5960 [00:05<00:09, 408.24 examples/s]Tokenizing eval dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2521/5960 [00:05<00:07, 475.80 examples/s]Tokenizing eval dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 2364/5960 [00:05<00:08, 435.05 examples/s]Tokenizing eval dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 2316/5960 [00:05<00:08, 422.47 examples/s]Tokenizing eval dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2415/5960 [00:05<00:07, 452.54 examples/s]Tokenizing eval dataset:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 2364/5960 [00:05<00:08, 435.30 examples/s]Tokenizing eval dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2595/5960 [00:06<00:07, 478.25 examples/s]Tokenizing eval dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2465/5960 [00:05<00:07, 462.65 examples/s]Tokenizing eval dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2415/5960 [00:05<00:07, 452.62 examples/s]Tokenizing eval dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2646/5960 [00:06<00:06, 483.53 examples/s]Tokenizing eval dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2515/5960 [00:06<00:07, 470.55 examples/s]Tokenizing eval dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2464/5960 [00:05<00:07, 461.96 examples/s]Tokenizing eval dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2699/5960 [00:06<00:06, 493.79 examples/s]Tokenizing eval dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2514/5960 [00:06<00:07, 470.51 examples/s]Tokenizing eval dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2752/5960 [00:06<00:06, 498.46 examples/s]Tokenizing eval dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2586/5960 [00:06<00:07, 465.29 examples/s]Tokenizing eval dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2803/5960 [00:06<00:06, 498.23 examples/s]Tokenizing eval dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2634/5960 [00:06<00:07, 468.85 examples/s]Tokenizing eval dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2585/5960 [00:06<00:07, 467.10 examples/s]Tokenizing eval dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2685/5960 [00:06<00:06, 476.87 examples/s]Tokenizing eval dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2635/5960 [00:06<00:07, 473.13 examples/s]Tokenizing eval dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2875/5960 [00:06<00:06, 485.97 examples/s]Tokenizing eval dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2736/5960 [00:06<00:06, 481.65 examples/s]Tokenizing eval dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2687/5960 [00:06<00:06, 483.15 examples/s]Tokenizing eval dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2928/5960 [00:06<00:06, 496.20 examples/s]Tokenizing eval dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2785/5960 [00:06<00:06, 479.53 examples/s]Tokenizing eval dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2739/5960 [00:06<00:06, 489.79 examples/s]Tokenizing eval dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2789/5960 [00:06<00:06, 488.77 examples/s]Tokenizing eval dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2856/5960 [00:06<00:06, 473.50 examples/s]Tokenizing eval dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2907/5960 [00:06<00:06, 482.49 examples/s]Tokenizing eval dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2861/5960 [00:06<00:06, 479.73 examples/s]Tokenizing eval dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3000/5960 [00:07<00:09, 327.62 examples/s]Tokenizing eval dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2957/5960 [00:06<00:06, 483.88 examples/s]Tokenizing eval dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2914/5960 [00:06<00:06, 491.17 examples/s]Tokenizing eval dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3047/5960 [00:07<00:08, 351.49 examples/s]Tokenizing eval dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3095/5960 [00:07<00:07, 376.95 examples/s]Tokenizing eval dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2987/5960 [00:07<00:06, 487.03 examples/s]Tokenizing eval dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 3145/5960 [00:07<00:06, 403.67 examples/s]Tokenizing eval dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3020/5960 [00:07<00:09, 309.61 examples/s]Tokenizing eval dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 3195/5960 [00:07<00:06, 423.77 examples/s]Tokenizing eval dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3065/5960 [00:07<00:08, 336.08 examples/s]Tokenizing eval dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3245/5960 [00:07<00:06, 442.85 examples/s]Tokenizing eval dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3045/5960 [00:07<00:09, 314.27 examples/s]Tokenizing eval dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3112/5960 [00:07<00:07, 363.32 examples/s]Tokenizing eval dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 3295/5960 [00:07<00:05, 455.47 examples/s]Tokenizing eval dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3091/5960 [00:07<00:08, 340.20 examples/s]Tokenizing eval dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 3161/5960 [00:07<00:07, 390.06 examples/s]Tokenizing eval dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 3346/5960 [00:07<00:05, 468.29 examples/s]Tokenizing eval dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 3139/5960 [00:07<00:07, 366.59 examples/s]Tokenizing eval dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3207/5960 [00:07<00:06, 406.15 examples/s]Tokenizing eval dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 3416/5960 [00:07<00:05, 461.61 examples/s]Tokenizing eval dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 3188/5960 [00:07<00:07, 391.96 examples/s]Tokenizing eval dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3257/5960 [00:07<00:06, 426.29 examples/s]Tokenizing eval dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3234/5960 [00:07<00:06, 407.23 examples/s]Tokenizing eval dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3465/5960 [00:08<00:05, 464.35 examples/s]Tokenizing eval dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 3306/5960 [00:07<00:06, 439.85 examples/s]Tokenizing eval dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 3286/5960 [00:07<00:06, 433.48 examples/s]Tokenizing eval dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3515/5960 [00:08<00:05, 472.09 examples/s]Tokenizing eval dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 3355/5960 [00:07<00:05, 450.21 examples/s]Tokenizing eval dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3564/5960 [00:08<00:05, 473.80 examples/s]Tokenizing eval dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 3356/5960 [00:08<00:05, 440.30 examples/s]Tokenizing eval dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 3421/5960 [00:08<00:05, 443.81 examples/s]Tokenizing eval dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3638/5960 [00:08<00:04, 477.02 examples/s]Tokenizing eval dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3469/5960 [00:08<00:05, 450.70 examples/s]Tokenizing eval dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 3423/5960 [00:08<00:05, 438.90 examples/s]Tokenizing eval dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3692/5960 [00:08<00:04, 488.70 examples/s]Tokenizing eval dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3521/5960 [00:08<00:05, 464.15 examples/s]Tokenizing eval dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 3470/5960 [00:08<00:05, 444.48 examples/s]Tokenizing eval dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3569/5960 [00:08<00:05, 466.15 examples/s]Tokenizing eval dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3765/5960 [00:08<00:04, 484.42 examples/s]Tokenizing eval dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3520/5960 [00:08<00:05, 454.12 examples/s]Tokenizing eval dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3619/5960 [00:08<00:04, 473.41 examples/s]Tokenizing eval dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3818/5960 [00:08<00:04, 494.14 examples/s]Tokenizing eval dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3567/5960 [00:08<00:05, 455.86 examples/s]Tokenizing eval dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3670/5960 [00:08<00:04, 482.43 examples/s]Tokenizing eval dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3869/5960 [00:08<00:04, 494.16 examples/s]Tokenizing eval dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3640/5960 [00:08<00:04, 464.12 examples/s]Tokenizing eval dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3721/5960 [00:08<00:04, 486.20 examples/s]Tokenizing eval dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3921/5960 [00:08<00:04, 496.87 examples/s]Tokenizing eval dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3692/5960 [00:08<00:04, 476.21 examples/s]Tokenizing eval dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3975/5960 [00:09<00:03, 506.75 examples/s]Tokenizing eval dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3796/5960 [00:08<00:04, 488.41 examples/s]Tokenizing eval dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3764/5960 [00:08<00:04, 473.62 examples/s]Tokenizing eval dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3849/5960 [00:09<00:04, 494.81 examples/s]Tokenizing eval dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3817/5960 [00:08<00:04, 482.82 examples/s]Tokenizing eval dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3902/5960 [00:09<00:04, 501.71 examples/s]Tokenizing eval dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3867/5960 [00:09<00:04, 484.74 examples/s]Tokenizing eval dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4046/5960 [00:09<00:05, 338.97 examples/s]Tokenizing eval dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3954/5960 [00:09<00:03, 504.08 examples/s]Tokenizing eval dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3917/5960 [00:09<00:04, 487.18 examples/s]Tokenizing eval dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4095/5960 [00:09<00:05, 366.26 examples/s]Tokenizing eval dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3971/5960 [00:09<00:03, 498.85 examples/s]Tokenizing eval dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 4145/5960 [00:09<00:04, 393.72 examples/s]Tokenizing eval dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 4196/5960 [00:09<00:04, 419.42 examples/s]Tokenizing eval dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4023/5960 [00:09<00:05, 336.10 examples/s]Tokenizing eval dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 4244/5960 [00:09<00:03, 431.82 examples/s]Tokenizing eval dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4068/5960 [00:09<00:05, 356.42 examples/s]Tokenizing eval dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4024/5960 [00:09<00:06, 321.33 examples/s]Tokenizing eval dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4293/5960 [00:09<00:03, 445.63 examples/s]Tokenizing eval dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 4118/5960 [00:09<00:04, 385.77 examples/s]Tokenizing eval dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4070/5960 [00:09<00:05, 348.27 examples/s]Tokenizing eval dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 4343/5960 [00:09<00:03, 457.05 examples/s]Tokenizing eval dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 4165/5960 [00:09<00:04, 402.08 examples/s]Tokenizing eval dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 4119/5960 [00:09<00:04, 379.05 examples/s]Tokenizing eval dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 4394/5960 [00:10<00:03, 468.32 examples/s]Tokenizing eval dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 4213/5960 [00:09<00:04, 420.09 examples/s]Tokenizing eval dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 4166/5960 [00:09<00:04, 397.38 examples/s]Tokenizing eval dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4443/5960 [00:10<00:03, 471.56 examples/s]Tokenizing eval dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4261/5960 [00:10<00:03, 432.03 examples/s]Tokenizing eval dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 4214/5960 [00:10<00:04, 416.27 examples/s]Tokenizing eval dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4494/5960 [00:10<00:03, 481.15 examples/s]Tokenizing eval dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4309/5960 [00:10<00:03, 442.80 examples/s]Tokenizing eval dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4261/5960 [00:10<00:03, 427.85 examples/s]Tokenizing eval dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4544/5960 [00:10<00:02, 484.02 examples/s]Tokenizing eval dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 4357/5960 [00:10<00:03, 451.90 examples/s]Tokenizing eval dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4310/5960 [00:10<00:03, 440.79 examples/s]Tokenizing eval dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4596/5960 [00:10<00:02, 490.92 examples/s]Tokenizing eval dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4408/5960 [00:10<00:03, 465.17 examples/s]Tokenizing eval dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 4360/5960 [00:10<00:03, 454.55 examples/s]Tokenizing eval dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4646/5960 [00:10<00:02, 490.90 examples/s]Tokenizing eval dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4410/5960 [00:10<00:03, 459.45 examples/s]Tokenizing eval dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4480/5960 [00:10<00:03, 466.74 examples/s]Tokenizing eval dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4722/5960 [00:10<00:02, 495.19 examples/s]Tokenizing eval dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4459/5960 [00:10<00:03, 464.97 examples/s]Tokenizing eval dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4529/5960 [00:10<00:03, 469.66 examples/s]Tokenizing eval dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4775/5960 [00:10<00:02, 497.63 examples/s]Tokenizing eval dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4510/5960 [00:10<00:03, 475.29 examples/s]Tokenizing eval dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4581/5960 [00:10<00:02, 481.66 examples/s]Tokenizing eval dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4825/5960 [00:10<00:02, 495.98 examples/s]Tokenizing eval dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4560/5960 [00:10<00:02, 480.58 examples/s]Tokenizing eval dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4654/5960 [00:10<00:02, 480.45 examples/s]Tokenizing eval dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4877/5960 [00:11<00:02, 501.30 examples/s]Tokenizing eval dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 4610/5960 [00:10<00:02, 482.30 examples/s]Tokenizing eval dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4704/5960 [00:10<00:02, 480.14 examples/s]Tokenizing eval dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4930/5960 [00:11<00:02, 503.77 examples/s]Tokenizing eval dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 4660/5960 [00:10<00:02, 483.53 examples/s]Tokenizing eval dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4758/5960 [00:11<00:02, 492.76 examples/s]Tokenizing eval dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4710/5960 [00:11<00:02, 482.85 examples/s]Tokenizing eval dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 4763/5960 [00:11<00:02, 494.42 examples/s]Tokenizing eval dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4832/5960 [00:11<00:02, 486.11 examples/s]Tokenizing eval dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4885/5960 [00:11<00:02, 494.32 examples/s]Tokenizing eval dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5000/5960 [00:11<00:03, 315.09 examples/s]Tokenizing eval dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4837/5960 [00:11<00:02, 489.09 examples/s]Tokenizing eval dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4937/5960 [00:11<00:02, 495.79 examples/s]Tokenizing eval dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5046/5960 [00:11<00:02, 341.54 examples/s]Tokenizing eval dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4889/5960 [00:11<00:02, 495.83 examples/s]Tokenizing eval dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 5093/5960 [00:11<00:02, 366.78 examples/s]Tokenizing eval dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 4939/5960 [00:11<00:02, 496.30 examples/s]Tokenizing eval dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 5139/5960 [00:11<00:02, 387.37 examples/s]Tokenizing eval dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 5186/5960 [00:11<00:01, 405.51 examples/s]Tokenizing eval dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5000/5960 [00:11<00:03, 309.35 examples/s]Tokenizing eval dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 5236/5960 [00:12<00:01, 428.91 examples/s]Tokenizing eval dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5044/5960 [00:11<00:02, 331.96 examples/s]Tokenizing eval dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5000/5960 [00:11<00:03, 302.48 examples/s]Tokenizing eval dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 5283/5960 [00:12<00:01, 437.90 examples/s]Tokenizing eval dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 5088/5960 [00:12<00:02, 352.66 examples/s]Tokenizing eval dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5044/5960 [00:11<00:02, 327.71 examples/s]Tokenizing eval dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 5333/5960 [00:12<00:01, 453.89 examples/s]Tokenizing eval dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 5133/5960 [00:12<00:02, 371.90 examples/s]Tokenizing eval dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 5088/5960 [00:12<00:02, 350.63 examples/s]Tokenizing eval dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 5385/5960 [00:12<00:01, 466.67 examples/s]Tokenizing eval dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 5180/5960 [00:12<00:01, 392.60 examples/s]Tokenizing eval dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 5133/5960 [00:12<00:02, 370.84 examples/s]Tokenizing eval dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 5434/5960 [00:12<00:01, 470.62 examples/s]Tokenizing eval dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 5227/5960 [00:12<00:01, 411.03 examples/s]Tokenizing eval dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 5179/5960 [00:12<00:01, 390.93 examples/s]Tokenizing eval dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5483/5960 [00:12<00:01, 471.87 examples/s]Tokenizing eval dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 5226/5960 [00:12<00:01, 411.05 examples/s]Tokenizing eval dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 5296/5960 [00:12<00:01, 425.93 examples/s]Tokenizing eval dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 5532/5960 [00:12<00:00, 475.06 examples/s]Tokenizing eval dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 5344/5960 [00:12<00:01, 439.08 examples/s]Tokenizing eval dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 5583/5960 [00:12<00:00, 480.32 examples/s]Tokenizing eval dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 5296/5960 [00:12<00:01, 426.53 examples/s]Tokenizing eval dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 5393/5960 [00:12<00:01, 450.39 examples/s]Tokenizing eval dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5635/5960 [00:12<00:00, 481.31 examples/s]Tokenizing eval dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 5344/5960 [00:12<00:01, 439.16 examples/s]Tokenizing eval dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5444/5960 [00:12<00:01, 462.39 examples/s]Tokenizing eval dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5688/5960 [00:12<00:00, 489.62 examples/s]Tokenizing eval dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 5393/5960 [00:12<00:01, 448.82 examples/s]Tokenizing eval dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5493/5960 [00:12<00:00, 467.35 examples/s]Tokenizing eval dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5443/5960 [00:12<00:01, 460.52 examples/s]Tokenizing eval dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5761/5960 [00:13<00:00, 486.19 examples/s]Tokenizing eval dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 5542/5960 [00:12<00:00, 470.13 examples/s]Tokenizing eval dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5491/5960 [00:12<00:01, 464.39 examples/s]Tokenizing eval dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5813/5960 [00:13<00:00, 493.14 examples/s]Tokenizing eval dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5592/5960 [00:13<00:00, 477.02 examples/s]Tokenizing eval dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 5541/5960 [00:13<00:00, 472.08 examples/s]Tokenizing eval dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5869/5960 [00:13<00:00, 509.03 examples/s]Tokenizing eval dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5641/5960 [00:13<00:00, 476.23 examples/s]Tokenizing eval dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 5591/5960 [00:13<00:00, 477.12 examples/s]Tokenizing eval dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5922/5960 [00:13<00:00, 511.15 examples/s]Tokenizing eval dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5692/5960 [00:13<00:00, 483.55 examples/s]Tokenizing eval dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5665/5960 [00:13<00:00, 479.82 examples/s]Tokenizing eval dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5766/5960 [00:13<00:00, 485.09 examples/s]Tokenizing eval dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 5715/5960 [00:13<00:00, 481.81 examples/s]Tokenizing eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:13<00:00, 435.21 examples/s]
Truncating eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Tokenizing eval dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5815/5960 [00:13<00:00, 484.59 examples/s]Tokenizing eval dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5788/5960 [00:13<00:00, 482.31 examples/s]Truncating eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:00<00:00, 53341.01 examples/s]Truncating eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:00<00:00, 51420.13 examples/s]
Tokenizing eval dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5871/5960 [00:13<00:00, 501.52 examples/s]The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128002}.
Tokenizing eval dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 5840/5960 [00:13<00:00, 488.87 examples/s]Tokenizing eval dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5923/5960 [00:13<00:00, 503.26 examples/s]Tokenizing eval dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5892/5960 [00:13<00:00, 496.68 examples/s]Tokenizing eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5944/5960 [00:13<00:00, 501.77 examples/s]Tokenizing eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:14<00:00, 425.15 examples/s]
Truncating eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Truncating eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:00<00:00, 52062.46 examples/s]Truncating eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:00<00:00, 50081.14 examples/s]
Tokenizing eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:14<00:00, 423.38 examples/s]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128002}.
Truncating eval dataset:   0%|          | 0/5960 [00:00<?, ? examples/s]Truncating eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:00<00:00, 51635.32 examples/s]Truncating eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5960/5960 [00:00<00:00, 49735.49 examples/s]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128002}.
  0%|          | 0/2235 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ubuntu/Part-II-Project/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ubuntu/Part-II-Project/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ubuntu/Part-II-Project/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ubuntu/Part-II-Project/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  0%|          | 1/2235 [00:06<4:14:39,  6.84s/it]  0%|          | 2/2235 [00:12<3:50:05,  6.18s/it]  0%|          | 3/2235 [00:18<3:42:13,  5.97s/it]  0%|          | 4/2235 [00:24<3:38:41,  5.88s/it]  0%|          | 5/2235 [00:29<3:36:56,  5.84s/it]  0%|          | 6/2235 [00:35<3:35:56,  5.81s/it]  0%|          | 7/2235 [00:41<3:32:42,  5.73s/it]  0%|          | 8/2235 [00:46<3:32:54,  5.74s/it]  0%|          | 9/2235 [00:52<3:33:02,  5.74s/it]  0%|          | 10/2235 [00:58<3:33:12,  5.75s/it]                                                   {'loss': 2.17, 'grad_norm': 1.0390625, 'learning_rate': 4.999799952130615e-06, 'entropy': 1.93502978682518, 'num_tokens': 321153.0, 'mean_token_accuracy': 0.55805484354496, 'epoch': 0.01}
  0%|          | 10/2235 [00:58<3:33:12,  5.75s/it]  0%|          | 11/2235 [01:04<3:33:30,  5.76s/it]  1%|          | 12/2235 [01:09<3:33:42,  5.77s/it]  1%|          | 13/2235 [01:15<3:33:54,  5.78s/it]  1%|          | 14/2235 [01:21<3:34:02,  5.78s/it]  1%|          | 15/2235 [01:27<3:34:13,  5.79s/it]  1%|          | 16/2235 [01:33<3:34:17,  5.79s/it]  1%|          | 17/2235 [01:38<3:34:29,  5.80s/it]  1%|          | 18/2235 [01:44<3:34:29,  5.81s/it]  1%|          | 19/2235 [01:50<3:34:45,  5.81s/it]  1%|          | 20/2235 [01:56<3:35:06,  5.83s/it]                                                   {'loss': 2.1291, 'grad_norm': 0.83203125, 'learning_rate': 4.999108469734243e-06, 'entropy': 1.941935008764267, 'num_tokens': 641054.0, 'mean_token_accuracy': 0.5642893046140671, 'epoch': 0.03}
  1%|          | 20/2235 [01:56<3:35:06,  5.83s/it]  1%|          | 21/2235 [02:02<3:35:07,  5.83s/it]  1%|          | 22/2235 [02:08<3:34:53,  5.83s/it]  1%|          | 23/2235 [02:13<3:34:28,  5.82s/it]  1%|          | 24/2235 [02:19<3:34:02,  5.81s/it]  1%|          | 25/2235 [02:25<3:33:52,  5.81s/it]  1%|          | 26/2235 [02:31<3:33:35,  5.80s/it]  1%|          | 27/2235 [02:37<3:33:29,  5.80s/it]  1%|â–         | 28/2235 [02:42<3:33:26,  5.80s/it]  1%|â–         | 29/2235 [02:48<3:33:27,  5.81s/it]  1%|â–         | 30/2235 [02:54<3:33:26,  5.81s/it]                                                   {'loss': 2.0684, 'grad_norm': 0.8828125, 'learning_rate': 4.99792321967295e-06, 'entropy': 1.9218070387840271, 'num_tokens': 962067.0, 'mean_token_accuracy': 0.5696366220712662, 'epoch': 0.04}
  1%|â–         | 30/2235 [02:54<3:33:26,  5.81s/it]  1%|â–         | 31/2235 [03:00<3:33:30,  5.81s/it]  1%|â–         | 32/2235 [03:06<3:33:34,  5.82s/it]  1%|â–         | 33/2235 [03:12<3:33:45,  5.82s/it]  2%|â–         | 34/2235 [03:17<3:33:48,  5.83s/it]  2%|â–         | 35/2235 [03:23<3:33:48,  5.83s/it]  2%|â–         | 36/2235 [03:29<3:33:30,  5.83s/it]  2%|â–         | 37/2235 [03:35<3:33:07,  5.82s/it]  2%|â–         | 38/2235 [03:41<3:33:06,  5.82s/it]  2%|â–         | 39/2235 [03:46<3:33:06,  5.82s/it]  2%|â–         | 40/2235 [03:52<3:33:12,  5.83s/it]                                                   {'loss': 1.9905, 'grad_norm': 0.84765625, 'learning_rate': 4.9962444361255095e-06, 'entropy': 1.8745918810367583, 'num_tokens': 1282548.0, 'mean_token_accuracy': 0.5810596495866776, 'epoch': 0.05}
  2%|â–         | 40/2235 [03:52<3:33:12,  5.83s/it]  2%|â–         | 41/2235 [03:58<3:33:16,  5.83s/it]  2%|â–         | 42/2235 [04:04<3:33:23,  5.84s/it]  2%|â–         | 43/2235 [04:10<3:33:17,  5.84s/it]  2%|â–         | 44/2235 [04:16<3:33:11,  5.84s/it]  2%|â–         | 45/2235 [04:22<3:33:05,  5.84s/it]  2%|â–         | 46/2235 [04:27<3:32:29,  5.82s/it]  2%|â–         | 47/2235 [04:33<3:32:33,  5.83s/it]  2%|â–         | 48/2235 [04:39<3:32:33,  5.83s/it]  2%|â–         | 49/2235 [04:45<3:32:36,  5.84s/it]  2%|â–         | 50/2235 [04:51<3:32:01,  5.82s/it]                                                   {'loss': 2.0208, 'grad_norm': 0.8671875, 'learning_rate': 4.994072450781818e-06, 'entropy': 1.953045701980591, 'num_tokens': 1604533.0, 'mean_token_accuracy': 0.5761570334434509, 'epoch': 0.07}
  2%|â–         | 50/2235 [04:51<3:32:01,  5.82s/it]  2%|â–         | 51/2235 [04:57<3:32:33,  5.84s/it]  2%|â–         | 52/2235 [05:02<3:32:34,  5.84s/it]  2%|â–         | 53/2235 [05:08<3:32:22,  5.84s/it]  2%|â–         | 54/2235 [05:14<3:31:28,  5.82s/it]  2%|â–         | 55/2235 [05:20<3:31:03,  5.81s/it]  3%|â–Ž         | 56/2235 [05:26<3:30:40,  5.80s/it]  3%|â–Ž         | 57/2235 [05:31<3:30:17,  5.79s/it]  3%|â–Ž         | 58/2235 [05:37<3:29:35,  5.78s/it]  3%|â–Ž         | 59/2235 [05:43<3:29:18,  5.77s/it]  3%|â–Ž         | 60/2235 [05:49<3:29:19,  5.77s/it]                                                   {'loss': 1.9583, 'grad_norm': 0.84765625, 'learning_rate': 4.991407692777363e-06, 'entropy': 1.904010885953903, 'num_tokens': 1926452.0, 'mean_token_accuracy': 0.5879896700382232, 'epoch': 0.08}
  3%|â–Ž         | 60/2235 [05:49<3:29:19,  5.77s/it]  3%|â–Ž         | 61/2235 [05:54<3:29:31,  5.78s/it]  3%|â–Ž         | 62/2235 [06:00<3:29:34,  5.79s/it]  3%|â–Ž         | 63/2235 [06:06<3:29:39,  5.79s/it]  3%|â–Ž         | 64/2235 [06:12<3:29:29,  5.79s/it]  3%|â–Ž         | 65/2235 [06:18<3:29:44,  5.80s/it]  3%|â–Ž         | 66/2235 [06:23<3:29:27,  5.79s/it]  3%|â–Ž         | 67/2235 [06:29<3:29:39,  5.80s/it]  3%|â–Ž         | 68/2235 [06:35<3:29:55,  5.81s/it]  3%|â–Ž         | 69/2235 [06:41<3:30:05,  5.82s/it]  3%|â–Ž         | 70/2235 [06:47<3:30:23,  5.83s/it]                                                   {'loss': 1.8925, 'grad_norm': 0.8046875, 'learning_rate': 4.988250688608436e-06, 'entropy': 1.8487972915172577, 'num_tokens': 2246630.0, 'mean_token_accuracy': 0.6002583980560303, 'epoch': 0.09}
  3%|â–Ž         | 70/2235 [06:47<3:30:23,  5.83s/it]  3%|â–Ž         | 71/2235 [06:53<3:30:30,  5.84s/it]  3%|â–Ž         | 72/2235 [06:58<3:29:43,  5.82s/it]  3%|â–Ž         | 73/2235 [07:04<3:29:23,  5.81s/it]  3%|â–Ž         | 74/2235 [07:10<3:29:11,  5.81s/it]  3%|â–Ž         | 75/2235 [07:16<3:28:48,  5.80s/it]  3%|â–Ž         | 76/2235 [07:22<3:28:31,  5.79s/it]  3%|â–Ž         | 77/2235 [07:27<3:28:13,  5.79s/it]  3%|â–Ž         | 78/2235 [07:33<3:27:53,  5.78s/it]  4%|â–Ž         | 79/2235 [07:39<3:27:39,  5.78s/it]  4%|â–Ž         | 80/2235 [07:45<3:27:32,  5.78s/it]                                                   {'loss': 1.8558, 'grad_norm': 0.859375, 'learning_rate': 4.984602062028104e-06, 'entropy': 1.8192457973957061, 'num_tokens': 2566504.0, 'mean_token_accuracy': 0.6073613822460174, 'epoch': 0.11}
  4%|â–Ž         | 80/2235 [07:45<3:27:32,  5.78s/it]  4%|â–Ž         | 81/2235 [07:50<3:27:00,  5.77s/it]  4%|â–Ž         | 82/2235 [07:56<3:27:01,  5.77s/it]  4%|â–Ž         | 83/2235 [08:02<3:26:35,  5.76s/it]  4%|â–         | 84/2235 [08:08<3:26:25,  5.76s/it]  4%|â–         | 85/2235 [08:13<3:26:18,  5.76s/it]  4%|â–         | 86/2235 [08:19<3:26:27,  5.76s/it]  4%|â–         | 87/2235 [08:25<3:26:32,  5.77s/it]  4%|â–         | 88/2235 [08:31<3:26:29,  5.77s/it]  4%|â–         | 89/2235 [08:36<3:26:30,  5.77s/it]  4%|â–         | 90/2235 [08:42<3:26:28,  5.78s/it]                                                   {'loss': 1.887, 'grad_norm': 0.859375, 'learning_rate': 4.980462533922975e-06, 'entropy': 1.8611939787864684, 'num_tokens': 2888239.0, 'mean_token_accuracy': 0.601941442489624, 'epoch': 0.12}
  4%|â–         | 90/2235 [08:42<3:26:28,  5.78s/it]  4%|â–         | 91/2235 [08:48<3:26:30,  5.78s/it]  4%|â–         | 92/2235 [08:54<3:26:13,  5.77s/it]  4%|â–         | 93/2235 [09:00<3:26:09,  5.77s/it]  4%|â–         | 94/2235 [09:05<3:26:18,  5.78s/it]  4%|â–         | 95/2235 [09:11<3:26:24,  5.79s/it]  4%|â–         | 96/2235 [09:17<3:26:24,  5.79s/it]  4%|â–         | 97/2235 [09:23<3:26:38,  5.80s/it]  4%|â–         | 98/2235 [09:29<3:26:55,  5.81s/it]  4%|â–         | 99/2235 [09:34<3:27:15,  5.82s/it]  4%|â–         | 100/2235 [09:40<3:27:32,  5.83s/it]                                                    {'loss': 1.7783, 'grad_norm': 0.73828125, 'learning_rate': 4.975832922170765e-06, 'entropy': 1.7594948589801789, 'num_tokens': 3208828.0, 'mean_token_accuracy': 0.6214654892683029, 'epoch': 0.13}
  4%|â–         | 100/2235 [09:40<3:27:32,  5.83s/it]  5%|â–         | 101/2235 [09:46<3:27:29,  5.83s/it]  5%|â–         | 102/2235 [09:52<3:27:17,  5.83s/it]  5%|â–         | 103/2235 [09:58<3:27:03,  5.83s/it]  5%|â–         | 104/2235 [10:04<3:26:45,  5.82s/it]  5%|â–         | 105/2235 [10:09<3:26:28,  5.82s/it]  5%|â–         | 106/2235 [10:15<3:26:10,  5.81s/it]  5%|â–         | 107/2235 [10:21<3:25:54,  5.81s/it]  5%|â–         | 108/2235 [10:27<3:25:43,  5.80s/it]  5%|â–         | 109/2235 [10:33<3:25:35,  5.80s/it]  5%|â–         | 110/2235 [10:38<3:25:27,  5.80s/it]                                                    {'loss': 1.7752, 'grad_norm': 0.87109375, 'learning_rate': 4.970714141478702e-06, 'entropy': 1.7585862576961517, 'num_tokens': 3528638.0, 'mean_token_accuracy': 0.6228155732154846, 'epoch': 0.15}
  5%|â–         | 110/2235 [10:38<3:25:27,  5.80s/it]  5%|â–         | 111/2235 [10:44<3:25:18,  5.80s/it]  5%|â–Œ         | 112/2235 [10:50<3:23:10,  5.74s/it]  5%|â–Œ         | 113/2235 [10:56<3:23:49,  5.76s/it]  5%|â–Œ         | 114/2235 [11:01<3:24:27,  5.78s/it]  5%|â–Œ         | 115/2235 [11:07<3:24:55,  5.80s/it]  5%|â–Œ         | 116/2235 [11:13<3:25:22,  5.82s/it]  5%|â–Œ         | 117/2235 [11:19<3:25:34,  5.82s/it]  5%|â–Œ         | 118/2235 [11:25<3:25:22,  5.82s/it]  5%|â–Œ         | 119/2235 [11:31<3:25:16,  5.82s/it]  5%|â–Œ         | 120/2235 [11:36<3:25:03,  5.82s/it]                                                    {'loss': 1.7466, 'grad_norm': 0.78125, 'learning_rate': 4.965107203202806e-06, 'entropy': 1.7160143613815309, 'num_tokens': 3847685.0, 'mean_token_accuracy': 0.6297763019800187, 'epoch': 0.16}
  5%|â–Œ         | 120/2235 [11:36<3:25:03,  5.82s/it]  5%|â–Œ         | 121/2235 [11:42<3:24:46,  5.81s/it]  5%|â–Œ         | 122/2235 [11:48<3:24:21,  5.80s/it]  6%|â–Œ         | 123/2235 [11:54<3:24:11,  5.80s/it]  6%|â–Œ         | 124/2235 [12:00<3:24:01,  5.80s/it]  6%|â–Œ         | 125/2235 [12:05<3:23:35,  5.79s/it]  6%|â–Œ         | 126/2235 [12:11<3:23:11,  5.78s/it]  6%|â–Œ         | 127/2235 [12:17<3:22:46,  5.77s/it]  6%|â–Œ         | 128/2235 [12:23<3:22:58,  5.78s/it]  6%|â–Œ         | 129/2235 [12:29<3:23:08,  5.79s/it]  6%|â–Œ         | 130/2235 [12:34<3:23:18,  5.79s/it]                                                    {'loss': 1.7305, 'grad_norm': 0.6328125, 'learning_rate': 4.959013215148059e-06, 'entropy': 1.7081381559371949, 'num_tokens': 4169338.0, 'mean_token_accuracy': 0.6313692450523376, 'epoch': 0.17}
  6%|â–Œ         | 130/2235 [12:34<3:23:18,  5.79s/it]  6%|â–Œ         | 131/2235 [12:40<3:23:18,  5.80s/it]  6%|â–Œ         | 132/2235 [12:46<3:23:31,  5.81s/it]  6%|â–Œ         | 133/2235 [12:52<3:23:42,  5.81s/it]  6%|â–Œ         | 134/2235 [12:58<3:23:59,  5.83s/it]  6%|â–Œ         | 135/2235 [13:03<3:24:20,  5.84s/it]  6%|â–Œ         | 136/2235 [13:09<3:24:13,  5.84s/it]  6%|â–Œ         | 137/2235 [13:15<3:24:08,  5.84s/it]  6%|â–Œ         | 138/2235 [13:21<3:23:55,  5.83s/it]  6%|â–Œ         | 139/2235 [13:27<3:23:35,  5.83s/it]  6%|â–‹         | 140/2235 [13:33<3:23:18,  5.82s/it]                                                    {'loss': 1.6634, 'grad_norm': 0.74609375, 'learning_rate': 4.952433381349538e-06, 'entropy': 1.6352466881275176, 'num_tokens': 4490028.0, 'mean_token_accuracy': 0.64282106757164, 'epoch': 0.19}
  6%|â–‹         | 140/2235 [13:33<3:23:18,  5.82s/it]  6%|â–‹         | 141/2235 [13:38<3:23:01,  5.82s/it]  6%|â–‹         | 142/2235 [13:44<3:22:44,  5.81s/it]  6%|â–‹         | 143/2235 [13:50<3:22:33,  5.81s/it]  6%|â–‹         | 144/2235 [13:56<3:22:21,  5.81s/it]  6%|â–‹         | 145/2235 [14:02<3:22:15,  5.81s/it]  7%|â–‹         | 146/2235 [14:07<3:22:10,  5.81s/it]  7%|â–‹         | 147/2235 [14:13<3:22:02,  5.81s/it]  7%|â–‹         | 148/2235 [14:19<3:22:00,  5.81s/it]  7%|â–‹         | 149/2235 [14:25<3:21:43,  5.80s/it]  7%|â–‹         | 150/2235 [14:31<3:21:28,  5.80s/it]                                                    {'loss': 1.6111, 'grad_norm': 0.765625, 'learning_rate': 4.9453690018345144e-06, 'entropy': 1.5817946016788482, 'num_tokens': 4809448.0, 'mean_token_accuracy': 0.6515047132968903, 'epoch': 0.2}
  7%|â–‹         | 150/2235 [14:31<3:21:28,  5.80s/it]  7%|â–‹         | 151/2235 [14:36<3:21:40,  5.81s/it]  7%|â–‹         | 152/2235 [14:42<3:22:01,  5.82s/it]  7%|â–‹         | 153/2235 [14:48<3:21:50,  5.82s/it]  7%|â–‹         | 154/2235 [14:54<3:22:12,  5.83s/it]  7%|â–‹         | 155/2235 [15:00<3:21:50,  5.82s/it]  7%|â–‹         | 156/2235 [15:06<3:21:40,  5.82s/it]  7%|â–‹         | 157/2235 [15:11<3:21:35,  5.82s/it]  7%|â–‹         | 158/2235 [15:17<3:21:19,  5.82s/it]  7%|â–‹         | 159/2235 [15:23<3:20:55,  5.81s/it]  7%|â–‹         | 160/2235 [15:29<3:20:32,  5.80s/it]                                                    {'loss': 1.6402, 'grad_norm': 0.6171875, 'learning_rate': 4.937821472365606e-06, 'entropy': 1.6175566017627716, 'num_tokens': 5130376.0, 'mean_token_accuracy': 0.6472060203552246, 'epoch': 0.21}
  7%|â–‹         | 160/2235 [15:29<3:20:32,  5.80s/it]  7%|â–‹         | 161/2235 [15:35<3:20:30,  5.80s/it]  7%|â–‹         | 162/2235 [15:40<3:20:13,  5.80s/it]  7%|â–‹         | 163/2235 [15:46<3:20:16,  5.80s/it]  7%|â–‹         | 164/2235 [15:52<3:20:20,  5.80s/it]  7%|â–‹         | 165/2235 [15:58<3:20:15,  5.80s/it]  7%|â–‹         | 166/2235 [16:04<3:20:19,  5.81s/it]  7%|â–‹         | 167/2235 [16:09<3:20:05,  5.81s/it]  8%|â–Š         | 168/2235 [16:15<3:20:00,  5.81s/it]  8%|â–Š         | 169/2235 [16:21<3:20:26,  5.82s/it]  8%|â–Š         | 170/2235 [16:27<3:20:45,  5.83s/it]                                                    {'loss': 1.5953, 'grad_norm': 0.6171875, 'learning_rate': 4.9297922841650005e-06, 'entropy': 1.5830468475818633, 'num_tokens': 5450089.0, 'mean_token_accuracy': 0.6534417241811752, 'epoch': 0.23}
  8%|â–Š         | 170/2235 [16:27<3:20:45,  5.83s/it]  8%|â–Š         | 171/2235 [16:33<3:20:34,  5.83s/it]  8%|â–Š         | 172/2235 [16:39<3:20:25,  5.83s/it]  8%|â–Š         | 173/2235 [16:44<3:20:14,  5.83s/it]  8%|â–Š         | 174/2235 [16:50<3:20:00,  5.82s/it]  8%|â–Š         | 175/2235 [16:56<3:19:45,  5.82s/it]  8%|â–Š         | 176/2235 [17:02<3:19:30,  5.81s/it]  8%|â–Š         | 177/2235 [17:08<3:19:20,  5.81s/it]  8%|â–Š         | 178/2235 [17:13<3:19:04,  5.81s/it]  8%|â–Š         | 179/2235 [17:19<3:18:56,  5.81s/it]  8%|â–Š         | 180/2235 [17:25<3:18:51,  5.81s/it]                                                    {'loss': 1.5502, 'grad_norm': 0.59375, 'learning_rate': 4.921283023619827e-06, 'entropy': 1.5516667246818543, 'num_tokens': 5770432.0, 'mean_token_accuracy': 0.6634119063615799, 'epoch': 0.24}
  8%|â–Š         | 180/2235 [17:25<3:18:51,  5.81s/it]  8%|â–Š         | 181/2235 [17:31<3:18:44,  5.81s/it]  8%|â–Š         | 182/2235 [17:37<3:18:33,  5.80s/it]  8%|â–Š         | 183/2235 [17:42<3:18:40,  5.81s/it]  8%|â–Š         | 184/2235 [17:48<3:18:49,  5.82s/it]  8%|â–Š         | 185/2235 [17:54<3:18:53,  5.82s/it]  8%|â–Š         | 186/2235 [18:00<3:18:50,  5.82s/it]  8%|â–Š         | 187/2235 [18:06<3:18:43,  5.82s/it]  8%|â–Š         | 188/2235 [18:12<3:18:36,  5.82s/it]  8%|â–Š         | 189/2235 [18:17<3:18:40,  5.83s/it]  9%|â–Š         | 190/2235 [18:23<3:18:33,  5.83s/it]                                                    {'loss': 1.5024, 'grad_norm': 0.609375, 'learning_rate': 4.91229537196872e-06, 'entropy': 1.5140109717845918, 'num_tokens': 6091146.0, 'mean_token_accuracy': 0.6744025558233261, 'epoch': 0.26}
  9%|â–Š         | 190/2235 [18:23<3:18:33,  5.83s/it]  9%|â–Š         | 191/2235 [18:29<3:18:20,  5.82s/it]  9%|â–Š         | 192/2235 [18:35<3:18:04,  5.82s/it]  9%|â–Š         | 193/2235 [18:41<3:17:44,  5.81s/it]  9%|â–Š         | 194/2235 [18:46<3:17:29,  5.81s/it]  9%|â–Š         | 195/2235 [18:52<3:17:20,  5.80s/it]  9%|â–‰         | 196/2235 [18:58<3:17:17,  5.81s/it]  9%|â–‰         | 197/2235 [19:04<3:15:01,  5.74s/it]  9%|â–‰         | 198/2235 [19:10<3:15:40,  5.76s/it]  9%|â–‰         | 199/2235 [19:15<3:15:55,  5.77s/it]  9%|â–‰         | 200/2235 [19:21<3:16:16,  5.79s/it]                                                    {'loss': 1.4271, 'grad_norm': 0.59765625, 'learning_rate': 4.902831104969643e-06, 'entropy': 1.4523830592632294, 'num_tokens': 6412214.0, 'mean_token_accuracy': 0.6896569788455963, 'epoch': 0.27}
  9%|â–‰         | 200/2235 [19:21<3:16:16,  5.79s/it]  9%|â–‰         | 201/2235 [19:27<3:16:13,  5.79s/it]  9%|â–‰         | 202/2235 [19:33<3:16:34,  5.80s/it]  9%|â–‰         | 203/2235 [19:39<3:16:59,  5.82s/it]  9%|â–‰         | 204/2235 [19:44<3:17:04,  5.82s/it]  9%|â–‰         | 205/2235 [19:50<3:16:58,  5.82s/it]  9%|â–‰         | 206/2235 [19:56<3:17:15,  5.83s/it]  9%|â–‰         | 207/2235 [20:02<3:17:11,  5.83s/it]  9%|â–‰         | 208/2235 [20:08<3:16:58,  5.83s/it]  9%|â–‰         | 209/2235 [20:14<3:16:54,  5.83s/it]  9%|â–‰         | 210/2235 [20:19<3:16:40,  5.83s/it]                                                    {'loss': 1.3605, 'grad_norm': 0.6171875, 'learning_rate': 4.892892092549041e-06, 'entropy': 1.3853138625621795, 'num_tokens': 6733263.0, 'mean_token_accuracy': 0.7040460795164108, 'epoch': 0.28}
  9%|â–‰         | 210/2235 [20:19<3:16:40,  5.83s/it]  9%|â–‰         | 211/2235 [20:25<3:16:21,  5.82s/it]  9%|â–‰         | 212/2235 [20:31<3:16:03,  5.81s/it] 10%|â–‰         | 213/2235 [20:37<3:15:36,  5.80s/it] 10%|â–‰         | 214/2235 [20:43<3:15:19,  5.80s/it] 10%|â–‰         | 215/2235 [20:48<3:15:09,  5.80s/it] 10%|â–‰         | 216/2235 [20:54<3:14:41,  5.79s/it] 10%|â–‰         | 217/2235 [21:00<3:14:25,  5.78s/it] 10%|â–‰         | 218/2235 [21:06<3:14:28,  5.79s/it] 10%|â–‰         | 219/2235 [21:11<3:14:10,  5.78s/it] 10%|â–‰         | 220/2235 [21:17<3:14:19,  5.79s/it]                                                    {'loss': 1.418, 'grad_norm': 0.61328125, 'learning_rate': 4.882480298432384e-06, 'entropy': 1.454261153936386, 'num_tokens': 7053836.0, 'mean_token_accuracy': 0.6933437258005142, 'epoch': 0.3}
 10%|â–‰         | 220/2235 [21:17<3:14:19,  5.79s/it] 10%|â–‰         | 221/2235 [21:23<3:14:27,  5.79s/it] 10%|â–‰         | 222/2235 [21:29<3:14:36,  5.80s/it] 10%|â–‰         | 223/2235 [21:35<3:12:41,  5.75s/it] 10%|â–ˆ         | 224/2235 [21:40<3:13:25,  5.77s/it] 10%|â–ˆ         | 225/2235 [21:46<3:13:59,  5.79s/it] 10%|â–ˆ         | 226/2235 [21:52<3:14:27,  5.81s/it] 10%|â–ˆ         | 227/2235 [21:58<3:14:48,  5.82s/it] 10%|â–ˆ         | 228/2235 [22:04<3:14:47,  5.82s/it] 10%|â–ˆ         | 229/2235 [22:10<3:14:50,  5.83s/it] 10%|â–ˆ         | 230/2235 [22:15<3:14:53,  5.83s/it]                                                    {'loss': 1.3112, 'grad_norm': 0.5546875, 'learning_rate': 4.871597779756179e-06, 'entropy': 1.3503801882267, 'num_tokens': 7373883.0, 'mean_token_accuracy': 0.7187741160392761, 'epoch': 0.31}
 10%|â–ˆ         | 230/2235 [22:15<3:14:53,  5.83s/it] 10%|â–ˆ         | 231/2235 [22:21<3:14:54,  5.84s/it] 10%|â–ˆ         | 232/2235 [22:27<3:14:55,  5.84s/it] 10%|â–ˆ         | 233/2235 [22:33<3:14:29,  5.83s/it] 10%|â–ˆ         | 234/2235 [22:39<3:14:42,  5.84s/it] 11%|â–ˆ         | 235/2235 [22:44<3:12:32,  5.78s/it] 11%|â–ˆ         | 236/2235 [22:50<3:13:00,  5.79s/it] 11%|â–ˆ         | 237/2235 [22:56<3:12:57,  5.79s/it] 11%|â–ˆ         | 238/2235 [23:02<3:12:46,  5.79s/it] 11%|â–ˆ         | 239/2235 [23:08<3:12:42,  5.79s/it] 11%|â–ˆ         | 240/2235 [23:13<3:12:33,  5.79s/it]                                                    {'loss': 1.275, 'grad_norm': 0.52734375, 'learning_rate': 4.8602466866615275e-06, 'entropy': 1.3140813708305359, 'num_tokens': 7694857.0, 'mean_token_accuracy': 0.7266584068536759, 'epoch': 0.32}
 11%|â–ˆ         | 240/2235 [23:13<3:12:33,  5.79s/it] 11%|â–ˆ         | 241/2235 [23:19<3:12:11,  5.78s/it] 11%|â–ˆ         | 242/2235 [23:25<3:12:09,  5.78s/it] 11%|â–ˆ         | 243/2235 [23:31<3:12:01,  5.78s/it] 11%|â–ˆ         | 244/2235 [23:36<3:11:42,  5.78s/it] 11%|â–ˆ         | 245/2235 [23:42<3:09:21,  5.71s/it] 11%|â–ˆ         | 246/2235 [23:48<3:10:05,  5.73s/it] 11%|â–ˆ         | 247/2235 [23:54<3:10:35,  5.75s/it] 11%|â–ˆ         | 248/2235 [23:59<3:08:35,  5.69s/it] 11%|â–ˆ         | 249/2235 [24:05<3:09:26,  5.72s/it] 11%|â–ˆ         | 250/2235 [24:11<3:09:44,  5.74s/it]                                                    {'loss': 1.2423, 'grad_norm': 0.6015625, 'learning_rate': 4.848429261869303e-06, 'entropy': 1.278803324699402, 'num_tokens': 8016509.0, 'mean_token_accuracy': 0.7344005942344666, 'epoch': 0.34}
 11%|â–ˆ         | 250/2235 [24:11<3:09:44,  5.74s/it] 11%|â–ˆ         | 251/2235 [24:17<3:10:12,  5.75s/it] 11%|â–ˆâ–        | 252/2235 [24:22<3:10:38,  5.77s/it] 11%|â–ˆâ–        | 253/2235 [24:28<3:11:01,  5.78s/it] 11%|â–ˆâ–        | 254/2235 [24:34<3:11:16,  5.79s/it] 11%|â–ˆâ–        | 255/2235 [24:40<3:11:34,  5.81s/it] 11%|â–ˆâ–        | 256/2235 [24:46<3:11:52,  5.82s/it] 11%|â–ˆâ–        | 257/2235 [24:52<3:12:14,  5.83s/it] 12%|â–ˆâ–        | 258/2235 [24:57<3:12:21,  5.84s/it] 12%|â–ˆâ–        | 259/2235 [25:03<3:12:15,  5.84s/it] 12%|â–ˆâ–        | 260/2235 [25:09<3:11:41,  5.82s/it]                                                    {'loss': 1.2146, 'grad_norm': 0.421875, 'learning_rate': 4.836147840237041e-06, 'entropy': 1.2518392741680144, 'num_tokens': 8337849.0, 'mean_token_accuracy': 0.739639064669609, 'epoch': 0.35}
 12%|â–ˆâ–        | 260/2235 [25:09<3:11:41,  5.82s/it] 12%|â–ˆâ–        | 261/2235 [25:15<3:11:00,  5.81s/it] 12%|â–ˆâ–        | 262/2235 [25:21<3:10:21,  5.79s/it] 12%|â–ˆâ–        | 263/2235 [25:26<3:09:54,  5.78s/it] 12%|â–ˆâ–        | 264/2235 [25:32<3:09:53,  5.78s/it] 12%|â–ˆâ–        | 265/2235 [25:38<3:09:46,  5.78s/it] 12%|â–ˆâ–        | 266/2235 [25:44<3:09:33,  5.78s/it] 12%|â–ˆâ–        | 267/2235 [25:49<3:09:24,  5.77s/it] 12%|â–ˆâ–        | 268/2235 [25:55<3:09:14,  5.77s/it] 12%|â–ˆâ–        | 269/2235 [26:01<3:09:08,  5.77s/it] 12%|â–ˆâ–        | 270/2235 [26:07<3:08:58,  5.77s/it]                                                    {'loss': 1.1932, 'grad_norm': 0.5625, 'learning_rate': 4.823404848297627e-06, 'entropy': 1.2184884190559386, 'num_tokens': 8657566.0, 'mean_token_accuracy': 0.7451629430055619, 'epoch': 0.36}
 12%|â–ˆâ–        | 270/2235 [26:07<3:08:58,  5.77s/it] 12%|â–ˆâ–        | 271/2235 [26:12<3:08:47,  5.77s/it] 12%|â–ˆâ–        | 272/2235 [26:18<3:06:40,  5.71s/it] 12%|â–ˆâ–        | 273/2235 [26:24<3:07:11,  5.72s/it] 12%|â–ˆâ–        | 274/2235 [26:30<3:07:32,  5.74s/it] 12%|â–ˆâ–        | 275/2235 [26:35<3:07:47,  5.75s/it] 12%|â–ˆâ–        | 276/2235 [26:41<3:07:52,  5.75s/it] 12%|â–ˆâ–        | 277/2235 [26:47<3:08:04,  5.76s/it] 12%|â–ˆâ–        | 278/2235 [26:53<3:07:40,  5.75s/it] 12%|â–ˆâ–        | 279/2235 [26:58<3:07:41,  5.76s/it] 13%|â–ˆâ–Ž        | 280/2235 [27:04<3:07:58,  5.77s/it]                                                    {'loss': 1.1978, 'grad_norm': 0.57421875, 'learning_rate': 4.810202803779862e-06, 'entropy': 1.223038411140442, 'num_tokens': 8978661.0, 'mean_token_accuracy': 0.7446862161159515, 'epoch': 0.38}
 13%|â–ˆâ–Ž        | 280/2235 [27:04<3:07:58,  5.77s/it] 13%|â–ˆâ–Ž        | 281/2235 [27:10<3:07:51,  5.77s/it] 13%|â–ˆâ–Ž        | 282/2235 [27:16<3:07:47,  5.77s/it] 13%|â–ˆâ–Ž        | 283/2235 [27:21<3:08:00,  5.78s/it] 13%|â–ˆâ–Ž        | 284/2235 [27:27<3:07:58,  5.78s/it] 13%|â–ˆâ–Ž        | 285/2235 [27:33<3:08:18,  5.79s/it] 13%|â–ˆâ–Ž        | 286/2235 [27:39<3:08:26,  5.80s/it] 13%|â–ˆâ–Ž        | 287/2235 [27:45<3:08:46,  5.81s/it] 13%|â–ˆâ–Ž        | 288/2235 [27:51<3:08:56,  5.82s/it] 13%|â–ˆâ–Ž        | 289/2235 [27:56<3:09:06,  5.83s/it] 13%|â–ˆâ–Ž        | 290/2235 [28:02<3:08:42,  5.82s/it]                                                    {'loss': 1.1948, 'grad_norm': 0.6640625, 'learning_rate': 4.796544315111019e-06, 'entropy': 1.2150287806987763, 'num_tokens': 9300260.0, 'mean_token_accuracy': 0.7450930953025818, 'epoch': 0.39}
 13%|â–ˆâ–Ž        | 290/2235 [28:02<3:08:42,  5.82s/it] 13%|â–ˆâ–Ž        | 291/2235 [28:08<3:08:45,  5.83s/it] 13%|â–ˆâ–Ž        | 292/2235 [28:14<3:08:34,  5.82s/it] 13%|â–ˆâ–Ž        | 293/2235 [28:20<3:08:23,  5.82s/it] 13%|â–ˆâ–Ž        | 294/2235 [28:26<3:07:56,  5.81s/it] 13%|â–ˆâ–Ž        | 295/2235 [28:31<3:07:36,  5.80s/it] 13%|â–ˆâ–Ž        | 296/2235 [28:37<3:07:29,  5.80s/it] 13%|â–ˆâ–Ž        | 297/2235 [28:43<3:07:24,  5.80s/it] 13%|â–ˆâ–Ž        | 298/2235 [28:49<3:07:22,  5.80s/it] 13%|â–ˆâ–Ž        | 299/2235 [28:55<3:07:19,  5.81s/it] 13%|â–ˆâ–Ž        | 300/2235 [29:00<3:07:16,  5.81s/it]                                                    {'loss': 1.1632, 'grad_norm': 0.58203125, 'learning_rate': 4.7824320809014816e-06, 'entropy': 1.1788744747638702, 'num_tokens': 9619780.0, 'mean_token_accuracy': 0.7537540286779404, 'epoch': 0.4}
 13%|â–ˆâ–Ž        | 300/2235 [29:00<3:07:16,  5.81s/it] 13%|â–ˆâ–Ž        | 301/2235 [29:06<3:07:19,  5.81s/it] 14%|â–ˆâ–Ž        | 302/2235 [29:12<3:07:19,  5.81s/it] 14%|â–ˆâ–Ž        | 303/2235 [29:18<3:07:30,  5.82s/it] 14%|â–ˆâ–Ž        | 304/2235 [29:23<3:05:39,  5.77s/it] 14%|â–ˆâ–Ž        | 305/2235 [29:29<3:06:10,  5.79s/it] 14%|â–ˆâ–Ž        | 306/2235 [29:35<3:06:45,  5.81s/it] 14%|â–ˆâ–Ž        | 307/2235 [29:41<3:06:51,  5.82s/it] 14%|â–ˆâ–        | 308/2235 [29:47<3:06:56,  5.82s/it] 14%|â–ˆâ–        | 309/2235 [29:53<3:06:48,  5.82s/it] 14%|â–ˆâ–        | 310/2235 [29:58<3:06:24,  5.81s/it]                                                    {'loss': 1.1622, 'grad_norm': 0.4375, 'learning_rate': 4.767868889411545e-06, 'entropy': 1.1800403147935867, 'num_tokens': 9939609.0, 'mean_token_accuracy': 0.7509799391031265, 'epoch': 0.42}
 14%|â–ˆâ–        | 310/2235 [29:58<3:06:24,  5.81s/it] 14%|â–ˆâ–        | 311/2235 [30:04<3:06:05,  5.80s/it] 14%|â–ˆâ–        | 312/2235 [30:10<3:05:59,  5.80s/it] 14%|â–ˆâ–        | 313/2235 [30:16<3:05:49,  5.80s/it] 14%|â–ˆâ–        | 314/2235 [30:22<3:05:41,  5.80s/it] 14%|â–ˆâ–        | 315/2235 [30:27<3:05:32,  5.80s/it] 14%|â–ˆâ–        | 316/2235 [30:33<3:05:13,  5.79s/it] 14%|â–ˆâ–        | 317/2235 [30:39<3:05:09,  5.79s/it] 14%|â–ˆâ–        | 318/2235 [30:45<3:05:12,  5.80s/it] 14%|â–ˆâ–        | 319/2235 [30:51<3:05:15,  5.80s/it] 14%|â–ˆâ–        | 320/2235 [30:56<3:05:14,  5.80s/it]                                                    {'loss': 1.1501, 'grad_norm': 0.37890625, 'learning_rate': 4.752857618000537e-06, 'entropy': 1.1722380816936493, 'num_tokens': 10258739.0, 'mean_token_accuracy': 0.754556593298912, 'epoch': 0.43}
 14%|â–ˆâ–        | 320/2235 [30:56<3:05:14,  5.80s/it] 14%|â–ˆâ–        | 321/2235 [31:02<3:05:12,  5.81s/it] 14%|â–ˆâ–        | 322/2235 [31:08<3:05:25,  5.82s/it] 14%|â–ˆâ–        | 323/2235 [31:14<3:05:36,  5.82s/it] 14%|â–ˆâ–        | 324/2235 [31:20<3:05:53,  5.84s/it] 15%|â–ˆâ–        | 325/2235 [31:26<3:06:04,  5.85s/it] 15%|â–ˆâ–        | 326/2235 [31:31<3:05:59,  5.85s/it] 15%|â–ˆâ–        | 327/2235 [31:37<3:05:25,  5.83s/it] 15%|â–ˆâ–        | 328/2235 [31:43<3:05:17,  5.83s/it] 15%|â–ˆâ–        | 329/2235 [31:49<3:05:01,  5.82s/it] 15%|â–ˆâ–        | 330/2235 [31:55<3:04:44,  5.82s/it]                                                    {'loss': 1.1781, 'grad_norm': 0.4453125, 'learning_rate': 4.7374012325583e-06, 'entropy': 1.1975304782390594, 'num_tokens': 10580214.0, 'mean_token_accuracy': 0.7482503533363343, 'epoch': 0.44}
 15%|â–ˆâ–        | 330/2235 [31:55<3:04:44,  5.82s/it] 15%|â–ˆâ–        | 331/2235 [32:00<3:04:26,  5.81s/it] 15%|â–ˆâ–        | 332/2235 [32:06<3:04:00,  5.80s/it] 15%|â–ˆâ–        | 333/2235 [32:12<3:03:24,  5.79s/it] 15%|â–ˆâ–        | 334/2235 [32:18<3:03:09,  5.78s/it] 15%|â–ˆâ–        | 335/2235 [32:24<3:03:15,  5.79s/it] 15%|â–ˆâ–Œ        | 336/2235 [32:29<3:03:20,  5.79s/it] 15%|â–ˆâ–Œ        | 337/2235 [32:35<3:03:21,  5.80s/it] 15%|â–ˆâ–Œ        | 338/2235 [32:41<3:03:25,  5.80s/it] 15%|â–ˆâ–Œ        | 339/2235 [32:47<3:03:32,  5.81s/it] 15%|â–ˆâ–Œ        | 340/2235 [32:53<3:03:31,  5.81s/it]                                                    {'loss': 1.1804, 'grad_norm': 0.42578125, 'learning_rate': 4.721502786919209e-06, 'entropy': 1.197679802775383, 'num_tokens': 10899652.0, 'mean_token_accuracy': 0.7476502120494842, 'epoch': 0.46}
 15%|â–ˆâ–Œ        | 340/2235 [32:53<3:03:31,  5.81s/it] 15%|â–ˆâ–Œ        | 341/2235 [32:58<3:03:28,  5.81s/it] 15%|â–ˆâ–Œ        | 342/2235 [33:04<3:03:49,  5.83s/it] 15%|â–ˆâ–Œ        | 343/2235 [33:10<3:03:44,  5.83s/it] 15%|â–ˆâ–Œ        | 344/2235 [33:16<3:03:55,  5.84s/it] 15%|â–ˆâ–Œ        | 345/2235 [33:22<3:03:43,  5.83s/it] 15%|â–ˆâ–Œ        | 346/2235 [33:28<3:03:32,  5.83s/it] 16%|â–ˆâ–Œ        | 347/2235 [33:33<3:03:19,  5.83s/it] 16%|â–ˆâ–Œ        | 348/2235 [33:39<3:02:46,  5.81s/it] 16%|â–ˆâ–Œ        | 349/2235 [33:45<3:02:26,  5.80s/it] 16%|â–ˆâ–Œ        | 350/2235 [33:51<3:02:21,  5.80s/it]                                                    {'loss': 1.1032, 'grad_norm': 0.4140625, 'learning_rate': 4.705165422258795e-06, 'entropy': 1.118578064441681, 'num_tokens': 11221479.0, 'mean_token_accuracy': 0.7626056432723999, 'epoch': 0.47}
 16%|â–ˆâ–Œ        | 350/2235 [33:51<3:02:21,  5.80s/it] 16%|â–ˆâ–Œ        | 351/2235 [33:57<3:02:12,  5.80s/it] 16%|â–ˆâ–Œ        | 352/2235 [34:02<3:02:06,  5.80s/it] 16%|â–ˆâ–Œ        | 353/2235 [34:08<3:01:31,  5.79s/it] 16%|â–ˆâ–Œ        | 354/2235 [34:14<3:01:40,  5.79s/it] 16%|â–ˆâ–Œ        | 355/2235 [34:20<3:01:45,  5.80s/it] 16%|â–ˆâ–Œ        | 356/2235 [34:26<3:01:32,  5.80s/it] 16%|â–ˆâ–Œ        | 357/2235 [34:31<3:01:34,  5.80s/it] 16%|â–ˆâ–Œ        | 358/2235 [34:37<3:01:48,  5.81s/it] 16%|â–ˆâ–Œ        | 359/2235 [34:43<3:01:50,  5.82s/it] 16%|â–ˆâ–Œ        | 360/2235 [34:49<3:01:52,  5.82s/it]                                                    {'loss': 1.1073, 'grad_norm': 0.4375, 'learning_rate': 4.6883923664731275e-06, 'entropy': 1.1258866369724274, 'num_tokens': 11542556.0, 'mean_token_accuracy': 0.7622796177864075, 'epoch': 0.48}
 16%|â–ˆâ–Œ        | 360/2235 [34:49<3:01:52,  5.82s/it] 16%|â–ˆâ–Œ        | 361/2235 [34:55<3:02:06,  5.83s/it] 16%|â–ˆâ–Œ        | 362/2235 [35:01<3:02:03,  5.83s/it] 16%|â–ˆâ–Œ        | 363/2235 [35:06<3:01:47,  5.83s/it] 16%|â–ˆâ–‹        | 364/2235 [35:12<3:01:02,  5.81s/it] 16%|â–ˆâ–‹        | 365/2235 [35:18<3:00:38,  5.80s/it] 16%|â–ˆâ–‹        | 366/2235 [35:24<3:00:37,  5.80s/it] 16%|â–ˆâ–‹        | 367/2235 [35:30<3:00:34,  5.80s/it] 16%|â–ˆâ–‹        | 368/2235 [35:35<3:00:14,  5.79s/it] 17%|â–ˆâ–‹        | 369/2235 [35:41<2:59:59,  5.79s/it] 17%|â–ˆâ–‹        | 370/2235 [35:47<3:00:04,  5.79s/it]                                                    {'loss': 1.146, 'grad_norm': 0.451171875, 'learning_rate': 4.671186933541044e-06, 'entropy': 1.1696177423000336, 'num_tokens': 11863904.0, 'mean_token_accuracy': 0.7542210757732392, 'epoch': 0.5}
 17%|â–ˆâ–‹        | 370/2235 [35:47<3:00:04,  5.79s/it] 17%|â–ˆâ–‹        | 371/2235 [35:53<3:00:09,  5.80s/it] 17%|â–ˆâ–‹        | 372/2235 [35:59<3:00:00,  5.80s/it] 17%|â–ˆâ–‹        | 373/2235 [36:04<3:00:06,  5.80s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

  0%|          | 0/187 [00:00<?, ?it/s][A
  1%|          | 2/187 [00:01<02:41,  1.14it/s][A
  2%|â–         | 3/187 [00:03<03:48,  1.24s/it][A
  2%|â–         | 4/187 [00:05<04:21,  1.43s/it][A
  3%|â–Ž         | 5/187 [00:07<04:40,  1.54s/it][A
  3%|â–Ž         | 6/187 [00:08<04:52,  1.61s/it][A
  4%|â–Ž         | 7/187 [00:10<04:58,  1.66s/it][A
  4%|â–         | 8/187 [00:12<05:02,  1.69s/it][A
  5%|â–         | 9/187 [00:14<05:04,  1.71s/it][A
  5%|â–Œ         | 10/187 [00:15<05:05,  1.73s/it][A
  6%|â–Œ         | 11/187 [00:17<05:05,  1.74s/it][A
  6%|â–‹         | 12/187 [00:19<05:05,  1.75s/it][A
  7%|â–‹         | 13/187 [00:21<05:04,  1.75s/it][A
  7%|â–‹         | 14/187 [00:22<05:03,  1.76s/it][A
  8%|â–Š         | 15/187 [00:24<05:02,  1.76s/it][A
  9%|â–Š         | 16/187 [00:26<05:01,  1.76s/it][A
  9%|â–‰         | 17/187 [00:28<04:59,  1.76s/it][A
 10%|â–‰         | 18/187 [00:29<04:57,  1.76s/it][A
 10%|â–ˆ         | 19/187 [00:31<04:55,  1.76s/it][A
 11%|â–ˆ         | 20/187 [00:33<04:53,  1.76s/it][A
 11%|â–ˆ         | 21/187 [00:35<04:51,  1.75s/it][A
 12%|â–ˆâ–        | 22/187 [00:36<04:48,  1.75s/it][A
 12%|â–ˆâ–        | 23/187 [00:38<04:46,  1.75s/it][A
 13%|â–ˆâ–Ž        | 24/187 [00:40<04:44,  1.74s/it][A
 13%|â–ˆâ–Ž        | 25/187 [00:42<04:42,  1.74s/it][A
 14%|â–ˆâ–        | 26/187 [00:43<04:40,  1.74s/it][A
 14%|â–ˆâ–        | 27/187 [00:45<04:38,  1.74s/it][A
 15%|â–ˆâ–        | 28/187 [00:47<04:36,  1.74s/it][A
 16%|â–ˆâ–Œ        | 29/187 [00:49<04:34,  1.74s/it][A
 16%|â–ˆâ–Œ        | 30/187 [00:50<04:32,  1.73s/it][A
 17%|â–ˆâ–‹        | 31/187 [00:52<04:30,  1.73s/it][A
 17%|â–ˆâ–‹        | 32/187 [00:54<04:28,  1.73s/it][A
 18%|â–ˆâ–Š        | 33/187 [00:55<04:26,  1.73s/it][A
 18%|â–ˆâ–Š        | 34/187 [00:57<04:24,  1.73s/it][A
 19%|â–ˆâ–Š        | 35/187 [00:59<04:22,  1.73s/it][A
 19%|â–ˆâ–‰        | 36/187 [01:01<04:21,  1.73s/it][A
 20%|â–ˆâ–‰        | 37/187 [01:02<04:19,  1.73s/it][A
 20%|â–ˆâ–ˆ        | 38/187 [01:04<04:17,  1.73s/it][A
 21%|â–ˆâ–ˆ        | 39/187 [01:06<04:15,  1.73s/it][A
 21%|â–ˆâ–ˆâ–       | 40/187 [01:08<04:13,  1.73s/it][A
 22%|â–ˆâ–ˆâ–       | 41/187 [01:09<04:11,  1.73s/it][A
 22%|â–ˆâ–ˆâ–       | 42/187 [01:11<04:10,  1.73s/it][A
 23%|â–ˆâ–ˆâ–Ž       | 43/187 [01:13<04:08,  1.72s/it][A
 24%|â–ˆâ–ˆâ–Ž       | 44/187 [01:14<04:06,  1.72s/it][A
 24%|â–ˆâ–ˆâ–       | 45/187 [01:16<04:04,  1.72s/it][A
 25%|â–ˆâ–ˆâ–       | 46/187 [01:18<04:02,  1.72s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 47/187 [01:20<04:01,  1.72s/it][A
 26%|â–ˆâ–ˆâ–Œ       | 48/187 [01:21<03:59,  1.72s/it][A
 26%|â–ˆâ–ˆâ–Œ       | 49/187 [01:23<03:57,  1.72s/it][A
 27%|â–ˆâ–ˆâ–‹       | 50/187 [01:25<03:55,  1.72s/it][A
 27%|â–ˆâ–ˆâ–‹       | 51/187 [01:27<03:53,  1.72s/it][A
 28%|â–ˆâ–ˆâ–Š       | 52/187 [01:28<03:52,  1.72s/it][A
 28%|â–ˆâ–ˆâ–Š       | 53/187 [01:30<03:50,  1.72s/it][A
 29%|â–ˆâ–ˆâ–‰       | 54/187 [01:32<03:48,  1.72s/it][A
 29%|â–ˆâ–ˆâ–‰       | 55/187 [01:33<03:46,  1.72s/it][A
 30%|â–ˆâ–ˆâ–‰       | 56/187 [01:35<03:45,  1.72s/it][A
 30%|â–ˆâ–ˆâ–ˆ       | 57/187 [01:37<03:43,  1.72s/it][A
 31%|â–ˆâ–ˆâ–ˆ       | 58/187 [01:39<03:41,  1.72s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 59/187 [01:40<03:40,  1.72s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 60/187 [01:42<03:38,  1.72s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 61/187 [01:44<03:36,  1.72s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 62/187 [01:45<03:34,  1.72s/it][A
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 63/187 [01:47<03:33,  1.72s/it][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 64/187 [01:49<03:31,  1.72s/it][A
 35%|â–ˆâ–ˆâ–ˆâ–      | 65/187 [01:51<03:29,  1.72s/it][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 66/187 [01:52<03:28,  1.72s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 67/187 [01:54<03:26,  1.72s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 68/187 [01:56<03:24,  1.72s/it][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 69/187 [01:57<03:22,  1.72s/it][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/187 [01:59<03:21,  1.72s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 71/187 [02:01<03:19,  1.72s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 72/187 [02:03<03:17,  1.72s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 73/187 [02:04<03:16,  1.72s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 74/187 [02:06<03:14,  1.72s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 75/187 [02:08<03:12,  1.72s/it][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76/187 [02:09<03:10,  1.72s/it][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77/187 [02:11<03:09,  1.72s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/187 [02:13<03:07,  1.72s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/187 [02:15<03:05,  1.72s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 80/187 [02:16<03:03,  1.72s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 81/187 [02:18<03:02,  1.72s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/187 [02:20<03:00,  1.72s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/187 [02:22<02:58,  1.72s/it][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/187 [02:23<02:57,  1.72s/it][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 85/187 [02:25<02:55,  1.72s/it][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86/187 [02:27<02:53,  1.72s/it][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 87/187 [02:28<02:51,  1.72s/it][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88/187 [02:30<02:50,  1.72s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 89/187 [02:32<02:48,  1.72s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/187 [02:34<02:46,  1.72s/it][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91/187 [02:35<02:45,  1.72s/it][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 92/187 [02:37<02:43,  1.72s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 93/187 [02:39<02:41,  1.72s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 94/187 [02:40<02:40,  1.72s/it][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 95/187 [02:42<02:38,  1.72s/it][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 96/187 [02:44<02:36,  1.72s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/187 [02:46<02:34,  1.72s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/187 [02:47<02:33,  1.72s/it][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 99/187 [02:49<02:31,  1.72s/it][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 100/187 [02:51<02:29,  1.72s/it][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 101/187 [02:53<02:28,  1.72s/it][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/187 [02:54<02:26,  1.72s/it][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 103/187 [02:56<02:24,  1.72s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 104/187 [02:58<02:23,  1.72s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105/187 [02:59<02:21,  1.72s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 106/187 [03:01<02:19,  1.72s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 107/187 [03:03<02:17,  1.72s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 108/187 [03:05<02:16,  1.72s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 109/187 [03:06<02:14,  1.72s/it][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 110/187 [03:08<02:12,  1.72s/it][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 111/187 [03:10<02:11,  1.72s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112/187 [03:11<02:09,  1.72s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 113/187 [03:13<02:07,  1.72s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114/187 [03:15<02:05,  1.72s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 115/187 [03:17<02:04,  1.73s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/187 [03:18<02:02,  1.73s/it][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 117/187 [03:20<02:00,  1.73s/it][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 118/187 [03:22<01:59,  1.73s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 119/187 [03:24<01:57,  1.73s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120/187 [03:25<01:55,  1.73s/it][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121/187 [03:27<01:54,  1.73s/it][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 122/187 [03:29<01:52,  1.73s/it][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 123/187 [03:30<01:50,  1.73s/it][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 124/187 [03:32<01:48,  1.73s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 125/187 [03:34<01:47,  1.73s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126/187 [03:36<01:45,  1.73s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 127/187 [03:37<01:43,  1.73s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 128/187 [03:39<01:41,  1.73s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 129/187 [03:41<01:40,  1.73s/it][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/187 [03:43<01:38,  1.73s/it][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 131/187 [03:44<01:36,  1.73s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 132/187 [03:46<01:35,  1.73s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133/187 [03:48<01:33,  1.73s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 134/187 [03:49<01:31,  1.73s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/187 [03:51<01:30,  1.73s/it][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 136/187 [03:53<01:28,  1.73s/it][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 137/187 [03:55<01:26,  1.73s/it][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138/187 [03:56<01:24,  1.73s/it][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 139/187 [03:58<01:23,  1.73s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/187 [04:00<01:21,  1.73s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/187 [04:02<01:19,  1.73s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 142/187 [04:03<01:18,  1.73s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 143/187 [04:05<01:16,  1.73s/it][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 144/187 [04:07<01:14,  1.74s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 145/187 [04:09<01:12,  1.74s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 146/187 [04:10<01:11,  1.74s/it][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 147/187 [04:12<01:09,  1.74s/it][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 148/187 [04:14<01:07,  1.74s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 149/187 [04:16<01:06,  1.74s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 150/187 [04:17<01:04,  1.74s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 151/187 [04:19<01:02,  1.74s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 152/187 [04:21<01:01,  1.74s/it][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 153/187 [04:23<00:59,  1.74s/it][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 154/187 [04:24<00:57,  1.74s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 155/187 [04:26<00:55,  1.74s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 156/187 [04:28<00:54,  1.74s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 157/187 [04:29<00:52,  1.75s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 158/187 [04:31<00:50,  1.75s/it][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 159/187 [04:33<00:48,  1.75s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 160/187 [04:35<00:47,  1.75s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 161/187 [04:37<00:45,  1.75s/it][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 162/187 [04:38<00:43,  1.75s/it][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 163/187 [04:40<00:42,  1.75s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 164/187 [04:42<00:40,  1.76s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 165/187 [04:44<00:38,  1.76s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 166/187 [04:45<00:36,  1.76s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 167/187 [04:47<00:35,  1.76s/it][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 168/187 [04:49<00:33,  1.76s/it][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 169/187 [04:51<00:31,  1.76s/it][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 170/187 [04:52<00:29,  1.76s/it][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 171/187 [04:54<00:28,  1.76s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 172/187 [04:56<00:26,  1.76s/it][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 173/187 [04:58<00:24,  1.75s/it][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 174/187 [04:59<00:22,  1.75s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 175/187 [05:01<00:20,  1.75s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 176/187 [05:03<00:19,  1.75s/it][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 177/187 [05:05<00:17,  1.74s/it][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 178/187 [05:06<00:15,  1.74s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 179/187 [05:08<00:13,  1.74s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 180/187 [05:10<00:12,  1.74s/it][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 181/187 [05:12<00:10,  1.74s/it][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 182/187 [05:13<00:08,  1.74s/it][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 183/187 [05:15<00:06,  1.74s/it][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 184/187 [05:17<00:05,  1.74s/it][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 185/187 [05:18<00:03,  1.74s/it][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 186/187 [05:20<00:01,  1.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [05:22<00:00,  1.76s/it][A                                                    
                                                 [A{'eval_loss': 1.1383497714996338, 'eval_runtime': 324.6591, 'eval_samples_per_second': 18.358, 'eval_steps_per_second': 0.576, 'eval_entropy': 1.1589769574410138, 'eval_num_tokens': 11960611.0, 'eval_mean_token_accuracy': 0.7553440498158256, 'epoch': 0.5}
 17%|â–ˆâ–‹        | 373/2235 [41:29<3:00:06,  5.80s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [05:22<00:00,  1.76s/it][A
                                                 [A/home/ubuntu/Part-II-Project/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 17%|â–ˆâ–‹        | 374/2235 [41:35<53:27:19, 103.41s/it] 17%|â–ˆâ–‹        | 375/2235 [41:41<38:17:43, 74.12s/it]  17%|â–ˆâ–‹        | 376/2235 [41:47<27:41:06, 53.61s/it] 17%|â–ˆâ–‹        | 377/2235 [41:53<20:15:41, 39.26s/it] 17%|â–ˆâ–‹        | 378/2235 [41:59<15:04:03, 29.21s/it] 17%|â–ˆâ–‹        | 379/2235 [42:04<11:25:56, 22.17s/it] 17%|â–ˆâ–‹        | 380/2235 [42:10<8:53:19, 17.25s/it]                                                     {'loss': 1.1801, 'grad_norm': 0.45703125, 'learning_rate': 4.653552522869393e-06, 'entropy': 1.2023034393787384, 'num_tokens': 12186371.0, 'mean_token_accuracy': 0.7473860651254653, 'epoch': 0.51}
 17%|â–ˆâ–‹        | 380/2235 [42:10<8:53:19, 17.25s/it] 17%|â–ˆâ–‹        | 381/2235 [42:16<7:06:31, 13.80s/it] 17%|â–ˆâ–‹        | 382/2235 [42:22<5:51:52, 11.39s/it] 17%|â–ˆâ–‹        | 383/2235 [42:27<4:59:42,  9.71s/it] 17%|â–ˆâ–‹        | 384/2235 [42:33<4:23:12,  8.53s/it] 17%|â–ˆâ–‹        | 385/2235 [42:39<3:57:35,  7.71s/it] 17%|â–ˆâ–‹        | 386/2235 [42:45<3:39:38,  7.13s/it] 17%|â–ˆâ–‹        | 387/2235 [42:51<3:27:04,  6.72s/it] 17%|â–ˆâ–‹        | 388/2235 [42:56<3:18:11,  6.44s/it] 17%|â–ˆâ–‹        | 389/2235 [43:02<3:12:06,  6.24s/it] 17%|â–ˆâ–‹        | 390/2235 [43:08<3:07:36,  6.10s/it]                                                    {'loss': 1.1073, 'grad_norm': 0.423828125, 'learning_rate': 4.635492618621381e-06, 'entropy': 1.1275073707103729, 'num_tokens': 12506460.0, 'mean_token_accuracy': 0.7621266394853592, 'epoch': 0.52}
 17%|â–ˆâ–‹        | 390/2235 [43:08<3:07:36,  6.10s/it] 17%|â–ˆâ–‹        | 391/2235 [43:14<3:04:39,  6.01s/it] 18%|â–ˆâ–Š        | 392/2235 [43:19<3:02:22,  5.94s/it] 18%|â–ˆâ–Š        | 393/2235 [43:25<3:01:05,  5.90s/it] 18%|â–ˆâ–Š        | 394/2235 [43:31<3:00:17,  5.88s/it] 18%|â–ˆâ–Š        | 395/2235 [43:37<2:59:28,  5.85s/it] 18%|â–ˆâ–Š        | 396/2235 [43:43<2:59:01,  5.84s/it] 18%|â–ˆâ–Š        | 397/2235 [43:49<2:58:58,  5.84s/it] 18%|â–ˆâ–Š        | 398/2235 [43:54<2:58:55,  5.84s/it] 18%|â–ˆâ–Š        | 399/2235 [44:00<2:58:22,  5.83s/it] 18%|â–ˆâ–Š        | 400/2235 [44:06<2:57:54,  5.82s/it]                                                    {'loss': 1.1038, 'grad_norm': 0.50390625, 'learning_rate': 4.6170107890281826e-06, 'entropy': 1.125632992386818, 'num_tokens': 12827667.0, 'mean_token_accuracy': 0.7614039957523346, 'epoch': 0.54}
 18%|â–ˆâ–Š        | 400/2235 [44:06<2:57:54,  5.82s/it] 18%|â–ˆâ–Š        | 401/2235 [44:12<2:57:33,  5.81s/it] 18%|â–ˆâ–Š        | 402/2235 [44:18<2:57:26,  5.81s/it] 18%|â–ˆâ–Š        | 403/2235 [44:23<2:57:15,  5.81s/it] 18%|â–ˆâ–Š        | 404/2235 [44:29<2:55:08,  5.74s/it] 18%|â–ˆâ–Š        | 405/2235 [44:35<2:55:22,  5.75s/it] 18%|â–ˆâ–Š        | 406/2235 [44:40<2:55:23,  5.75s/it] 18%|â–ˆâ–Š        | 407/2235 [44:46<2:55:32,  5.76s/it] 18%|â–ˆâ–Š        | 408/2235 [44:52<2:55:39,  5.77s/it] 18%|â–ˆâ–Š        | 409/2235 [44:58<2:55:53,  5.78s/it] 18%|â–ˆâ–Š        | 410/2235 [45:04<2:56:05,  5.79s/it]                                                    {'loss': 1.1054, 'grad_norm': 0.455078125, 'learning_rate': 4.598110685683936e-06, 'entropy': 1.1148680716753006, 'num_tokens': 13148183.0, 'mean_token_accuracy': 0.7612309962511062, 'epoch': 0.55}
 18%|â–ˆâ–Š        | 410/2235 [45:04<2:56:05,  5.79s/it] 18%|â–ˆâ–Š        | 411/2235 [45:09<2:56:17,  5.80s/it] 18%|â–ˆâ–Š        | 412/2235 [45:15<2:56:32,  5.81s/it] 18%|â–ˆâ–Š        | 413/2235 [45:21<2:56:32,  5.81s/it] 19%|â–ˆâ–Š        | 414/2235 [45:27<2:56:33,  5.82s/it] 19%|â–ˆâ–Š        | 415/2235 [45:33<2:56:38,  5.82s/it] 19%|â–ˆâ–Š        | 416/2235 [45:39<2:56:53,  5.83s/it] 19%|â–ˆâ–Š        | 417/2235 [45:44<2:56:43,  5.83s/it] 19%|â–ˆâ–Š        | 418/2235 [45:50<2:56:26,  5.83s/it] 19%|â–ˆâ–Š        | 419/2235 [45:56<2:56:08,  5.82s/it] 19%|â–ˆâ–‰        | 420/2235 [46:02<2:55:44,  5.81s/it]                                                    {'loss': 1.1258, 'grad_norm': 0.453125, 'learning_rate': 4.578796042824272e-06, 'entropy': 1.1396809995174408, 'num_tokens': 13467832.0, 'mean_token_accuracy': 0.7580690830945969, 'epoch': 0.56}
 19%|â–ˆâ–‰        | 420/2235 [46:02<2:55:44,  5.81s/it] 19%|â–ˆâ–‰        | 421/2235 [46:08<2:55:35,  5.81s/it] 19%|â–ˆâ–‰        | 422/2235 [46:13<2:55:19,  5.80s/it] 19%|â–ˆâ–‰        | 423/2235 [46:19<2:55:03,  5.80s/it] 19%|â–ˆâ–‰        | 424/2235 [46:25<2:54:51,  5.79s/it] 19%|â–ˆâ–‰        | 425/2235 [46:31<2:54:40,  5.79s/it] 19%|â–ˆâ–‰        | 426/2235 [46:37<2:54:33,  5.79s/it] 19%|â–ˆâ–‰        | 427/2235 [46:42<2:54:29,  5.79s/it] 19%|â–ˆâ–‰        | 428/2235 [46:48<2:54:03,  5.78s/it] 19%|â–ˆâ–‰        | 429/2235 [46:54<2:54:07,  5.79s/it] 19%|â–ˆâ–‰        | 430/2235 [47:00<2:53:58,  5.78s/it]                                                    {'loss': 1.1219, 'grad_norm': 0.50390625, 'learning_rate': 4.559070676588516e-06, 'entropy': 1.139620640873909, 'num_tokens': 13789711.0, 'mean_token_accuracy': 0.7600348860025405, 'epoch': 0.58}
 19%|â–ˆâ–‰        | 430/2235 [47:00<2:53:58,  5.78s/it] 19%|â–ˆâ–‰        | 431/2235 [47:06<2:54:04,  5.79s/it] 19%|â–ˆâ–‰        | 432/2235 [47:11<2:54:10,  5.80s/it] 19%|â–ˆâ–‰        | 433/2235 [47:17<2:53:39,  5.78s/it] 19%|â–ˆâ–‰        | 434/2235 [47:23<2:53:40,  5.79s/it] 19%|â–ˆâ–‰        | 435/2235 [47:29<2:53:59,  5.80s/it] 20%|â–ˆâ–‰        | 436/2235 [47:35<2:54:10,  5.81s/it] 20%|â–ˆâ–‰        | 437/2235 [47:40<2:54:16,  5.82s/it] 20%|â–ˆâ–‰        | 438/2235 [47:46<2:54:25,  5.82s/it] 20%|â–ˆâ–‰        | 439/2235 [47:52<2:52:40,  5.77s/it] 20%|â–ˆâ–‰        | 440/2235 [47:58<2:53:28,  5.80s/it]                                                    {'loss': 0.9946, 'grad_norm': 0.400390625, 'learning_rate': 4.5389384842656966e-06, 'entropy': 1.0111248910427093, 'num_tokens': 14110549.0, 'mean_token_accuracy': 0.7864084094762802, 'epoch': 0.59}
 20%|â–ˆâ–‰        | 440/2235 [47:58<2:53:28,  5.80s/it] 20%|â–ˆâ–‰        | 441/2235 [48:04<2:54:03,  5.82s/it] 20%|â–ˆâ–‰        | 442/2235 [48:09<2:54:15,  5.83s/it] 20%|â–ˆâ–‰        | 443/2235 [48:15<2:54:06,  5.83s/it] 20%|â–ˆâ–‰        | 444/2235 [48:21<2:53:55,  5.83s/it] 20%|â–ˆâ–‰        | 445/2235 [48:27<2:53:44,  5.82s/it] 20%|â–ˆâ–‰        | 446/2235 [48:33<2:53:31,  5.82s/it] 20%|â–ˆâ–ˆ        | 447/2235 [48:39<2:53:19,  5.82s/it] 20%|â–ˆâ–ˆ        | 448/2235 [48:44<2:53:10,  5.81s/it] 20%|â–ˆâ–ˆ        | 449/2235 [48:50<2:52:57,  5.81s/it] 20%|â–ˆâ–ˆ        | 450/2235 [48:56<2:52:50,  5.81s/it]                                                    {'loss': 1.1167, 'grad_norm': 0.43359375, 'learning_rate': 4.518403443524538e-06, 'entropy': 1.1350430816411972, 'num_tokens': 14431285.0, 'mean_token_accuracy': 0.7618815660476684, 'epoch': 0.6}
 20%|â–ˆâ–ˆ        | 450/2235 [48:56<2:52:50,  5.81s/it] 20%|â–ˆâ–ˆ        | 451/2235 [49:02<2:52:46,  5.81s/it] 20%|â–ˆâ–ˆ        | 452/2235 [49:08<2:52:43,  5.81s/it] 20%|â–ˆâ–ˆ        | 453/2235 [49:13<2:52:44,  5.82s/it] 20%|â–ˆâ–ˆ        | 454/2235 [49:19<2:52:47,  5.82s/it] 20%|â–ˆâ–ˆ        | 455/2235 [49:25<2:52:53,  5.83s/it] 20%|â–ˆâ–ˆ        | 456/2235 [49:31<2:52:43,  5.83s/it] 20%|â–ˆâ–ˆ        | 457/2235 [49:37<2:52:42,  5.83s/it] 20%|â–ˆâ–ˆ        | 458/2235 [49:43<2:52:42,  5.83s/it] 21%|â–ˆâ–ˆ        | 459/2235 [49:48<2:52:41,  5.83s/it] 21%|â–ˆâ–ˆ        | 460/2235 [49:54<2:52:37,  5.84s/it]                                                    {'loss': 1.1308, 'grad_norm': 0.46875, 'learning_rate': 4.497469611627554e-06, 'entropy': 1.1472623020410537, 'num_tokens': 14751480.0, 'mean_token_accuracy': 0.7603067964315414, 'epoch': 0.62}
 21%|â–ˆâ–ˆ        | 460/2235 [49:54<2:52:37,  5.84s/it] 21%|â–ˆâ–ˆ        | 461/2235 [50:00<2:52:22,  5.83s/it] 21%|â–ˆâ–ˆ        | 462/2235 [50:06<2:52:26,  5.84s/it] 21%|â–ˆâ–ˆ        | 463/2235 [50:12<2:52:30,  5.84s/it] 21%|â–ˆâ–ˆ        | 464/2235 [50:18<2:52:35,  5.85s/it] 21%|â–ˆâ–ˆ        | 465/2235 [50:23<2:52:27,  5.85s/it] 21%|â–ˆâ–ˆ        | 466/2235 [50:29<2:52:32,  5.85s/it] 21%|â–ˆâ–ˆ        | 467/2235 [50:35<2:52:26,  5.85s/it] 21%|â–ˆâ–ˆ        | 468/2235 [50:41<2:52:05,  5.84s/it] 21%|â–ˆâ–ˆ        | 469/2235 [50:47<2:51:32,  5.83s/it] 21%|â–ˆâ–ˆ        | 470/2235 [50:53<2:51:30,  5.83s/it]                                                    {'loss': 1.073, 'grad_norm': 0.5859375, 'learning_rate': 4.476141124629429e-06, 'entropy': 1.0861685931682588, 'num_tokens': 15072760.0, 'mean_token_accuracy': 0.7703844428062439, 'epoch': 0.63}
 21%|â–ˆâ–ˆ        | 470/2235 [50:53<2:51:30,  5.83s/it] 21%|â–ˆâ–ˆ        | 471/2235 [50:58<2:51:15,  5.83s/it] 21%|â–ˆâ–ˆ        | 472/2235 [51:04<2:51:01,  5.82s/it] 21%|â–ˆâ–ˆ        | 473/2235 [51:10<2:50:27,  5.80s/it] 21%|â–ˆâ–ˆ        | 474/2235 [51:16<2:50:08,  5.80s/it] 21%|â–ˆâ–ˆâ–       | 475/2235 [51:22<2:50:05,  5.80s/it] 21%|â–ˆâ–ˆâ–       | 476/2235 [51:27<2:50:00,  5.80s/it] 21%|â–ˆâ–ˆâ–       | 477/2235 [51:33<2:50:00,  5.80s/it] 21%|â–ˆâ–ˆâ–       | 478/2235 [51:39<2:49:56,  5.80s/it] 21%|â–ˆâ–ˆâ–       | 479/2235 [51:45<2:49:32,  5.79s/it] 21%|â–ˆâ–ˆâ–       | 480/2235 [51:51<2:49:35,  5.80s/it]                                                    {'loss': 1.0708, 'grad_norm': 0.4453125, 'learning_rate': 4.454422196559827e-06, 'entropy': 1.0852952361106873, 'num_tokens': 15393319.0, 'mean_token_accuracy': 0.7712160468101501, 'epoch': 0.64}
 21%|â–ˆâ–ˆâ–       | 480/2235 [51:51<2:49:35,  5.80s/it] 22%|â–ˆâ–ˆâ–       | 481/2235 [51:56<2:49:32,  5.80s/it] 22%|â–ˆâ–ˆâ–       | 482/2235 [52:02<2:47:42,  5.74s/it] 22%|â–ˆâ–ˆâ–       | 483/2235 [52:08<2:48:03,  5.76s/it] 22%|â–ˆâ–ˆâ–       | 484/2235 [52:14<2:48:20,  5.77s/it] 22%|â–ˆâ–ˆâ–       | 485/2235 [52:19<2:48:50,  5.79s/it] 22%|â–ˆâ–ˆâ–       | 486/2235 [52:25<2:49:05,  5.80s/it] 22%|â–ˆâ–ˆâ–       | 487/2235 [52:31<2:49:29,  5.82s/it] 22%|â–ˆâ–ˆâ–       | 488/2235 [52:37<2:49:29,  5.82s/it] 22%|â–ˆâ–ˆâ–       | 489/2235 [52:43<2:49:41,  5.83s/it] 22%|â–ˆâ–ˆâ–       | 490/2235 [52:49<2:49:34,  5.83s/it]                                                    {'loss': 1.1169, 'grad_norm': 0.4375, 'learning_rate': 4.432317118590789e-06, 'entropy': 1.1329826980829238, 'num_tokens': 15714869.0, 'mean_token_accuracy': 0.7613546490669251, 'epoch': 0.66}
 22%|â–ˆâ–ˆâ–       | 490/2235 [52:49<2:49:34,  5.83s/it] 22%|â–ˆâ–ˆâ–       | 491/2235 [52:54<2:49:13,  5.82s/it] 22%|â–ˆâ–ˆâ–       | 492/2235 [53:00<2:48:55,  5.81s/it] 22%|â–ˆâ–ˆâ–       | 493/2235 [53:06<2:48:20,  5.80s/it] 22%|â–ˆâ–ˆâ–       | 494/2235 [53:12<2:48:09,  5.80s/it] 22%|â–ˆâ–ˆâ–       | 495/2235 [53:17<2:46:14,  5.73s/it] 22%|â–ˆâ–ˆâ–       | 496/2235 [53:23<2:46:37,  5.75s/it] 22%|â–ˆâ–ˆâ–       | 497/2235 [53:29<2:46:44,  5.76s/it] 22%|â–ˆâ–ˆâ–       | 498/2235 [53:35<2:46:54,  5.77s/it] 22%|â–ˆâ–ˆâ–       | 499/2235 [53:41<2:46:57,  5.77s/it] 22%|â–ˆâ–ˆâ–       | 500/2235 [53:46<2:46:56,  5.77s/it]                                                    {'loss': 1.0696, 'grad_norm': 0.396484375, 'learning_rate': 4.409830258188901e-06, 'entropy': 1.0881073504686356, 'num_tokens': 16036213.0, 'mean_token_accuracy': 0.772344496846199, 'epoch': 0.67}
 22%|â–ˆâ–ˆâ–       | 500/2235 [53:46<2:46:56,  5.77s/it] 22%|â–ˆâ–ˆâ–       | 501/2235 [53:52<2:46:53,  5.77s/it] 22%|â–ˆâ–ˆâ–       | 502/2235 [53:58<2:46:45,  5.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 503/2235 [54:04<2:46:27,  5.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 504/2235 [54:09<2:46:21,  5.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 505/2235 [54:15<2:46:18,  5.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 506/2235 [54:21<2:46:19,  5.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 507/2235 [54:27<2:46:21,  5.78s/it] 23%|â–ˆâ–ˆâ–Ž       | 508/2235 [54:32<2:46:04,  5.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 509/2235 [54:38<2:46:04,  5.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 510/2235 [54:44<2:45:40,  5.76s/it]                                                    {'loss': 1.0714, 'grad_norm': 0.5390625, 'learning_rate': 4.3869660582523735e-06, 'entropy': 1.091824582219124, 'num_tokens': 16357660.0, 'mean_token_accuracy': 0.7720764577388763, 'epoch': 0.68}
 23%|â–ˆâ–ˆâ–Ž       | 510/2235 [54:44<2:45:40,  5.76s/it] 23%|â–ˆâ–ˆâ–Ž       | 511/2235 [54:50<2:45:43,  5.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 512/2235 [54:56<2:45:43,  5.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 513/2235 [55:01<2:45:50,  5.78s/it] 23%|â–ˆâ–ˆâ–Ž       | 514/2235 [55:07<2:45:56,  5.79s/it] 23%|â–ˆâ–ˆâ–Ž       | 515/2235 [55:13<2:45:51,  5.79s/it] 23%|â–ˆâ–ˆâ–Ž       | 516/2235 [55:19<2:46:04,  5.80s/it] 23%|â–ˆâ–ˆâ–Ž       | 517/2235 [55:25<2:46:08,  5.80s/it] 23%|â–ˆâ–ˆâ–Ž       | 518/2235 [55:30<2:46:19,  5.81s/it] 23%|â–ˆâ–ˆâ–Ž       | 519/2235 [55:36<2:46:31,  5.82s/it] 23%|â–ˆâ–ˆâ–Ž       | 520/2235 [55:42<2:46:44,  5.83s/it]                                                    {'loss': 1.1232, 'grad_norm': 0.458984375, 'learning_rate': 4.363729036233231e-06, 'entropy': 1.137914764881134, 'num_tokens': 16678186.0, 'mean_token_accuracy': 0.7623632907867431, 'epoch': 0.7}
 23%|â–ˆâ–ˆâ–Ž       | 520/2235 [55:42<2:46:44,  5.83s/it] 23%|â–ˆâ–ˆâ–Ž       | 521/2235 [55:48<2:46:48,  5.84s/it] 23%|â–ˆâ–ˆâ–Ž       | 522/2235 [55:54<2:44:34,  5.76s/it] 23%|â–ˆâ–ˆâ–Ž       | 523/2235 [55:59<2:44:56,  5.78s/it] 23%|â–ˆâ–ˆâ–Ž       | 524/2235 [56:05<2:45:09,  5.79s/it] 23%|â–ˆâ–ˆâ–Ž       | 525/2235 [56:11<2:45:08,  5.79s/it] 24%|â–ˆâ–ˆâ–Ž       | 526/2235 [56:17<2:44:56,  5.79s/it] 24%|â–ˆâ–ˆâ–Ž       | 527/2235 [56:23<2:44:40,  5.78s/it] 24%|â–ˆâ–ˆâ–Ž       | 528/2235 [56:28<2:44:47,  5.79s/it] 24%|â–ˆâ–ˆâ–Ž       | 529/2235 [56:34<2:44:41,  5.79s/it] 24%|â–ˆâ–ˆâ–Ž       | 530/2235 [56:40<2:44:37,  5.79s/it]                                                    {'loss': 1.0624, 'grad_norm': 0.416015625, 'learning_rate': 4.340123783244759e-06, 'entropy': 1.07880137860775, 'num_tokens': 16999529.0, 'mean_token_accuracy': 0.7719903081655503, 'epoch': 0.71}
 24%|â–ˆâ–ˆâ–Ž       | 530/2235 [56:40<2:44:37,  5.79s/it] 24%|â–ˆâ–ˆâ–       | 531/2235 [56:46<2:44:35,  5.80s/it] 24%|â–ˆâ–ˆâ–       | 532/2235 [56:52<2:44:33,  5.80s/it] 24%|â–ˆâ–ˆâ–       | 533/2235 [56:57<2:44:34,  5.80s/it] 24%|â–ˆâ–ˆâ–       | 534/2235 [57:03<2:44:33,  5.80s/it] 24%|â–ˆâ–ˆâ–       | 535/2235 [57:09<2:44:25,  5.80s/it] 24%|â–ˆâ–ˆâ–       | 536/2235 [57:15<2:44:34,  5.81s/it] 24%|â–ˆâ–ˆâ–       | 537/2235 [57:21<2:44:39,  5.82s/it] 24%|â–ˆâ–ˆâ–       | 538/2235 [57:26<2:44:33,  5.82s/it] 24%|â–ˆâ–ˆâ–       | 539/2235 [57:32<2:44:08,  5.81s/it] 24%|â–ˆâ–ˆâ–       | 540/2235 [57:38<2:43:40,  5.79s/it]                                                    {'loss': 1.1378, 'grad_norm': 0.333984375, 'learning_rate': 4.316154963154411e-06, 'entropy': 1.1520520836114883, 'num_tokens': 17322069.0, 'mean_token_accuracy': 0.7561624586582184, 'epoch': 0.72}
 24%|â–ˆâ–ˆâ–       | 540/2235 [57:38<2:43:40,  5.79s/it] 24%|â–ˆâ–ˆâ–       | 541/2235 [57:44<2:43:36,  5.80s/it] 24%|â–ˆâ–ˆâ–       | 542/2235 [57:50<2:43:28,  5.79s/it] 24%|â–ˆâ–ˆâ–       | 543/2235 [57:55<2:43:18,  5.79s/it] 24%|â–ˆâ–ˆâ–       | 544/2235 [58:01<2:43:16,  5.79s/it] 24%|â–ˆâ–ˆâ–       | 545/2235 [58:07<2:43:11,  5.79s/it] 24%|â–ˆâ–ˆâ–       | 546/2235 [58:13<2:43:07,  5.79s/it] 24%|â–ˆâ–ˆâ–       | 547/2235 [58:19<2:42:48,  5.79s/it] 25%|â–ˆâ–ˆâ–       | 548/2235 [58:24<2:42:45,  5.79s/it] 25%|â–ˆâ–ˆâ–       | 549/2235 [58:30<2:42:41,  5.79s/it] 25%|â–ˆâ–ˆâ–       | 550/2235 [58:36<2:42:39,  5.79s/it]                                                    {'loss': 1.0413, 'grad_norm': 0.427734375, 'learning_rate': 4.2918273116623245e-06, 'entropy': 1.065866893529892, 'num_tokens': 17642016.0, 'mean_token_accuracy': 0.7783291131258011, 'epoch': 0.74}
 25%|â–ˆâ–ˆâ–       | 550/2235 [58:36<2:42:39,  5.79s/it] 25%|â–ˆâ–ˆâ–       | 551/2235 [58:42<2:42:41,  5.80s/it] 25%|â–ˆâ–ˆâ–       | 552/2235 [58:48<2:42:45,  5.80s/it] 25%|â–ˆâ–ˆâ–       | 553/2235 [58:53<2:42:26,  5.79s/it] 25%|â–ˆâ–ˆâ–       | 554/2235 [58:59<2:42:13,  5.79s/it] 25%|â–ˆâ–ˆâ–       | 555/2235 [59:05<2:42:24,  5.80s/it] 25%|â–ˆâ–ˆâ–       | 556/2235 [59:11<2:42:24,  5.80s/it] 25%|â–ˆâ–ˆâ–       | 557/2235 [59:17<2:42:31,  5.81s/it] 25%|â–ˆâ–ˆâ–       | 558/2235 [59:22<2:42:21,  5.81s/it] 25%|â–ˆâ–ˆâ–Œ       | 559/2235 [59:28<2:42:24,  5.81s/it] 25%|â–ˆâ–ˆâ–Œ       | 560/2235 [59:34<2:42:29,  5.82s/it]                                                    {'loss': 1.0315, 'grad_norm': 0.45703125, 'learning_rate': 4.267145635365658e-06, 'entropy': 1.04546879529953, 'num_tokens': 17962715.0, 'mean_token_accuracy': 0.7796427130699157, 'epoch': 0.75}
 25%|â–ˆâ–ˆâ–Œ       | 560/2235 [59:34<2:42:29,  5.82s/it] 25%|â–ˆâ–ˆâ–Œ       | 561/2235 [59:40<2:42:41,  5.83s/it] 25%|â–ˆâ–ˆâ–Œ       | 562/2235 [59:46<2:42:25,  5.83s/it] 25%|â–ˆâ–ˆâ–Œ       | 563/2235 [59:51<2:42:12,  5.82s/it] 25%|â–ˆâ–ˆâ–Œ       | 564/2235 [59:57<2:42:08,  5.82s/it] 25%|â–ˆâ–ˆâ–Œ       | 565/2235 [1:00:03<2:41:56,  5.82s/it] 25%|â–ˆâ–ˆâ–Œ       | 566/2235 [1:00:09<2:41:47,  5.82s/it] 25%|â–ˆâ–ˆâ–Œ       | 567/2235 [1:00:15<2:41:36,  5.81s/it] 25%|â–ˆâ–ˆâ–Œ       | 568/2235 [1:00:21<2:41:10,  5.80s/it] 25%|â–ˆâ–ˆâ–Œ       | 569/2235 [1:00:26<2:41:05,  5.80s/it] 26%|â–ˆâ–ˆâ–Œ       | 570/2235 [1:00:32<2:41:03,  5.80s/it]                                                      {'loss': 1.0433, 'grad_norm': 0.54296875, 'learning_rate': 4.242114810808915e-06, 'entropy': 1.0628984570503235, 'num_tokens': 18282614.0, 'mean_token_accuracy': 0.7793721377849578, 'epoch': 0.77}
 26%|â–ˆâ–ˆâ–Œ       | 570/2235 [1:00:32<2:41:03,  5.80s/it] 26%|â–ˆâ–ˆâ–Œ       | 571/2235 [1:00:38<2:40:57,  5.80s/it] 26%|â–ˆâ–ˆâ–Œ       | 572/2235 [1:00:44<2:40:56,  5.81s/it] 26%|â–ˆâ–ˆâ–Œ       | 573/2235 [1:00:50<2:40:53,  5.81s/it] 26%|â–ˆâ–ˆâ–Œ       | 574/2235 [1:00:55<2:40:31,  5.80s/it] 26%|â–ˆâ–ˆâ–Œ       | 575/2235 [1:01:01<2:40:43,  5.81s/it] 26%|â–ˆâ–ˆâ–Œ       | 576/2235 [1:01:07<2:40:41,  5.81s/it] 26%|â–ˆâ–ˆâ–Œ       | 577/2235 [1:01:13<2:40:35,  5.81s/it] 26%|â–ˆâ–ˆâ–Œ       | 578/2235 [1:01:19<2:40:51,  5.82s/it] 26%|â–ˆâ–ˆâ–Œ       | 579/2235 [1:01:24<2:40:42,  5.82s/it] 26%|â–ˆâ–ˆâ–Œ       | 580/2235 [1:01:30<2:40:15,  5.81s/it]                                                      {'loss': 1.0484, 'grad_norm': 0.369140625, 'learning_rate': 4.216739783520447e-06, 'entropy': 1.066180568933487, 'num_tokens': 18603461.0, 'mean_token_accuracy': 0.776048657298088, 'epoch': 0.78}
 26%|â–ˆâ–ˆâ–Œ       | 580/2235 [1:01:30<2:40:15,  5.81s/it] 26%|â–ˆâ–ˆâ–Œ       | 581/2235 [1:01:36<2:40:01,  5.80s/it] 26%|â–ˆâ–ˆâ–Œ       | 582/2235 [1:01:42<2:39:42,  5.80s/it] 26%|â–ˆâ–ˆâ–Œ       | 583/2235 [1:01:48<2:39:24,  5.79s/it] 26%|â–ˆâ–ˆâ–Œ       | 584/2235 [1:01:53<2:38:57,  5.78s/it] 26%|â–ˆâ–ˆâ–Œ       | 585/2235 [1:01:59<2:38:31,  5.76s/it] 26%|â–ˆâ–ˆâ–Œ       | 586/2235 [1:02:05<2:38:13,  5.76s/it] 26%|â–ˆâ–ˆâ–‹       | 587/2235 [1:02:11<2:37:57,  5.75s/it] 26%|â–ˆâ–ˆâ–‹       | 588/2235 [1:02:16<2:37:51,  5.75s/it] 26%|â–ˆâ–ˆâ–‹       | 589/2235 [1:02:22<2:37:45,  5.75s/it] 26%|â–ˆâ–ˆâ–‹       | 590/2235 [1:02:28<2:37:37,  5.75s/it]                                                      {'loss': 1.1137, 'grad_norm': 0.54296875, 'learning_rate': 4.191025567035333e-06, 'entropy': 1.127314254641533, 'num_tokens': 18924342.0, 'mean_token_accuracy': 0.7622944384813308, 'epoch': 0.79}
 26%|â–ˆâ–ˆâ–‹       | 590/2235 [1:02:28<2:37:37,  5.75s/it] 26%|â–ˆâ–ˆâ–‹       | 591/2235 [1:02:34<2:37:33,  5.75s/it] 26%|â–ˆâ–ˆâ–‹       | 592/2235 [1:02:39<2:37:23,  5.75s/it] 27%|â–ˆâ–ˆâ–‹       | 593/2235 [1:02:45<2:37:07,  5.74s/it] 27%|â–ˆâ–ˆâ–‹       | 594/2235 [1:02:51<2:37:04,  5.74s/it] 27%|â–ˆâ–ˆâ–‹       | 595/2235 [1:02:56<2:36:43,  5.73s/it] 27%|â–ˆâ–ˆâ–‹       | 596/2235 [1:03:02<2:36:41,  5.74s/it] 27%|â–ˆâ–ˆâ–‹       | 597/2235 [1:03:08<2:36:33,  5.73s/it] 27%|â–ˆâ–ˆâ–‹       | 598/2235 [1:03:14<2:36:36,  5.74s/it] 27%|â–ˆâ–ˆâ–‹       | 599/2235 [1:03:19<2:36:36,  5.74s/it] 27%|â–ˆâ–ˆâ–‹       | 600/2235 [1:03:25<2:36:34,  5.75s/it]                                                      {'loss': 1.0421, 'grad_norm': 0.44140625, 'learning_rate': 4.164977241904806e-06, 'entropy': 1.0569198101758956, 'num_tokens': 19244867.0, 'mean_token_accuracy': 0.7767518758773804, 'epoch': 0.81}
 27%|â–ˆâ–ˆâ–‹       | 600/2235 [1:03:25<2:36:34,  5.75s/it] 27%|â–ˆâ–ˆâ–‹       | 601/2235 [1:03:31<2:36:14,  5.74s/it] 27%|â–ˆâ–ˆâ–‹       | 602/2235 [1:03:37<2:36:16,  5.74s/it] 27%|â–ˆâ–ˆâ–‹       | 603/2235 [1:03:42<2:36:17,  5.75s/it] 27%|â–ˆâ–ˆâ–‹       | 604/2235 [1:03:48<2:36:17,  5.75s/it] 27%|â–ˆâ–ˆâ–‹       | 605/2235 [1:03:54<2:36:05,  5.75s/it] 27%|â–ˆâ–ˆâ–‹       | 606/2235 [1:04:00<2:36:07,  5.75s/it] 27%|â–ˆâ–ˆâ–‹       | 607/2235 [1:04:05<2:35:56,  5.75s/it] 27%|â–ˆâ–ˆâ–‹       | 608/2235 [1:04:11<2:35:54,  5.75s/it] 27%|â–ˆâ–ˆâ–‹       | 609/2235 [1:04:17<2:35:52,  5.75s/it] 27%|â–ˆâ–ˆâ–‹       | 610/2235 [1:04:23<2:35:55,  5.76s/it]                                                      {'loss': 1.0488, 'grad_norm': 0.376953125, 'learning_rate': 4.138599954692467e-06, 'entropy': 1.0571848154067993, 'num_tokens': 19564943.0, 'mean_token_accuracy': 0.7765240430831909, 'epoch': 0.82}
 27%|â–ˆâ–ˆâ–‹       | 610/2235 [1:04:23<2:35:55,  5.76s/it] 27%|â–ˆâ–ˆâ–‹       | 611/2235 [1:04:28<2:35:43,  5.75s/it] 27%|â–ˆâ–ˆâ–‹       | 612/2235 [1:04:34<2:35:36,  5.75s/it] 27%|â–ˆâ–ˆâ–‹       | 613/2235 [1:04:40<2:35:41,  5.76s/it] 27%|â–ˆâ–ˆâ–‹       | 614/2235 [1:04:46<2:35:40,  5.76s/it] 28%|â–ˆâ–ˆâ–Š       | 615/2235 [1:04:52<2:35:38,  5.76s/it] 28%|â–ˆâ–ˆâ–Š       | 616/2235 [1:04:57<2:35:38,  5.77s/it] 28%|â–ˆâ–ˆâ–Š       | 617/2235 [1:05:03<2:35:36,  5.77s/it] 28%|â–ˆâ–ˆâ–Š       | 618/2235 [1:05:09<2:35:25,  5.77s/it] 28%|â–ˆâ–ˆâ–Š       | 619/2235 [1:05:15<2:35:31,  5.77s/it] 28%|â–ˆâ–ˆâ–Š       | 620/2235 [1:05:20<2:35:36,  5.78s/it]                                                      {'loss': 1.0907, 'grad_norm': 0.46484375, 'learning_rate': 4.1118989169574245e-06, 'entropy': 1.1054577440023423, 'num_tokens': 19885002.0, 'mean_token_accuracy': 0.7662038296461106, 'epoch': 0.83}
 28%|â–ˆâ–ˆâ–Š       | 620/2235 [1:05:20<2:35:36,  5.78s/it] 28%|â–ˆâ–ˆâ–Š       | 621/2235 [1:05:26<2:35:47,  5.79s/it] 28%|â–ˆâ–ˆâ–Š       | 622/2235 [1:05:32<2:35:57,  5.80s/it] 28%|â–ˆâ–ˆâ–Š       | 623/2235 [1:05:38<2:36:07,  5.81s/it] 28%|â–ˆâ–ˆâ–Š       | 624/2235 [1:05:44<2:36:05,  5.81s/it] 28%|â–ˆâ–ˆâ–Š       | 625/2235 [1:05:50<2:35:59,  5.81s/it] 28%|â–ˆâ–ˆâ–Š       | 626/2235 [1:05:55<2:36:06,  5.82s/it] 28%|â–ˆâ–ˆâ–Š       | 627/2235 [1:06:01<2:36:05,  5.82s/it] 28%|â–ˆâ–ˆâ–Š       | 628/2235 [1:06:07<2:36:01,  5.83s/it] 28%|â–ˆâ–ˆâ–Š       | 629/2235 [1:06:13<2:35:58,  5.83s/it] 28%|â–ˆâ–ˆâ–Š       | 630/2235 [1:06:19<2:35:55,  5.83s/it]                                                      {'loss': 1.0453, 'grad_norm': 0.4453125, 'learning_rate': 4.084879404224616e-06, 'entropy': 1.0584942370653152, 'num_tokens': 20203747.0, 'mean_token_accuracy': 0.7779683828353882, 'epoch': 0.85}
 28%|â–ˆâ–ˆâ–Š       | 630/2235 [1:06:19<2:35:55,  5.83s/it] 28%|â–ˆâ–ˆâ–Š       | 631/2235 [1:06:25<2:35:54,  5.83s/it] 28%|â–ˆâ–ˆâ–Š       | 632/2235 [1:06:30<2:35:41,  5.83s/it] 28%|â–ˆâ–ˆâ–Š       | 633/2235 [1:06:36<2:35:48,  5.84s/it] 28%|â–ˆâ–ˆâ–Š       | 634/2235 [1:06:42<2:35:30,  5.83s/it] 28%|â–ˆâ–ˆâ–Š       | 635/2235 [1:06:48<2:35:00,  5.81s/it] 28%|â–ˆâ–ˆâ–Š       | 636/2235 [1:06:54<2:34:42,  5.81s/it] 29%|â–ˆâ–ˆâ–Š       | 637/2235 [1:06:59<2:34:31,  5.80s/it] 29%|â–ˆâ–ˆâ–Š       | 638/2235 [1:07:05<2:34:19,  5.80s/it] 29%|â–ˆâ–ˆâ–Š       | 639/2235 [1:07:11<2:32:24,  5.73s/it] 29%|â–ˆâ–ˆâ–Š       | 640/2235 [1:07:16<2:32:39,  5.74s/it]                                                      {'loss': 1.0679, 'grad_norm': 0.54296875, 'learning_rate': 4.057546754942482e-06, 'entropy': 1.0837658047676086, 'num_tokens': 20523684.0, 'mean_token_accuracy': 0.772279492020607, 'epoch': 0.86}
 29%|â–ˆâ–ˆâ–Š       | 640/2235 [1:07:16<2:32:39,  5.74s/it] 29%|â–ˆâ–ˆâ–Š       | 641/2235 [1:07:22<2:32:51,  5.75s/it] 29%|â–ˆâ–ˆâ–Š       | 642/2235 [1:07:28<2:32:58,  5.76s/it] 29%|â–ˆâ–ˆâ–‰       | 643/2235 [1:07:34<2:32:42,  5.76s/it] 29%|â–ˆâ–ˆâ–‰       | 644/2235 [1:07:40<2:32:44,  5.76s/it] 29%|â–ˆâ–ˆâ–‰       | 645/2235 [1:07:45<2:32:43,  5.76s/it] 29%|â–ˆâ–ˆâ–‰       | 646/2235 [1:07:51<2:32:40,  5.76s/it] 29%|â–ˆâ–ˆâ–‰       | 647/2235 [1:07:57<2:32:35,  5.77s/it] 29%|â–ˆâ–ˆâ–‰       | 648/2235 [1:08:03<2:32:22,  5.76s/it] 29%|â–ˆâ–ˆâ–‰       | 649/2235 [1:08:08<2:32:12,  5.76s/it] 29%|â–ˆâ–ˆâ–‰       | 650/2235 [1:08:14<2:30:37,  5.70s/it]                                                      {'loss': 1.1113, 'grad_norm': 0.435546875, 'learning_rate': 4.029906369428204e-06, 'entropy': 1.1239175856113435, 'num_tokens': 20844391.0, 'mean_token_accuracy': 0.7637724071741104, 'epoch': 0.87}
 29%|â–ˆâ–ˆâ–‰       | 650/2235 [1:08:14<2:30:37,  5.70s/it] 29%|â–ˆâ–ˆâ–‰       | 651/2235 [1:08:20<2:30:58,  5.72s/it] 29%|â–ˆâ–ˆâ–‰       | 652/2235 [1:08:25<2:31:21,  5.74s/it] 29%|â–ˆâ–ˆâ–‰       | 653/2235 [1:08:31<2:31:38,  5.75s/it] 29%|â–ˆâ–ˆâ–‰       | 654/2235 [1:08:37<2:31:28,  5.75s/it] 29%|â–ˆâ–ˆâ–‰       | 655/2235 [1:08:43<2:31:29,  5.75s/it] 29%|â–ˆâ–ˆâ–‰       | 656/2235 [1:08:49<2:31:42,  5.76s/it] 29%|â–ˆâ–ˆâ–‰       | 657/2235 [1:08:54<2:31:33,  5.76s/it] 29%|â–ˆâ–ˆâ–‰       | 658/2235 [1:09:00<2:31:43,  5.77s/it] 29%|â–ˆâ–ˆâ–‰       | 659/2235 [1:09:06<2:31:50,  5.78s/it] 30%|â–ˆâ–ˆâ–‰       | 660/2235 [1:09:12<2:31:46,  5.78s/it]                                                      {'loss': 0.999, 'grad_norm': 0.380859375, 'learning_rate': 4.001963708800729e-06, 'entropy': 1.0195373713970184, 'num_tokens': 21164175.0, 'mean_token_accuracy': 0.7883616536855698, 'epoch': 0.89}
 30%|â–ˆâ–ˆâ–‰       | 660/2235 [1:09:12<2:31:46,  5.78s/it] 30%|â–ˆâ–ˆâ–‰       | 661/2235 [1:09:18<2:31:50,  5.79s/it] 30%|â–ˆâ–ˆâ–‰       | 662/2235 [1:09:23<2:31:57,  5.80s/it] 30%|â–ˆâ–ˆâ–‰       | 663/2235 [1:09:29<2:32:19,  5.81s/it] 30%|â–ˆâ–ˆâ–‰       | 664/2235 [1:09:35<2:32:32,  5.83s/it] 30%|â–ˆâ–ˆâ–‰       | 665/2235 [1:09:41<2:32:12,  5.82s/it] 30%|â–ˆâ–ˆâ–‰       | 666/2235 [1:09:47<2:32:10,  5.82s/it] 30%|â–ˆâ–ˆâ–‰       | 667/2235 [1:09:52<2:32:01,  5.82s/it] 30%|â–ˆâ–ˆâ–‰       | 668/2235 [1:09:58<2:31:50,  5.81s/it] 30%|â–ˆâ–ˆâ–‰       | 669/2235 [1:10:04<2:31:38,  5.81s/it] 30%|â–ˆâ–ˆâ–‰       | 670/2235 [1:10:10<2:29:50,  5.74s/it]                                                      {'loss': 1.0822, 'grad_norm': 0.4296875, 'learning_rate': 3.973724293901772e-06, 'entropy': 1.0949528008699416, 'num_tokens': 21485915.0, 'mean_token_accuracy': 0.7700092285871506, 'epoch': 0.9}
 30%|â–ˆâ–ˆâ–‰       | 670/2235 [1:10:10<2:29:50,  5.74s/it] 30%|â–ˆâ–ˆâ–ˆ       | 671/2235 [1:10:15<2:28:32,  5.70s/it] 30%|â–ˆâ–ˆâ–ˆ       | 672/2235 [1:10:21<2:29:03,  5.72s/it] 30%|â–ˆâ–ˆâ–ˆ       | 673/2235 [1:10:27<2:29:17,  5.73s/it] 30%|â–ˆâ–ˆâ–ˆ       | 674/2235 [1:10:33<2:29:33,  5.75s/it] 30%|â–ˆâ–ˆâ–ˆ       | 675/2235 [1:10:38<2:29:30,  5.75s/it] 30%|â–ˆâ–ˆâ–ˆ       | 676/2235 [1:10:44<2:29:54,  5.77s/it] 30%|â–ˆâ–ˆâ–ˆ       | 677/2235 [1:10:50<2:28:34,  5.72s/it] 30%|â–ˆâ–ˆâ–ˆ       | 678/2235 [1:10:56<2:29:07,  5.75s/it] 30%|â–ˆâ–ˆâ–ˆ       | 679/2235 [1:11:01<2:29:45,  5.77s/it] 30%|â–ˆâ–ˆâ–ˆ       | 680/2235 [1:11:07<2:30:12,  5.80s/it]                                                      {'loss': 1.1097, 'grad_norm': 0.4375, 'learning_rate': 3.945193704205022e-06, 'entropy': 1.1235942602157594, 'num_tokens': 21807634.0, 'mean_token_accuracy': 0.7624377906322479, 'epoch': 0.91}
 30%|â–ˆâ–ˆâ–ˆ       | 680/2235 [1:11:07<2:30:12,  5.80s/it] 30%|â–ˆâ–ˆâ–ˆ       | 681/2235 [1:11:13<2:28:59,  5.75s/it] 31%|â–ˆâ–ˆâ–ˆ       | 682/2235 [1:11:19<2:29:41,  5.78s/it] 31%|â–ˆâ–ˆâ–ˆ       | 683/2235 [1:11:25<2:29:40,  5.79s/it] 31%|â–ˆâ–ˆâ–ˆ       | 684/2235 [1:11:30<2:29:43,  5.79s/it] 31%|â–ˆâ–ˆâ–ˆ       | 685/2235 [1:11:36<2:29:38,  5.79s/it] 31%|â–ˆâ–ˆâ–ˆ       | 686/2235 [1:11:42<2:29:25,  5.79s/it] 31%|â–ˆâ–ˆâ–ˆ       | 687/2235 [1:11:48<2:29:04,  5.78s/it] 31%|â–ˆâ–ˆâ–ˆ       | 688/2235 [1:11:53<2:28:51,  5.77s/it] 31%|â–ˆâ–ˆâ–ˆ       | 689/2235 [1:11:59<2:28:29,  5.76s/it] 31%|â–ˆâ–ˆâ–ˆ       | 690/2235 [1:12:05<2:28:30,  5.77s/it]                                                      {'loss': 1.1209, 'grad_norm': 0.46484375, 'learning_rate': 3.9163775767137645e-06, 'entropy': 1.1331445753574372, 'num_tokens': 22129294.0, 'mean_token_accuracy': 0.7612185537815094, 'epoch': 0.93}
 31%|â–ˆâ–ˆâ–ˆ       | 690/2235 [1:12:05<2:28:30,  5.77s/it] 31%|â–ˆâ–ˆâ–ˆ       | 691/2235 [1:12:11<2:28:32,  5.77s/it] 31%|â–ˆâ–ˆâ–ˆ       | 692/2235 [1:12:16<2:28:05,  5.76s/it] 31%|â–ˆâ–ˆâ–ˆ       | 693/2235 [1:12:22<2:28:09,  5.76s/it] 31%|â–ˆâ–ˆâ–ˆ       | 694/2235 [1:12:28<2:28:10,  5.77s/it] 31%|â–ˆâ–ˆâ–ˆ       | 695/2235 [1:12:34<2:28:00,  5.77s/it] 31%|â–ˆâ–ˆâ–ˆ       | 696/2235 [1:12:40<2:27:59,  5.77s/it] 31%|â–ˆâ–ˆâ–ˆ       | 697/2235 [1:12:45<2:27:47,  5.77s/it] 31%|â–ˆâ–ˆâ–ˆ       | 698/2235 [1:12:51<2:27:50,  5.77s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 699/2235 [1:12:57<2:27:51,  5.78s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 700/2235 [1:13:03<2:27:49,  5.78s/it]                                                      {'loss': 1.0708, 'grad_norm': 0.390625, 'learning_rate': 3.887281604847134e-06, 'entropy': 1.0888219505548478, 'num_tokens': 22451384.0, 'mean_token_accuracy': 0.7726546317338944, 'epoch': 0.94}
 31%|â–ˆâ–ˆâ–ˆâ–      | 700/2235 [1:13:03<2:27:49,  5.78s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 701/2235 [1:13:08<2:27:49,  5.78s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 702/2235 [1:13:14<2:27:35,  5.78s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 703/2235 [1:13:20<2:27:10,  5.76s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 704/2235 [1:13:26<2:27:16,  5.77s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 705/2235 [1:13:32<2:27:20,  5.78s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 706/2235 [1:13:37<2:27:23,  5.78s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 707/2235 [1:13:43<2:25:50,  5.73s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 708/2235 [1:13:49<2:26:22,  5.75s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 709/2235 [1:13:55<2:26:49,  5.77s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 710/2235 [1:14:00<2:27:13,  5.79s/it]                                                      {'loss': 1.0621, 'grad_norm': 0.435546875, 'learning_rate': 3.8579115373152265e-06, 'entropy': 1.0717623353004455, 'num_tokens': 22772517.0, 'mean_token_accuracy': 0.7726196467876434, 'epoch': 0.95}
 32%|â–ˆâ–ˆâ–ˆâ–      | 710/2235 [1:14:00<2:27:13,  5.79s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 711/2235 [1:14:06<2:27:13,  5.80s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 712/2235 [1:14:12<2:27:34,  5.81s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 713/2235 [1:14:18<2:27:39,  5.82s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 714/2235 [1:14:24<2:27:31,  5.82s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 715/2235 [1:14:30<2:27:18,  5.81s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 716/2235 [1:14:35<2:26:54,  5.80s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 717/2235 [1:14:41<2:26:43,  5.80s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 718/2235 [1:14:47<2:26:31,  5.80s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 719/2235 [1:14:53<2:26:21,  5.79s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 720/2235 [1:14:58<2:26:14,  5.79s/it]                                                      {'loss': 0.9903, 'grad_norm': 0.451171875, 'learning_rate': 3.8282731769832786e-06, 'entropy': 1.0039895474910736, 'num_tokens': 23092396.0, 'mean_token_accuracy': 0.7884808361530304, 'epoch': 0.97}
 32%|â–ˆâ–ˆâ–ˆâ–      | 720/2235 [1:14:58<2:26:14,  5.79s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 721/2235 [1:15:04<2:26:00,  5.79s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 722/2235 [1:15:10<2:25:50,  5.78s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 723/2235 [1:15:16<2:25:44,  5.78s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 724/2235 [1:15:22<2:25:40,  5.78s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 725/2235 [1:15:27<2:25:33,  5.78s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 726/2235 [1:15:33<2:25:30,  5.79s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 727/2235 [1:15:39<2:25:26,  5.79s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 728/2235 [1:15:45<2:25:17,  5.78s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 729/2235 [1:15:50<2:25:11,  5.78s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 730/2235 [1:15:56<2:25:04,  5.78s/it]                                                      {'loss': 0.9723, 'grad_norm': 0.453125, 'learning_rate': 3.798372379725155e-06, 'entropy': 0.9860069632530213, 'num_tokens': 23412949.0, 'mean_token_accuracy': 0.7897576361894607, 'epoch': 0.98}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 730/2235 [1:15:56<2:25:04,  5.78s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 731/2235 [1:16:02<2:25:11,  5.79s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 732/2235 [1:16:08<2:25:23,  5.80s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 733/2235 [1:16:14<2:25:37,  5.82s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 734/2235 [1:16:20<2:25:49,  5.83s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 735/2235 [1:16:25<2:25:54,  5.84s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 736/2235 [1:16:31<2:25:30,  5.82s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 737/2235 [1:16:37<2:25:21,  5.82s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 738/2235 [1:16:43<2:25:00,  5.81s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 739/2235 [1:16:49<2:24:51,  5.81s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 740/2235 [1:16:54<2:24:39,  5.81s/it]                                                      {'loss': 0.9932, 'grad_norm': 0.458984375, 'learning_rate': 3.7682150532663585e-06, 'entropy': 1.0076333552598953, 'num_tokens': 23733995.0, 'mean_token_accuracy': 0.787479043006897, 'epoch': 0.99}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 740/2235 [1:16:54<2:24:39,  5.81s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 741/2235 [1:17:00<2:24:29,  5.80s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 742/2235 [1:17:06<2:24:17,  5.80s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 743/2235 [1:17:12<2:24:02,  5.79s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 744/2235 [1:17:18<2:23:58,  5.79s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 745/2235 [1:17:24<2:24:22,  5.81s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 746/2235 [1:17:30<2:27:14,  5.93s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

  0%|          | 0/187 [00:00<?, ?it/s][A
  1%|          | 2/187 [00:01<02:41,  1.15it/s][A
  2%|â–         | 3/187 [00:03<03:47,  1.23s/it][A
  2%|â–         | 4/187 [00:05<04:20,  1.42s/it][A
  3%|â–Ž         | 5/187 [00:06<04:39,  1.54s/it][A
  3%|â–Ž         | 6/187 [00:08<04:50,  1.61s/it][A
  4%|â–Ž         | 7/187 [00:10<04:57,  1.65s/it][A
  4%|â–         | 8/187 [00:12<05:00,  1.68s/it][A
  5%|â–         | 9/187 [00:13<05:02,  1.70s/it][A
  5%|â–Œ         | 10/187 [00:15<05:03,  1.72s/it][A
  6%|â–Œ         | 11/187 [00:17<05:03,  1.73s/it][A
  6%|â–‹         | 12/187 [00:19<05:03,  1.73s/it][A
  7%|â–‹         | 13/187 [00:20<05:02,  1.74s/it][A
  7%|â–‹         | 14/187 [00:22<05:01,  1.74s/it][A
  8%|â–Š         | 15/187 [00:24<05:00,  1.75s/it][A
  9%|â–Š         | 16/187 [00:26<04:59,  1.75s/it][A
  9%|â–‰         | 17/187 [00:27<04:57,  1.75s/it][A
 10%|â–‰         | 18/187 [00:29<04:56,  1.75s/it][A
 10%|â–ˆ         | 19/187 [00:31<04:55,  1.76s/it][A
 11%|â–ˆ         | 20/187 [00:33<04:53,  1.76s/it][A
 11%|â–ˆ         | 21/187 [00:35<04:52,  1.76s/it][A
 12%|â–ˆâ–        | 22/187 [00:36<04:50,  1.76s/it][A
 12%|â–ˆâ–        | 23/187 [00:38<04:48,  1.76s/it][A
 13%|â–ˆâ–Ž        | 24/187 [00:40<04:46,  1.76s/it][A
 13%|â–ˆâ–Ž        | 25/187 [00:42<04:44,  1.76s/it][A
 14%|â–ˆâ–        | 26/187 [00:43<04:42,  1.76s/it][A
 14%|â–ˆâ–        | 27/187 [00:45<04:40,  1.76s/it][A
 15%|â–ˆâ–        | 28/187 [00:47<04:38,  1.75s/it][A
 16%|â–ˆâ–Œ        | 29/187 [00:49<04:36,  1.75s/it][A
 16%|â–ˆâ–Œ        | 30/187 [00:50<04:34,  1.75s/it][A
 17%|â–ˆâ–‹        | 31/187 [00:52<04:32,  1.75s/it][A
 17%|â–ˆâ–‹        | 32/187 [00:54<04:30,  1.75s/it][A
 18%|â–ˆâ–Š        | 33/187 [00:56<04:29,  1.75s/it][A
 18%|â–ˆâ–Š        | 34/187 [00:57<04:27,  1.75s/it][A
 19%|â–ˆâ–Š        | 35/187 [00:59<04:25,  1.75s/it][A
 19%|â–ˆâ–‰        | 36/187 [01:01<04:23,  1.75s/it][A
 20%|â–ˆâ–‰        | 37/187 [01:03<04:21,  1.74s/it][A
 20%|â–ˆâ–ˆ        | 38/187 [01:04<04:19,  1.74s/it][A
 21%|â–ˆâ–ˆ        | 39/187 [01:06<04:17,  1.74s/it][A
 21%|â–ˆâ–ˆâ–       | 40/187 [01:08<04:16,  1.74s/it][A
 22%|â–ˆâ–ˆâ–       | 41/187 [01:09<04:14,  1.74s/it][A
 22%|â–ˆâ–ˆâ–       | 42/187 [01:11<04:12,  1.74s/it][A
 23%|â–ˆâ–ˆâ–Ž       | 43/187 [01:13<04:10,  1.74s/it][A
 24%|â–ˆâ–ˆâ–Ž       | 44/187 [01:15<04:09,  1.74s/it][A
 24%|â–ˆâ–ˆâ–       | 45/187 [01:16<04:07,  1.74s/it][A
 25%|â–ˆâ–ˆâ–       | 46/187 [01:18<04:05,  1.74s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 47/187 [01:20<04:03,  1.74s/it][A
 26%|â–ˆâ–ˆâ–Œ       | 48/187 [01:22<04:01,  1.74s/it][A
 26%|â–ˆâ–ˆâ–Œ       | 49/187 [01:23<04:00,  1.74s/it][A
 27%|â–ˆâ–ˆâ–‹       | 50/187 [01:25<03:58,  1.74s/it][A
 27%|â–ˆâ–ˆâ–‹       | 51/187 [01:27<03:56,  1.74s/it][A
 28%|â–ˆâ–ˆâ–Š       | 52/187 [01:29<03:54,  1.74s/it][A
 28%|â–ˆâ–ˆâ–Š       | 53/187 [01:30<03:53,  1.74s/it][A
 29%|â–ˆâ–ˆâ–‰       | 54/187 [01:32<03:51,  1.74s/it][A
 29%|â–ˆâ–ˆâ–‰       | 55/187 [01:34<03:49,  1.74s/it][A
 30%|â–ˆâ–ˆâ–‰       | 56/187 [01:36<03:48,  1.74s/it][A
 30%|â–ˆâ–ˆâ–ˆ       | 57/187 [01:37<03:46,  1.74s/it][A
 31%|â–ˆâ–ˆâ–ˆ       | 58/187 [01:39<03:44,  1.74s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 59/187 [01:41<03:43,  1.74s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 60/187 [01:43<03:41,  1.74s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 61/187 [01:44<03:39,  1.74s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 62/187 [01:46<03:38,  1.74s/it][A
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 63/187 [01:48<03:36,  1.75s/it][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 64/187 [01:50<03:34,  1.75s/it][A
 35%|â–ˆâ–ˆâ–ˆâ–      | 65/187 [01:51<03:33,  1.75s/it][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 66/187 [01:53<03:31,  1.75s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 67/187 [01:55<03:29,  1.75s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 68/187 [01:57<03:28,  1.75s/it][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 69/187 [01:58<03:26,  1.75s/it][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/187 [02:00<03:24,  1.75s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 71/187 [02:02<03:23,  1.75s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 72/187 [02:04<03:21,  1.75s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 73/187 [02:05<03:20,  1.75s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 74/187 [02:07<03:18,  1.76s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 75/187 [02:09<03:16,  1.76s/it][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76/187 [02:11<03:15,  1.76s/it][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77/187 [02:12<03:13,  1.76s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/187 [02:14<03:12,  1.76s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/187 [02:16<03:10,  1.76s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 80/187 [02:18<03:08,  1.76s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 81/187 [02:19<03:06,  1.76s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/187 [02:21<03:04,  1.76s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/187 [02:23<03:02,  1.75s/it][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/187 [02:25<03:00,  1.75s/it][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 85/187 [02:26<02:58,  1.75s/it][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86/187 [02:28<02:56,  1.75s/it][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 87/187 [02:30<02:54,  1.75s/it][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88/187 [02:32<02:53,  1.75s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 89/187 [02:33<02:51,  1.75s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/187 [02:35<02:49,  1.75s/it][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91/187 [02:37<02:47,  1.75s/it][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 92/187 [02:39<02:45,  1.75s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 93/187 [02:40<02:43,  1.74s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 94/187 [02:42<02:42,  1.74s/it][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 95/187 [02:44<02:40,  1.74s/it][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 96/187 [02:46<02:38,  1.74s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/187 [02:47<02:36,  1.74s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/187 [02:49<02:35,  1.74s/it][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 99/187 [02:51<02:33,  1.74s/it][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 100/187 [02:53<02:31,  1.74s/it][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 101/187 [02:54<02:29,  1.74s/it][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/187 [02:56<02:28,  1.74s/it][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 103/187 [02:58<02:26,  1.74s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 104/187 [03:00<02:24,  1.74s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105/187 [03:01<02:22,  1.74s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 106/187 [03:03<02:21,  1.74s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 107/187 [03:05<02:19,  1.74s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 108/187 [03:07<02:17,  1.74s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 109/187 [03:08<02:15,  1.74s/it][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 110/187 [03:10<02:14,  1.74s/it][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 111/187 [03:12<02:12,  1.74s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112/187 [03:13<02:10,  1.74s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 113/187 [03:15<02:08,  1.74s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114/187 [03:17<02:07,  1.74s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 115/187 [03:19<02:05,  1.74s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/187 [03:20<02:03,  1.74s/it][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 117/187 [03:22<02:02,  1.75s/it][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 118/187 [03:24<02:00,  1.75s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 119/187 [03:26<01:58,  1.75s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120/187 [03:27<01:56,  1.75s/it][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121/187 [03:29<01:55,  1.75s/it][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 122/187 [03:31<01:53,  1.75s/it][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 123/187 [03:33<01:51,  1.75s/it][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 124/187 [03:34<01:50,  1.75s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 125/187 [03:36<01:48,  1.75s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126/187 [03:38<01:46,  1.75s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 127/187 [03:40<01:45,  1.75s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 128/187 [03:41<01:43,  1.76s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 129/187 [03:43<01:41,  1.76s/it][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/187 [03:45<01:40,  1.76s/it][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 131/187 [03:47<01:38,  1.76s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 132/187 [03:49<01:36,  1.76s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133/187 [03:50<01:34,  1.76s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 134/187 [03:52<01:33,  1.76s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/187 [03:54<01:31,  1.76s/it][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 136/187 [03:56<01:29,  1.76s/it][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 137/187 [03:57<01:27,  1.76s/it][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138/187 [03:59<01:25,  1.75s/it][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 139/187 [04:01<01:24,  1.75s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/187 [04:03<01:22,  1.75s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/187 [04:04<01:20,  1.75s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 142/187 [04:06<01:18,  1.75s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 143/187 [04:08<01:16,  1.75s/it][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 144/187 [04:10<01:15,  1.74s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 145/187 [04:11<01:13,  1.74s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 146/187 [04:13<01:11,  1.74s/it][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 147/187 [04:15<01:09,  1.74s/it][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 148/187 [04:16<01:07,  1.74s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 149/187 [04:18<01:06,  1.74s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 150/187 [04:20<01:04,  1.74s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 151/187 [04:22<01:02,  1.74s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 152/187 [04:23<01:00,  1.74s/it][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 153/187 [04:25<00:59,  1.74s/it][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 154/187 [04:27<00:57,  1.74s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 155/187 [04:29<00:55,  1.73s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 156/187 [04:30<00:53,  1.73s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 157/187 [04:32<00:51,  1.73s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 158/187 [04:34<00:50,  1.73s/it][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 159/187 [04:36<00:48,  1.73s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 160/187 [04:37<00:46,  1.73s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 161/187 [04:39<00:45,  1.73s/it][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 162/187 [04:41<00:43,  1.73s/it][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 163/187 [04:42<00:41,  1.73s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 164/187 [04:44<00:39,  1.73s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 165/187 [04:46<00:38,  1.73s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 166/187 [04:48<00:36,  1.73s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 167/187 [04:49<00:34,  1.73s/it][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 168/187 [04:51<00:32,  1.73s/it][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 169/187 [04:53<00:31,  1.73s/it][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 170/187 [04:55<00:29,  1.73s/it][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 171/187 [04:56<00:27,  1.73s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 172/187 [04:58<00:25,  1.73s/it][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 173/187 [05:00<00:24,  1.73s/it][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 174/187 [05:02<00:22,  1.73s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 175/187 [05:03<00:20,  1.73s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 176/187 [05:05<00:19,  1.73s/it][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 177/187 [05:07<00:17,  1.73s/it][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 178/187 [05:08<00:15,  1.73s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 179/187 [05:10<00:13,  1.73s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 180/187 [05:12<00:12,  1.73s/it][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 181/187 [05:14<00:10,  1.74s/it][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 182/187 [05:15<00:08,  1.74s/it][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 183/187 [05:17<00:06,  1.74s/it][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 184/187 [05:19<00:05,  1.74s/it][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 185/187 [05:21<00:03,  1.74s/it][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 186/187 [05:22<00:01,  1.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [05:24<00:00,  1.76s/it][A                                                      
                                                 [A{'eval_loss': 1.052016258239746, 'eval_runtime': 326.8495, 'eval_samples_per_second': 18.235, 'eval_steps_per_second': 0.572, 'eval_entropy': 1.0690938090895588, 'eval_num_tokens': 23926310.0, 'eval_mean_token_accuracy': 0.7752448674191765, 'epoch': 1.0}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 746/2235 [1:22:57<2:27:14,  5.93s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [05:24<00:00,  1.76s/it][A
                                                 [A/home/ubuntu/Part-II-Project/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 747/2235 [1:23:03<43:02:55, 104.15s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 748/2235 [1:23:09<30:49:57, 74.65s/it]  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 749/2235 [1:23:15<22:16:57, 53.98s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 750/2235 [1:23:20<16:18:26, 39.53s/it]                                                       {'loss': 0.9585, 'grad_norm': 0.4453125, 'learning_rate': 3.7378071560167972e-06, 'entropy': 0.9726844549179077, 'num_tokens': 24054684.0, 'mean_token_accuracy': 0.795480141043663, 'epoch': 1.01}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 750/2235 [1:23:20<16:18:26, 39.53s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 751/2235 [1:23:26<12:07:40, 29.42s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 752/2235 [1:23:32<9:12:16, 22.34s/it]  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 753/2235 [1:23:38<7:09:31, 17.39s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 754/2235 [1:23:44<5:43:55, 13.93s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 755/2235 [1:23:50<4:44:00, 11.51s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 756/2235 [1:23:55<4:01:29,  9.80s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 757/2235 [1:24:01<3:31:49,  8.60s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 758/2235 [1:24:07<3:10:52,  7.75s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 759/2235 [1:24:13<2:56:09,  7.16s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 760/2235 [1:24:19<2:45:47,  6.74s/it]                                                      {'loss': 1.0218, 'grad_norm': 0.390625, 'learning_rate': 3.707154695893535e-06, 'entropy': 1.0358541697263717, 'num_tokens': 24374768.0, 'mean_token_accuracy': 0.7828753590583801, 'epoch': 1.02}
 34%|â–ˆâ–ˆâ–ˆâ–      | 760/2235 [1:24:19<2:45:47,  6.74s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 761/2235 [1:24:24<2:38:33,  6.45s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 762/2235 [1:24:30<2:33:26,  6.25s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 763/2235 [1:24:36<2:29:47,  6.11s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 764/2235 [1:24:42<2:27:09,  6.00s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 765/2235 [1:24:47<2:25:15,  5.93s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 766/2235 [1:24:53<2:23:57,  5.88s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 767/2235 [1:24:59<2:22:57,  5.84s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 768/2235 [1:25:05<2:22:16,  5.82s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 769/2235 [1:25:10<2:21:47,  5.80s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 770/2235 [1:25:16<2:21:25,  5.79s/it]                                                      {'loss': 1.0527, 'grad_norm': 0.455078125, 'learning_rate': 3.676263729133763e-06, 'entropy': 1.0662991136312485, 'num_tokens': 24694025.0, 'mean_token_accuracy': 0.7748148083686829, 'epoch': 1.03}
 34%|â–ˆâ–ˆâ–ˆâ–      | 770/2235 [1:25:16<2:21:25,  5.79s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 771/2235 [1:25:22<2:21:00,  5.78s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 772/2235 [1:25:28<2:20:31,  5.76s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 773/2235 [1:25:33<2:20:18,  5.76s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 774/2235 [1:25:39<2:18:37,  5.69s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 775/2235 [1:25:45<2:19:06,  5.72s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 776/2235 [1:25:51<2:19:20,  5.73s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 777/2235 [1:25:56<2:19:32,  5.74s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 778/2235 [1:26:02<2:19:27,  5.74s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 779/2235 [1:26:08<2:19:35,  5.75s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 780/2235 [1:26:14<2:19:20,  5.75s/it]                                                      {'loss': 1.0426, 'grad_norm': 0.40625, 'learning_rate': 3.645140359098223e-06, 'entropy': 1.0518782079219817, 'num_tokens': 25016584.0, 'mean_token_accuracy': 0.7761045336723328, 'epoch': 1.05}
 35%|â–ˆâ–ˆâ–ˆâ–      | 780/2235 [1:26:14<2:19:20,  5.75s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 781/2235 [1:26:19<2:19:28,  5.76s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 782/2235 [1:26:25<2:19:34,  5.76s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 783/2235 [1:26:31<2:19:36,  5.77s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 784/2235 [1:26:37<2:19:38,  5.77s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 785/2235 [1:26:42<2:18:08,  5.72s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 786/2235 [1:26:48<2:18:35,  5.74s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 787/2235 [1:26:54<2:18:51,  5.75s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 788/2235 [1:27:00<2:19:12,  5.77s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 789/2235 [1:27:05<2:19:25,  5.78s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 790/2235 [1:27:11<2:19:43,  5.80s/it]                                                      {'loss': 1.0716, 'grad_norm': 0.4453125, 'learning_rate': 3.613790735065321e-06, 'entropy': 1.0860304415225983, 'num_tokens': 25338267.0, 'mean_token_accuracy': 0.7704268753528595, 'epoch': 1.06}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 790/2235 [1:27:11<2:19:43,  5.80s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 791/2235 [1:27:17<2:19:54,  5.81s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 792/2235 [1:27:23<2:20:04,  5.82s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 793/2235 [1:27:29<2:19:56,  5.82s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 794/2235 [1:27:35<2:19:16,  5.80s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 795/2235 [1:27:40<2:19:04,  5.80s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 796/2235 [1:27:46<2:18:52,  5.79s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 797/2235 [1:27:52<2:18:45,  5.79s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 798/2235 [1:27:58<2:18:34,  5.79s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 799/2235 [1:28:03<2:18:07,  5.77s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 800/2235 [1:28:09<2:18:01,  5.77s/it]                                                      {'loss': 0.9974, 'grad_norm': 0.423828125, 'learning_rate': 3.582221051016167e-06, 'entropy': 1.0109491735696792, 'num_tokens': 25660673.0, 'mean_token_accuracy': 0.7846065580844879, 'epoch': 1.07}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 800/2235 [1:28:09<2:18:01,  5.77s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 801/2235 [1:28:15<2:17:47,  5.77s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 802/2235 [1:28:21<2:17:45,  5.77s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 803/2235 [1:28:26<2:17:30,  5.76s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 804/2235 [1:28:32<2:17:29,  5.76s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 805/2235 [1:28:38<2:17:25,  5.77s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 806/2235 [1:28:44<2:17:18,  5.77s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 807/2235 [1:28:50<2:17:15,  5.77s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 808/2235 [1:28:55<2:16:48,  5.75s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 809/2235 [1:29:01<2:16:51,  5.76s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 810/2235 [1:29:07<2:16:53,  5.76s/it]                                                      {'loss': 1.0607, 'grad_norm': 0.361328125, 'learning_rate': 3.5504375444107857e-06, 'entropy': 1.0704508244991302, 'num_tokens': 25981155.0, 'mean_token_accuracy': 0.7717965364456176, 'epoch': 1.09}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 810/2235 [1:29:07<2:16:53,  5.76s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 811/2235 [1:29:13<2:16:43,  5.76s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 812/2235 [1:29:18<2:16:22,  5.75s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 813/2235 [1:29:24<2:14:59,  5.70s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 814/2235 [1:29:30<2:15:27,  5.72s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 815/2235 [1:29:35<2:15:43,  5.73s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 816/2235 [1:29:41<2:15:52,  5.75s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 817/2235 [1:29:47<2:15:59,  5.75s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 818/2235 [1:29:53<2:16:04,  5.76s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 819/2235 [1:29:59<2:16:10,  5.77s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 820/2235 [1:30:04<2:16:09,  5.77s/it]                                                      {'loss': 1.0384, 'grad_norm': 0.302734375, 'learning_rate': 3.518446494955732e-06, 'entropy': 1.049910706281662, 'num_tokens': 26303464.0, 'mean_token_accuracy': 0.776140707731247, 'epoch': 1.1}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 820/2235 [1:30:04<2:16:09,  5.77s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 821/2235 [1:30:10<2:16:06,  5.78s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 822/2235 [1:30:16<2:16:16,  5.79s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 823/2235 [1:30:22<2:16:14,  5.79s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 824/2235 [1:30:27<2:15:00,  5.74s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 825/2235 [1:30:33<2:15:45,  5.78s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 826/2235 [1:30:39<2:16:09,  5.80s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 827/2235 [1:30:45<2:16:15,  5.81s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 828/2235 [1:30:51<2:15:56,  5.80s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 829/2235 [1:30:56<2:15:51,  5.80s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 830/2235 [1:31:02<2:15:43,  5.80s/it]                                                      {'loss': 1.0081, 'grad_norm': 0.435546875, 'learning_rate': 3.486254223363363e-06, 'entropy': 1.017721939086914, 'num_tokens': 26624419.0, 'mean_token_accuracy': 0.7837122052907943, 'epoch': 1.11}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 830/2235 [1:31:02<2:15:43,  5.80s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 831/2235 [1:31:08<2:15:34,  5.79s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 832/2235 [1:31:14<2:15:24,  5.79s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 833/2235 [1:31:20<2:15:13,  5.79s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 834/2235 [1:31:25<2:15:07,  5.79s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 835/2235 [1:31:31<2:15:00,  5.79s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 836/2235 [1:31:37<2:14:57,  5.79s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 837/2235 [1:31:43<2:14:52,  5.79s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 838/2235 [1:31:49<2:14:44,  5.79s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 839/2235 [1:31:54<2:14:23,  5.78s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 840/2235 [1:32:00<2:14:28,  5.78s/it]                                                      {'loss': 1.047, 'grad_norm': 0.498046875, 'learning_rate': 3.45386709010301e-06, 'entropy': 1.0612652212381364, 'num_tokens': 26945368.0, 'mean_token_accuracy': 0.7753722727298736, 'epoch': 1.13}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 840/2235 [1:32:00<2:14:28,  5.78s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 841/2235 [1:32:06<2:14:18,  5.78s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 842/2235 [1:32:12<2:14:22,  5.79s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 843/2235 [1:32:17<2:14:27,  5.80s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 844/2235 [1:32:23<2:14:33,  5.80s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 845/2235 [1:32:29<2:14:42,  5.81s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 846/2235 [1:32:35<2:14:21,  5.80s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 847/2235 [1:32:41<2:14:39,  5.82s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 848/2235 [1:32:47<2:14:47,  5.83s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 849/2235 [1:32:52<2:14:45,  5.83s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 850/2235 [1:32:58<2:14:36,  5.83s/it]                                                      {'loss': 1.0402, 'grad_norm': 0.37890625, 'learning_rate': 3.4212914941442866e-06, 'entropy': 1.0517249524593353, 'num_tokens': 27266372.0, 'mean_token_accuracy': 0.7777868688106537, 'epoch': 1.14}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 850/2235 [1:32:58<2:14:36,  5.83s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 851/2235 [1:33:04<2:14:28,  5.83s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 852/2235 [1:33:10<2:12:48,  5.76s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 853/2235 [1:33:16<2:13:01,  5.78s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 854/2235 [1:33:21<2:12:58,  5.78s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 855/2235 [1:33:27<2:12:53,  5.78s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 856/2235 [1:33:33<2:12:57,  5.78s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 857/2235 [1:33:39<2:12:35,  5.77s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 858/2235 [1:33:44<2:12:46,  5.79s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 859/2235 [1:33:50<2:12:52,  5.79s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 860/2235 [1:33:56<2:12:51,  5.80s/it]                                                      {'loss': 1.0565, 'grad_norm': 0.322265625, 'learning_rate': 3.3885338716928018e-06, 'entropy': 1.0694272726774217, 'num_tokens': 27588250.0, 'mean_token_accuracy': 0.7739326179027557, 'epoch': 1.15}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 860/2235 [1:33:56<2:12:51,  5.80s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 861/2235 [1:34:02<2:12:51,  5.80s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 862/2235 [1:34:08<2:13:02,  5.81s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 863/2235 [1:34:14<2:13:13,  5.83s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 864/2235 [1:34:19<2:13:06,  5.83s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 865/2235 [1:34:25<2:13:10,  5.83s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 866/2235 [1:34:31<2:13:07,  5.83s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 867/2235 [1:34:37<2:11:37,  5.77s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 868/2235 [1:34:43<2:11:56,  5.79s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 869/2235 [1:34:48<2:11:52,  5.79s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 870/2235 [1:34:54<2:11:49,  5.79s/it]                                                      {'loss': 1.0865, 'grad_norm': 0.490234375, 'learning_rate': 3.355600694918509e-06, 'entropy': 1.0952667474746705, 'num_tokens': 27908871.0, 'mean_token_accuracy': 0.7681076914072037, 'epoch': 1.17}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 870/2235 [1:34:54<2:11:49,  5.79s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 871/2235 [1:35:00<2:11:59,  5.81s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 872/2235 [1:35:06<2:11:49,  5.80s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 873/2235 [1:35:12<2:11:48,  5.81s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 874/2235 [1:35:17<2:11:56,  5.82s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 875/2235 [1:35:23<2:12:09,  5.83s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 876/2235 [1:35:29<2:12:03,  5.83s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 877/2235 [1:35:35<2:11:56,  5.83s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 878/2235 [1:35:41<2:11:51,  5.83s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 879/2235 [1:35:47<2:11:42,  5.83s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 880/2235 [1:35:52<2:11:31,  5.82s/it]                                                      {'loss': 1.0666, 'grad_norm': 0.3515625, 'learning_rate': 3.3224984706769493e-06, 'entropy': 1.081832680106163, 'num_tokens': 28230246.0, 'mean_token_accuracy': 0.770926421880722, 'epoch': 1.18}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 880/2235 [1:35:52<2:11:31,  5.82s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 881/2235 [1:35:58<2:11:18,  5.82s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 882/2235 [1:36:04<2:10:57,  5.81s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 883/2235 [1:36:10<2:10:40,  5.80s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 884/2235 [1:36:16<2:10:40,  5.80s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 885/2235 [1:36:21<2:10:21,  5.79s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 886/2235 [1:36:27<2:10:19,  5.80s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 887/2235 [1:36:33<2:10:00,  5.79s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 888/2235 [1:36:39<2:09:56,  5.79s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 889/2235 [1:36:45<2:10:01,  5.80s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 890/2235 [1:36:50<2:09:50,  5.79s/it]                                                      {'loss': 1.0728, 'grad_norm': 0.359375, 'learning_rate': 3.289233739223643e-06, 'entropy': 1.0894311308860778, 'num_tokens': 28552198.0, 'mean_token_accuracy': 0.7702942490577698, 'epoch': 1.19}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 890/2235 [1:36:50<2:09:50,  5.79s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 891/2235 [1:36:56<2:10:01,  5.80s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 892/2235 [1:37:02<2:10:07,  5.81s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 893/2235 [1:37:08<2:10:16,  5.82s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 894/2235 [1:37:14<2:10:27,  5.84s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 895/2235 [1:37:20<2:10:30,  5.84s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 896/2235 [1:37:25<2:10:12,  5.83s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 897/2235 [1:37:31<2:10:00,  5.83s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 898/2235 [1:37:37<2:09:46,  5.82s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 899/2235 [1:37:43<2:09:36,  5.82s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 900/2235 [1:37:49<2:09:23,  5.82s/it]                                                      {'loss': 1.0355, 'grad_norm': 0.408203125, 'learning_rate': 3.2558130729218838e-06, 'entropy': 1.0447801679372788, 'num_tokens': 28871415.0, 'mean_token_accuracy': 0.779350534081459, 'epoch': 1.21}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 900/2235 [1:37:49<2:09:23,  5.82s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 901/2235 [1:37:54<2:09:13,  5.81s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 902/2235 [1:38:00<2:09:08,  5.81s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 903/2235 [1:38:06<2:09:03,  5.81s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 904/2235 [1:38:12<2:08:57,  5.81s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 905/2235 [1:38:18<2:08:45,  5.81s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 906/2235 [1:38:23<2:08:42,  5.81s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 907/2235 [1:38:29<2:08:43,  5.82s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 908/2235 [1:38:35<2:08:40,  5.82s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 909/2235 [1:38:41<2:08:30,  5.82s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 910/2235 [1:38:47<2:08:34,  5.82s/it]                                                      {'loss': 1.0, 'grad_norm': 0.369140625, 'learning_rate': 3.2222430749441885e-06, 'entropy': 1.0129377841949463, 'num_tokens': 29191415.0, 'mean_token_accuracy': 0.7850586831569671, 'epoch': 1.22}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 910/2235 [1:38:47<2:08:34,  5.82s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 911/2235 [1:38:53<2:08:35,  5.83s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 912/2235 [1:38:58<2:08:43,  5.84s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 913/2235 [1:39:04<2:08:45,  5.84s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 914/2235 [1:39:10<2:08:44,  5.85s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 915/2235 [1:39:16<2:08:18,  5.83s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 916/2235 [1:39:22<2:07:55,  5.82s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 917/2235 [1:39:28<2:07:48,  5.82s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 918/2235 [1:39:33<2:07:39,  5.82s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 919/2235 [1:39:39<2:07:15,  5.80s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 920/2235 [1:39:45<2:07:09,  5.80s/it]                                                      {'loss': 0.9943, 'grad_norm': 0.388671875, 'learning_rate': 3.1885303779676547e-06, 'entropy': 1.0093487471342086, 'num_tokens': 29512290.0, 'mean_token_accuracy': 0.7863223046064377, 'epoch': 1.23}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 920/2235 [1:39:45<2:07:09,  5.80s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 921/2235 [1:39:51<2:07:03,  5.80s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 922/2235 [1:39:57<2:06:56,  5.80s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 923/2235 [1:40:02<2:06:41,  5.79s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 924/2235 [1:40:08<2:06:40,  5.80s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 925/2235 [1:40:14<2:06:41,  5.80s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 926/2235 [1:40:20<2:06:31,  5.80s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 927/2235 [1:40:25<2:05:13,  5.74s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 928/2235 [1:40:31<2:05:42,  5.77s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 929/2235 [1:40:37<2:05:57,  5.79s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 930/2235 [1:40:43<2:06:08,  5.80s/it]                                                      {'loss': 1.0712, 'grad_norm': 0.421875, 'learning_rate': 3.154681642863495e-06, 'entropy': 1.084900113940239, 'num_tokens': 29834050.0, 'mean_token_accuracy': 0.7708555936813355, 'epoch': 1.25}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 930/2235 [1:40:43<2:06:08,  5.80s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 931/2235 [1:40:49<2:06:26,  5.82s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 932/2235 [1:40:55<2:06:30,  5.83s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 933/2235 [1:41:00<2:06:27,  5.83s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 934/2235 [1:41:06<2:06:18,  5.82s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 935/2235 [1:41:12<2:06:07,  5.82s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 936/2235 [1:41:18<2:05:56,  5.82s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 937/2235 [1:41:24<2:05:34,  5.80s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 938/2235 [1:41:29<2:05:29,  5.81s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 939/2235 [1:41:35<2:05:25,  5.81s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 940/2235 [1:41:41<2:05:17,  5.81s/it]                                                      {'loss': 1.0114, 'grad_norm': 0.341796875, 'learning_rate': 3.120703557380999e-06, 'entropy': 1.0233947038650513, 'num_tokens': 30153470.0, 'mean_token_accuracy': 0.7859404414892197, 'epoch': 1.26}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 940/2235 [1:41:41<2:05:17,  5.81s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 941/2235 [1:41:47<2:05:16,  5.81s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 942/2235 [1:41:53<2:04:58,  5.80s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 943/2235 [1:41:58<2:04:55,  5.80s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 944/2235 [1:42:04<2:05:01,  5.81s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 945/2235 [1:42:10<2:03:49,  5.76s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 946/2235 [1:42:16<2:04:21,  5.79s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 947/2235 [1:42:22<2:04:40,  5.81s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 948/2235 [1:42:27<2:04:41,  5.81s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 949/2235 [1:42:33<2:04:22,  5.80s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 950/2235 [1:42:39<2:04:16,  5.80s/it]                                                      {'loss': 0.9773, 'grad_norm': 0.359375, 'learning_rate': 3.086602834826184e-06, 'entropy': 0.9873128771781922, 'num_tokens': 30474409.0, 'mean_token_accuracy': 0.7897472977638245, 'epoch': 1.28}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 950/2235 [1:42:39<2:04:16,  5.80s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 951/2235 [1:42:45<2:04:09,  5.80s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 952/2235 [1:42:51<2:03:51,  5.79s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 953/2235 [1:42:56<2:03:49,  5.80s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 954/2235 [1:43:02<2:03:35,  5.79s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 955/2235 [1:43:08<2:03:19,  5.78s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 956/2235 [1:43:14<2:03:18,  5.78s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 957/2235 [1:43:20<2:03:11,  5.78s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 958/2235 [1:43:25<2:03:13,  5.79s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 959/2235 [1:43:31<2:03:11,  5.79s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 960/2235 [1:43:37<2:03:00,  5.79s/it]                                                      {'loss': 1.0829, 'grad_norm': 0.314453125, 'learning_rate': 3.052386212735396e-06, 'entropy': 1.0888720393180846, 'num_tokens': 30795151.0, 'mean_token_accuracy': 0.7697723865509033, 'epoch': 1.29}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 960/2235 [1:43:37<2:03:00,  5.79s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 961/2235 [1:43:43<2:03:03,  5.80s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 962/2235 [1:43:48<2:02:56,  5.79s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 963/2235 [1:43:54<2:03:00,  5.80s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 964/2235 [1:44:00<2:02:49,  5.80s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 965/2235 [1:44:06<2:02:49,  5.80s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 966/2235 [1:44:12<2:02:53,  5.81s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 967/2235 [1:44:17<2:01:41,  5.76s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 968/2235 [1:44:23<2:02:03,  5.78s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 969/2235 [1:44:29<2:02:19,  5.80s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 970/2235 [1:44:35<2:02:28,  5.81s/it]                                                      {'loss': 1.0841, 'grad_norm': 0.455078125, 'learning_rate': 3.0180604515441252e-06, 'entropy': 1.1010666161775589, 'num_tokens': 31115844.0, 'mean_token_accuracy': 0.7681137681007385, 'epoch': 1.3}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 970/2235 [1:44:35<2:02:28,  5.81s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 971/2235 [1:44:41<2:02:26,  5.81s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 972/2235 [1:44:47<2:02:32,  5.82s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 973/2235 [1:44:52<2:02:39,  5.83s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 974/2235 [1:44:58<2:02:47,  5.84s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 975/2235 [1:45:04<2:02:32,  5.83s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 976/2235 [1:45:10<2:02:34,  5.84s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 977/2235 [1:45:16<2:02:30,  5.84s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 978/2235 [1:45:22<2:02:30,  5.85s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 979/2235 [1:45:28<2:02:26,  5.85s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 980/2235 [1:45:33<2:02:21,  5.85s/it]                                                      {'loss': 1.0941, 'grad_norm': 0.337890625, 'learning_rate': 2.983632333251294e-06, 'entropy': 1.1095115512609481, 'num_tokens': 31437053.0, 'mean_token_accuracy': 0.7650691449642182, 'epoch': 1.32}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 980/2235 [1:45:33<2:02:21,  5.85s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 981/2235 [1:45:39<2:02:09,  5.84s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 982/2235 [1:45:45<2:01:43,  5.83s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 983/2235 [1:45:51<2:01:16,  5.81s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 984/2235 [1:45:57<2:00:51,  5.80s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 985/2235 [1:46:02<2:00:43,  5.80s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 986/2235 [1:46:08<2:00:36,  5.79s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 987/2235 [1:46:14<2:00:14,  5.78s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 988/2235 [1:46:20<2:00:07,  5.78s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 989/2235 [1:46:25<1:59:58,  5.78s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 990/2235 [1:46:31<1:59:41,  5.77s/it]                                                      {'loss': 1.0624, 'grad_norm': 0.37890625, 'learning_rate': 2.949108660079287e-06, 'entropy': 1.0731544315814971, 'num_tokens': 31758296.0, 'mean_token_accuracy': 0.7724775850772858, 'epoch': 1.33}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 990/2235 [1:46:31<1:59:41,  5.77s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 991/2235 [1:46:37<1:59:33,  5.77s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 992/2235 [1:46:43<1:59:29,  5.77s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 993/2235 [1:46:48<1:59:26,  5.77s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 994/2235 [1:46:54<1:59:26,  5.78s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 995/2235 [1:47:00<1:59:16,  5.77s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 996/2235 [1:47:06<1:59:19,  5.78s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 997/2235 [1:47:12<1:59:07,  5.77s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 998/2235 [1:47:17<1:59:06,  5.78s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 999/2235 [1:47:23<1:59:01,  5.78s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1000/2235 [1:47:29<1:59:03,  5.78s/it]                                                       {'loss': 1.0454, 'grad_norm': 0.34375, 'learning_rate': 2.914496253129988e-06, 'entropy': 1.0559472173452378, 'num_tokens': 32078874.0, 'mean_token_accuracy': 0.7758657991886139, 'epoch': 1.34}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1000/2235 [1:47:29<1:59:03,  5.78s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1001/2235 [1:47:35<1:59:09,  5.79s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1002/2235 [1:47:41<1:59:15,  5.80s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1003/2235 [1:47:46<1:59:13,  5.81s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1004/2235 [1:47:52<1:59:23,  5.82s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1005/2235 [1:47:58<1:59:34,  5.83s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1006/2235 [1:48:04<1:59:37,  5.84s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1007/2235 [1:48:10<1:59:22,  5.83s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1008/2235 [1:48:16<1:59:01,  5.82s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1009/2235 [1:48:21<1:58:42,  5.81s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1010/2235 [1:48:27<1:58:12,  5.79s/it]                                                       {'loss': 1.0377, 'grad_norm': 0.431640625, 'learning_rate': 2.879801951037079e-06, 'entropy': 1.047998884320259, 'num_tokens': 32399098.0, 'mean_token_accuracy': 0.7769852876663208, 'epoch': 1.36}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1010/2235 [1:48:27<1:58:12,  5.79s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1011/2235 [1:48:33<1:57:58,  5.78s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1012/2235 [1:48:39<1:57:53,  5.78s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1013/2235 [1:48:44<1:57:28,  5.77s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1014/2235 [1:48:50<1:57:25,  5.77s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1015/2235 [1:48:56<1:57:18,  5.77s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1016/2235 [1:49:02<1:57:15,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1017/2235 [1:49:07<1:56:57,  5.76s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1018/2235 [1:49:13<1:56:57,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1019/2235 [1:49:19<1:56:54,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1020/2235 [1:49:25<1:56:50,  5.77s/it]                                                       {'loss': 1.0385, 'grad_norm': 0.400390625, 'learning_rate': 2.845032608614883e-06, 'entropy': 1.0438404023647307, 'num_tokens': 32718907.0, 'mean_token_accuracy': 0.778150862455368, 'epoch': 1.37}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1020/2235 [1:49:25<1:56:50,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1021/2235 [1:49:31<1:56:40,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1022/2235 [1:49:36<1:56:37,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1023/2235 [1:49:42<1:56:29,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1024/2235 [1:49:48<1:56:21,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1025/2235 [1:49:54<1:56:15,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1026/2235 [1:49:59<1:56:13,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1027/2235 [1:50:05<1:56:11,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1028/2235 [1:50:11<1:56:06,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1029/2235 [1:50:17<1:56:04,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1030/2235 [1:50:22<1:56:03,  5.78s/it]                                                       {'loss': 1.0338, 'grad_norm': 0.376953125, 'learning_rate': 2.8101950955040014e-06, 'entropy': 1.0398916512727738, 'num_tokens': 33038487.0, 'mean_token_accuracy': 0.7786698579788208, 'epoch': 1.38}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1030/2235 [1:50:22<1:56:03,  5.78s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1031/2235 [1:50:28<1:55:48,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1032/2235 [1:50:34<1:55:42,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1033/2235 [1:50:40<1:55:34,  5.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1034/2235 [1:50:46<1:55:49,  5.79s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1035/2235 [1:50:51<1:55:54,  5.80s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1036/2235 [1:50:57<1:56:09,  5.81s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1037/2235 [1:51:03<1:56:18,  5.83s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1038/2235 [1:51:09<1:56:18,  5.83s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1039/2235 [1:51:15<1:56:01,  5.82s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1040/2235 [1:51:21<1:55:44,  5.81s/it]                                                       {'loss': 1.042, 'grad_norm': 0.37890625, 'learning_rate': 2.7752962948140366e-06, 'entropy': 1.054762241244316, 'num_tokens': 33360222.0, 'mean_token_accuracy': 0.7759602278470993, 'epoch': 1.4}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1040/2235 [1:51:21<1:55:44,  5.81s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1041/2235 [1:51:26<1:55:33,  5.81s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1042/2235 [1:51:32<1:55:21,  5.80s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1043/2235 [1:51:38<1:55:07,  5.80s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1044/2235 [1:51:44<1:54:56,  5.79s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1045/2235 [1:51:49<1:54:45,  5.79s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1046/2235 [1:51:55<1:54:38,  5.78s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1047/2235 [1:52:01<1:54:22,  5.78s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1048/2235 [1:52:07<1:54:18,  5.78s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1049/2235 [1:52:13<1:54:14,  5.78s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1050/2235 [1:52:18<1:54:05,  5.78s/it]                                                       {'loss': 0.9608, 'grad_norm': 0.314453125, 'learning_rate': 2.7403431017636373e-06, 'entropy': 0.9731470316648483, 'num_tokens': 33678261.0, 'mean_token_accuracy': 0.7945953696966171, 'epoch': 1.41}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1050/2235 [1:52:18<1:54:05,  5.78s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1051/2235 [1:52:24<1:54:04,  5.78s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1052/2235 [1:52:30<1:54:03,  5.79s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1053/2235 [1:52:36<1:54:05,  5.79s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1054/2235 [1:52:42<1:54:08,  5.80s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1055/2235 [1:52:47<1:54:10,  5.81s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1056/2235 [1:52:53<1:54:12,  5.81s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1057/2235 [1:52:59<1:54:16,  5.82s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1058/2235 [1:53:05<1:54:22,  5.83s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1059/2235 [1:53:11<1:54:09,  5.82s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1060/2235 [1:53:17<1:53:59,  5.82s/it]                                                       {'loss': 1.0421, 'grad_norm': 0.359375, 'learning_rate': 2.7053424223181597e-06, 'entropy': 1.0576844692230225, 'num_tokens': 33998824.0, 'mean_token_accuracy': 0.7766127616167069, 'epoch': 1.42}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1060/2235 [1:53:17<1:53:59,  5.82s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1061/2235 [1:53:22<1:53:49,  5.82s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1062/2235 [1:53:28<1:53:26,  5.80s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1063/2235 [1:53:34<1:53:16,  5.80s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1064/2235 [1:53:40<1:53:10,  5.80s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1065/2235 [1:53:45<1:52:57,  5.79s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1066/2235 [1:53:51<1:52:50,  5.79s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1067/2235 [1:53:57<1:52:43,  5.79s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1068/2235 [1:54:03<1:52:38,  5.79s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1069/2235 [1:54:09<1:52:32,  5.79s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1070/2235 [1:54:14<1:52:25,  5.79s/it]                                                       {'loss': 1.044, 'grad_norm': 0.42578125, 'learning_rate': 2.670301171825204e-06, 'entropy': 1.0571115136146545, 'num_tokens': 34318229.0, 'mean_token_accuracy': 0.7770692825317382, 'epoch': 1.44}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1070/2235 [1:54:14<1:52:25,  5.79s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1071/2235 [1:54:20<1:52:13,  5.78s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1072/2235 [1:54:26<1:52:05,  5.78s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1073/2235 [1:54:32<1:52:04,  5.79s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1074/2235 [1:54:38<1:52:01,  5.79s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1075/2235 [1:54:43<1:52:00,  5.79s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1076/2235 [1:54:49<1:52:02,  5.80s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1077/2235 [1:54:55<1:52:03,  5.81s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1078/2235 [1:55:01<1:52:08,  5.82s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1079/2235 [1:55:07<1:52:15,  5.83s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1080/2235 [1:55:13<1:52:11,  5.83s/it]                                                       {'loss': 1.0076, 'grad_norm': 0.390625, 'learning_rate': 2.635226273648299e-06, 'entropy': 1.0233392417430878, 'num_tokens': 34638749.0, 'mean_token_accuracy': 0.7849345713853836, 'epoch': 1.45}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1080/2235 [1:55:13<1:52:11,  5.83s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1081/2235 [1:55:18<1:52:16,  5.84s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1082/2235 [1:55:24<1:52:14,  5.84s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1083/2235 [1:55:30<1:52:02,  5.84s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1084/2235 [1:55:36<1:51:44,  5.82s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1085/2235 [1:55:42<1:51:32,  5.82s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1086/2235 [1:55:47<1:51:24,  5.82s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1087/2235 [1:55:53<1:51:13,  5.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1088/2235 [1:55:59<1:51:04,  5.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1089/2235 [1:56:05<1:50:56,  5.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1090/2235 [1:56:11<1:50:48,  5.81s/it]                                                       {'loss': 0.9933, 'grad_norm': 0.376953125, 'learning_rate': 2.600124657798998e-06, 'entropy': 1.0054837137460708, 'num_tokens': 34958319.0, 'mean_token_accuracy': 0.7891388654708862, 'epoch': 1.46}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1090/2235 [1:56:11<1:50:48,  5.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1091/2235 [1:56:16<1:50:44,  5.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1092/2235 [1:56:22<1:50:42,  5.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1093/2235 [1:56:28<1:50:39,  5.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1094/2235 [1:56:34<1:50:38,  5.82s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1095/2235 [1:56:40<1:50:36,  5.82s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1096/2235 [1:56:46<1:50:16,  5.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1097/2235 [1:56:51<1:50:16,  5.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1098/2235 [1:56:57<1:50:27,  5.83s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1099/2235 [1:57:03<1:50:25,  5.83s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1100/2235 [1:57:09<1:50:20,  5.83s/it]                                                       {'loss': 1.0415, 'grad_norm': 0.345703125, 'learning_rate': 2.565003259567667e-06, 'entropy': 1.055000039935112, 'num_tokens': 35279331.0, 'mean_token_accuracy': 0.7775984466075897, 'epoch': 1.48}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1100/2235 [1:57:09<1:50:20,  5.83s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1101/2235 [1:57:15<1:50:07,  5.83s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1102/2235 [1:57:21<1:49:56,  5.82s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1103/2235 [1:57:26<1:49:46,  5.82s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1104/2235 [1:57:32<1:49:32,  5.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1105/2235 [1:57:38<1:49:24,  5.81s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1106/2235 [1:57:44<1:49:17,  5.81s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1107/2235 [1:57:50<1:49:11,  5.81s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1108/2235 [1:57:55<1:49:00,  5.80s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1109/2235 [1:58:01<1:48:57,  5.81s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1110/2235 [1:58:07<1:48:38,  5.79s/it]                                                       {'loss': 1.064, 'grad_norm': 0.4140625, 'learning_rate': 2.529869018153227e-06, 'entropy': 1.0733202546834946, 'num_tokens': 35598664.0, 'mean_token_accuracy': 0.7719453305006028, 'epoch': 1.49}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1110/2235 [1:58:07<1:48:38,  5.79s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1111/2235 [1:58:13<1:48:44,  5.80s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1112/2235 [1:58:19<1:48:32,  5.80s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1113/2235 [1:58:24<1:48:41,  5.81s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1114/2235 [1:58:30<1:48:45,  5.82s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1115/2235 [1:58:36<1:47:30,  5.76s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1116/2235 [1:58:42<1:47:46,  5.78s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1117/2235 [1:58:47<1:47:51,  5.79s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1118/2235 [1:58:53<1:47:34,  5.78s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1119/2235 [1:58:59<1:47:36,  5.79s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

  0%|          | 0/187 [00:00<?, ?it/s][A
  1%|          | 2/187 [00:01<02:41,  1.15it/s][A
  2%|â–         | 3/187 [00:03<03:47,  1.23s/it][A
  2%|â–         | 4/187 [00:05<04:20,  1.42s/it][A
  3%|â–Ž         | 5/187 [00:06<04:39,  1.54s/it][A
  3%|â–Ž         | 6/187 [00:08<04:50,  1.61s/it][A
  4%|â–Ž         | 7/187 [00:10<04:56,  1.65s/it][A
  4%|â–         | 8/187 [00:12<05:00,  1.68s/it][A
  5%|â–         | 9/187 [00:13<05:02,  1.70s/it][A
  5%|â–Œ         | 10/187 [00:15<05:03,  1.71s/it][A
  6%|â–Œ         | 11/187 [00:17<05:03,  1.72s/it][A
  6%|â–‹         | 12/187 [00:19<05:02,  1.73s/it][A
  7%|â–‹         | 13/187 [00:20<05:02,  1.74s/it][A
  7%|â–‹         | 14/187 [00:22<05:00,  1.74s/it][A
  8%|â–Š         | 15/187 [00:24<04:59,  1.74s/it][A
  9%|â–Š         | 16/187 [00:26<04:58,  1.74s/it][A
  9%|â–‰         | 17/187 [00:27<04:56,  1.75s/it][A
 10%|â–‰         | 18/187 [00:29<04:55,  1.75s/it][A
 10%|â–ˆ         | 19/187 [00:31<04:53,  1.75s/it][A
 11%|â–ˆ         | 20/187 [00:33<04:52,  1.75s/it][A
 11%|â–ˆ         | 21/187 [00:34<04:50,  1.75s/it][A
 12%|â–ˆâ–        | 22/187 [00:36<04:49,  1.75s/it][A
 12%|â–ˆâ–        | 23/187 [00:38<04:47,  1.75s/it][A
 13%|â–ˆâ–Ž        | 24/187 [00:40<04:45,  1.75s/it][A
 13%|â–ˆâ–Ž        | 25/187 [00:41<04:44,  1.76s/it][A
 14%|â–ˆâ–        | 26/187 [00:43<04:42,  1.76s/it][A
 14%|â–ˆâ–        | 27/187 [00:45<04:41,  1.76s/it][A
 15%|â–ˆâ–        | 28/187 [00:47<04:39,  1.76s/it][A
 16%|â–ˆâ–Œ        | 29/187 [00:49<04:38,  1.76s/it][A
 16%|â–ˆâ–Œ        | 30/187 [00:50<04:36,  1.76s/it][A
 17%|â–ˆâ–‹        | 31/187 [00:52<04:34,  1.76s/it][A
 17%|â–ˆâ–‹        | 32/187 [00:54<04:33,  1.76s/it][A
 18%|â–ˆâ–Š        | 33/187 [00:56<04:31,  1.76s/it][A
 18%|â–ˆâ–Š        | 34/187 [00:57<04:29,  1.76s/it][A
 19%|â–ˆâ–Š        | 35/187 [00:59<04:27,  1.76s/it][A
 19%|â–ˆâ–‰        | 36/187 [01:01<04:25,  1.76s/it][A
 20%|â–ˆâ–‰        | 37/187 [01:03<04:23,  1.76s/it][A
 20%|â–ˆâ–ˆ        | 38/187 [01:04<04:21,  1.75s/it][A
 21%|â–ˆâ–ˆ        | 39/187 [01:06<04:19,  1.75s/it][A
 21%|â–ˆâ–ˆâ–       | 40/187 [01:08<04:17,  1.75s/it][A
 22%|â–ˆâ–ˆâ–       | 41/187 [01:10<04:15,  1.75s/it][A
 22%|â–ˆâ–ˆâ–       | 42/187 [01:11<04:13,  1.75s/it][A
 23%|â–ˆâ–ˆâ–Ž       | 43/187 [01:13<04:11,  1.75s/it][A
 24%|â–ˆâ–ˆâ–Ž       | 44/187 [01:15<04:09,  1.74s/it][A
 24%|â–ˆâ–ˆâ–       | 45/187 [01:17<04:07,  1.74s/it][A
 25%|â–ˆâ–ˆâ–       | 46/187 [01:18<04:05,  1.74s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 47/187 [01:20<04:03,  1.74s/it][A
 26%|â–ˆâ–ˆâ–Œ       | 48/187 [01:22<04:01,  1.74s/it][A
 26%|â–ˆâ–ˆâ–Œ       | 49/187 [01:23<03:59,  1.74s/it][A
 27%|â–ˆâ–ˆâ–‹       | 50/187 [01:25<03:57,  1.74s/it][A
 27%|â–ˆâ–ˆâ–‹       | 51/187 [01:27<03:56,  1.74s/it][A
 28%|â–ˆâ–ˆâ–Š       | 52/187 [01:29<03:54,  1.74s/it][A
 28%|â–ˆâ–ˆâ–Š       | 53/187 [01:30<03:52,  1.73s/it][A
 29%|â–ˆâ–ˆâ–‰       | 54/187 [01:32<03:50,  1.73s/it][A
 29%|â–ˆâ–ˆâ–‰       | 55/187 [01:34<03:48,  1.73s/it][A
 30%|â–ˆâ–ˆâ–‰       | 56/187 [01:36<03:47,  1.73s/it][A
 30%|â–ˆâ–ˆâ–ˆ       | 57/187 [01:37<03:45,  1.73s/it][A
 31%|â–ˆâ–ˆâ–ˆ       | 58/187 [01:39<03:43,  1.73s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 59/187 [01:41<03:41,  1.73s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 60/187 [01:43<03:39,  1.73s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 61/187 [01:44<03:38,  1.73s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 62/187 [01:46<03:36,  1.73s/it][A
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 63/187 [01:48<03:34,  1.73s/it][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 64/187 [01:49<03:33,  1.73s/it][A
 35%|â–ˆâ–ˆâ–ˆâ–      | 65/187 [01:51<03:31,  1.73s/it][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 66/187 [01:53<03:29,  1.73s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 67/187 [01:55<03:27,  1.73s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 68/187 [01:56<03:26,  1.73s/it][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 69/187 [01:58<03:24,  1.73s/it][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/187 [02:00<03:22,  1.73s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 71/187 [02:02<03:21,  1.73s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 72/187 [02:03<03:19,  1.73s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 73/187 [02:05<03:17,  1.73s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 74/187 [02:07<03:15,  1.73s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 75/187 [02:09<03:14,  1.73s/it][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76/187 [02:10<03:12,  1.73s/it][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77/187 [02:12<03:10,  1.73s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/187 [02:14<03:09,  1.73s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/187 [02:15<03:07,  1.73s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 80/187 [02:17<03:05,  1.74s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 81/187 [02:19<03:03,  1.74s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/187 [02:21<03:02,  1.74s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/187 [02:22<03:00,  1.74s/it][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/187 [02:24<02:59,  1.74s/it][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 85/187 [02:26<02:57,  1.74s/it][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86/187 [02:28<02:55,  1.74s/it][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 87/187 [02:29<02:53,  1.74s/it][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88/187 [02:31<02:52,  1.74s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 89/187 [02:33<02:50,  1.74s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/187 [02:35<02:48,  1.74s/it][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91/187 [02:36<02:47,  1.74s/it][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 92/187 [02:38<02:45,  1.74s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 93/187 [02:40<02:44,  1.75s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 94/187 [02:42<02:42,  1.75s/it][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 95/187 [02:43<02:40,  1.75s/it][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 96/187 [02:45<02:39,  1.75s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/187 [02:47<02:37,  1.75s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/187 [02:49<02:35,  1.75s/it][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 99/187 [02:50<02:34,  1.75s/it][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 100/187 [02:52<02:32,  1.75s/it][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 101/187 [02:54<02:30,  1.75s/it][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/187 [02:56<02:29,  1.75s/it][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 103/187 [02:57<02:27,  1.75s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 104/187 [02:59<02:25,  1.75s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105/187 [03:01<02:23,  1.75s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 106/187 [03:03<02:21,  1.75s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 107/187 [03:04<02:19,  1.75s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 108/187 [03:06<02:17,  1.75s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 109/187 [03:08<02:16,  1.75s/it][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 110/187 [03:10<02:14,  1.74s/it][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 111/187 [03:11<02:12,  1.74s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112/187 [03:13<02:10,  1.74s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 113/187 [03:15<02:09,  1.74s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114/187 [03:17<02:07,  1.74s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 115/187 [03:18<02:05,  1.74s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/187 [03:20<02:03,  1.74s/it][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 117/187 [03:22<02:01,  1.74s/it][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 118/187 [03:24<02:00,  1.74s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 119/187 [03:25<01:58,  1.74s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120/187 [03:27<01:56,  1.74s/it][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121/187 [03:29<01:54,  1.74s/it][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 122/187 [03:31<01:53,  1.74s/it][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 123/187 [03:32<01:51,  1.74s/it][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 124/187 [03:34<01:49,  1.74s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 125/187 [03:36<01:47,  1.74s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126/187 [03:37<01:46,  1.74s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 127/187 [03:39<01:44,  1.74s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 128/187 [03:41<01:42,  1.74s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 129/187 [03:43<01:41,  1.74s/it][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/187 [03:44<01:39,  1.74s/it][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 131/187 [03:46<01:37,  1.74s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 132/187 [03:48<01:35,  1.74s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133/187 [03:50<01:34,  1.74s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 134/187 [03:51<01:32,  1.75s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/187 [03:53<01:30,  1.75s/it][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 136/187 [03:55<01:28,  1.74s/it][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 137/187 [03:57<01:27,  1.75s/it][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138/187 [03:58<01:25,  1.75s/it][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 139/187 [04:00<01:23,  1.75s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/187 [04:02<01:22,  1.75s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/187 [04:04<01:20,  1.75s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 142/187 [04:05<01:18,  1.75s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 143/187 [04:07<01:17,  1.75s/it][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 144/187 [04:09<01:15,  1.75s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 145/187 [04:11<01:13,  1.75s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 146/187 [04:12<01:11,  1.75s/it][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 147/187 [04:14<01:10,  1.76s/it][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 148/187 [04:16<01:08,  1.76s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 149/187 [04:18<01:06,  1.76s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 150/187 [04:19<01:05,  1.76s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 151/187 [04:21<01:03,  1.76s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 152/187 [04:23<01:01,  1.76s/it][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 153/187 [04:25<00:59,  1.76s/it][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 154/187 [04:27<00:58,  1.76s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 155/187 [04:28<00:56,  1.76s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 156/187 [04:30<00:54,  1.76s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 157/187 [04:32<00:52,  1.75s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 158/187 [04:34<00:50,  1.75s/it][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 159/187 [04:35<00:49,  1.75s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 160/187 [04:37<00:47,  1.75s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 161/187 [04:39<00:45,  1.75s/it][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 162/187 [04:41<00:43,  1.74s/it][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 163/187 [04:42<00:41,  1.74s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 164/187 [04:44<00:40,  1.74s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 165/187 [04:46<00:38,  1.74s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 166/187 [04:47<00:36,  1.74s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 167/187 [04:49<00:34,  1.74s/it][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 168/187 [04:51<00:32,  1.74s/it][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 169/187 [04:53<00:31,  1.74s/it][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 170/187 [04:54<00:29,  1.73s/it][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 171/187 [04:56<00:27,  1.73s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 172/187 [04:58<00:26,  1.73s/it][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 173/187 [05:00<00:24,  1.73s/it][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 174/187 [05:01<00:22,  1.73s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 175/187 [05:03<00:20,  1.73s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 176/187 [05:05<00:19,  1.73s/it][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 177/187 [05:07<00:17,  1.73s/it][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 178/187 [05:08<00:15,  1.73s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 179/187 [05:10<00:13,  1.73s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 180/187 [05:12<00:12,  1.73s/it][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 181/187 [05:13<00:10,  1.73s/it][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 182/187 [05:15<00:08,  1.73s/it][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 183/187 [05:17<00:06,  1.73s/it][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 184/187 [05:19<00:05,  1.73s/it][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 185/187 [05:20<00:03,  1.73s/it][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 186/187 [05:22<00:01,  1.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [05:24<00:00,  1.76s/it][A                                                       
                                                 [A{'eval_loss': 1.0424184799194336, 'eval_runtime': 326.5858, 'eval_samples_per_second': 18.249, 'eval_steps_per_second': 0.573, 'eval_entropy': 1.05737207216375, 'eval_num_tokens': 35887428.0, 'eval_mean_token_accuracy': 0.7766447296754562, 'epoch': 1.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1119/2235 [2:04:26<1:47:36,  5.79s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [05:24<00:00,  1.76s/it][A
                                                 [A/home/ubuntu/Part-II-Project/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1120/2235 [2:04:32<32:11:44, 103.95s/it]                                                         {'loss': 1.085, 'grad_norm': 0.45703125, 'learning_rate': 2.4947288752921203e-06, 'entropy': 1.0939969420433044, 'num_tokens': 35919347.0, 'mean_token_accuracy': 0.7677031457424164, 'epoch': 1.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1120/2235 [2:04:32<32:11:44, 103.95s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1121/2235 [2:04:38<23:03:13, 74.50s/it]  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1122/2235 [2:04:44<16:39:33, 53.88s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1123/2235 [2:04:49<12:11:12, 39.45s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1124/2235 [2:04:55<9:03:27, 29.35s/it]  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1125/2235 [2:05:01<6:52:06, 22.28s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1126/2235 [2:05:07<5:20:11, 17.32s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1127/2235 [2:05:12<4:15:55, 13.86s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1128/2235 [2:05:18<3:30:58, 11.43s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1129/2235 [2:05:24<2:59:31,  9.74s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1130/2235 [2:05:30<2:37:27,  8.55s/it]                                                       {'loss': 0.9939, 'grad_norm': 0.34375, 'learning_rate': 2.459589773886784e-06, 'entropy': 1.0065664976835251, 'num_tokens': 36239095.0, 'mean_token_accuracy': 0.7865509986877441, 'epoch': 1.52}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1130/2235 [2:05:30<2:37:27,  8.55s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1131/2235 [2:05:36<2:22:06,  7.72s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1132/2235 [2:05:41<2:11:16,  7.14s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1133/2235 [2:05:47<2:03:44,  6.74s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1134/2235 [2:05:53<1:58:31,  6.46s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1135/2235 [2:05:59<1:54:39,  6.25s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1136/2235 [2:06:05<1:52:12,  6.13s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1137/2235 [2:06:10<1:50:26,  6.04s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1138/2235 [2:06:16<1:49:11,  5.97s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1139/2235 [2:06:22<1:48:18,  5.93s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1140/2235 [2:06:28<1:47:42,  5.90s/it]                                                       {'loss': 1.0532, 'grad_norm': 0.39453125, 'learning_rate': 2.424458656633889e-06, 'entropy': 1.0626732587814331, 'num_tokens': 36561067.0, 'mean_token_accuracy': 0.7741308778524398, 'epoch': 1.53}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1140/2235 [2:06:28<1:47:42,  5.90s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1141/2235 [2:06:34<1:47:02,  5.87s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1142/2235 [2:06:40<1:46:49,  5.86s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1143/2235 [2:06:45<1:46:41,  5.86s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1144/2235 [2:06:51<1:46:32,  5.86s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1145/2235 [2:06:57<1:46:15,  5.85s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1146/2235 [2:07:03<1:45:46,  5.83s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1147/2235 [2:07:09<1:45:32,  5.82s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1148/2235 [2:07:14<1:45:18,  5.81s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1149/2235 [2:07:20<1:45:04,  5.81s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1150/2235 [2:07:26<1:44:43,  5.79s/it]                                                       {'loss': 1.0562, 'grad_norm': 0.38671875, 'learning_rate': 2.3893424646526135e-06, 'entropy': 1.0675539165735244, 'num_tokens': 36882026.0, 'mean_token_accuracy': 0.7734154284000396, 'epoch': 1.54}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1150/2235 [2:07:26<1:44:43,  5.79s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1151/2235 [2:07:32<1:44:33,  5.79s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1152/2235 [2:07:38<1:44:21,  5.78s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1153/2235 [2:07:43<1:44:14,  5.78s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1154/2235 [2:07:49<1:44:08,  5.78s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1155/2235 [2:07:55<1:44:03,  5.78s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1156/2235 [2:08:01<1:43:57,  5.78s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1157/2235 [2:08:06<1:43:54,  5.78s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1158/2235 [2:08:12<1:43:53,  5.79s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1159/2235 [2:08:18<1:43:52,  5.79s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1160/2235 [2:08:24<1:43:52,  5.80s/it]                                                       {'loss': 0.975, 'grad_norm': 0.375, 'learning_rate': 2.35424813611324e-06, 'entropy': 0.984581372141838, 'num_tokens': 37201959.0, 'mean_token_accuracy': 0.7918414026498795, 'epoch': 1.56}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1160/2235 [2:08:24<1:43:52,  5.80s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1161/2235 [2:08:30<1:43:31,  5.78s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1162/2235 [2:08:35<1:42:33,  5.73s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1163/2235 [2:08:41<1:42:51,  5.76s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1164/2235 [2:08:47<1:43:16,  5.79s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1165/2235 [2:08:53<1:43:22,  5.80s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1166/2235 [2:08:59<1:43:36,  5.82s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1167/2235 [2:09:04<1:43:22,  5.81s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1168/2235 [2:09:10<1:43:23,  5.81s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1169/2235 [2:09:16<1:43:13,  5.81s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1170/2235 [2:09:22<1:43:07,  5.81s/it]                                                       {'loss': 1.0682, 'grad_norm': 0.333984375, 'learning_rate': 2.3191826048663247e-06, 'entropy': 1.0780428767204284, 'num_tokens': 37523762.0, 'mean_token_accuracy': 0.7695128232240677, 'epoch': 1.57}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1170/2235 [2:09:22<1:43:07,  5.81s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1171/2235 [2:09:28<1:43:02,  5.81s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1172/2235 [2:09:33<1:42:54,  5.81s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1173/2235 [2:09:39<1:42:43,  5.80s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1174/2235 [2:09:45<1:42:34,  5.80s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1175/2235 [2:09:51<1:42:31,  5.80s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1176/2235 [2:09:57<1:42:17,  5.80s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1177/2235 [2:10:02<1:42:07,  5.79s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1178/2235 [2:10:08<1:41:59,  5.79s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1179/2235 [2:10:14<1:42:06,  5.80s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1180/2235 [2:10:20<1:42:08,  5.81s/it]                                                       {'loss': 1.0773, 'grad_norm': 0.390625, 'learning_rate': 2.284152799072729e-06, 'entropy': 1.0899081259965897, 'num_tokens': 37845204.0, 'mean_token_accuracy': 0.7706481128931045, 'epoch': 1.58}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1180/2235 [2:10:20<1:42:08,  5.81s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1181/2235 [2:10:26<1:42:19,  5.83s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1182/2235 [2:10:32<1:42:21,  5.83s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1183/2235 [2:10:37<1:42:14,  5.83s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1184/2235 [2:10:43<1:42:09,  5.83s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1185/2235 [2:10:49<1:40:52,  5.76s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1186/2235 [2:10:55<1:40:48,  5.77s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1187/2235 [2:11:00<1:40:51,  5.77s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1188/2235 [2:11:06<1:40:50,  5.78s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1189/2235 [2:11:12<1:40:48,  5.78s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1190/2235 [2:11:18<1:40:43,  5.78s/it]                                                       {'loss': 0.9957, 'grad_norm': 0.384765625, 'learning_rate': 2.2491656398347624e-06, 'entropy': 1.0101190894842147, 'num_tokens': 38163895.0, 'mean_token_accuracy': 0.7865963220596314, 'epoch': 1.6}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1190/2235 [2:11:18<1:40:43,  5.78s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1191/2235 [2:11:24<1:40:39,  5.78s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1192/2235 [2:11:29<1:40:32,  5.78s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1193/2235 [2:11:35<1:40:27,  5.78s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1194/2235 [2:11:41<1:40:22,  5.79s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1195/2235 [2:11:47<1:40:18,  5.79s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1196/2235 [2:11:52<1:40:13,  5.79s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1197/2235 [2:11:58<1:39:59,  5.78s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1198/2235 [2:12:04<1:40:00,  5.79s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1199/2235 [2:12:10<1:40:01,  5.79s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1200/2235 [2:12:16<1:40:03,  5.80s/it]                                                       {'loss': 0.9991, 'grad_norm': 0.41015625, 'learning_rate': 2.214228039828739e-06, 'entropy': 1.0087910741567612, 'num_tokens': 38481977.0, 'mean_token_accuracy': 0.7855011224746704, 'epoch': 1.61}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1200/2235 [2:12:16<1:40:03,  5.80s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1201/2235 [2:12:21<1:40:04,  5.81s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1202/2235 [2:12:27<1:39:46,  5.80s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1203/2235 [2:12:33<1:39:44,  5.80s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1204/2235 [2:12:39<1:40:03,  5.82s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1205/2235 [2:12:45<1:40:02,  5.83s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1206/2235 [2:12:51<1:39:53,  5.82s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1207/2235 [2:12:56<1:39:29,  5.81s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1208/2235 [2:13:02<1:39:14,  5.80s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1209/2235 [2:13:08<1:39:06,  5.80s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1210/2235 [2:13:14<1:38:56,  5.79s/it]                                                       {'loss': 1.0272, 'grad_norm': 0.3984375, 'learning_rate': 2.1793469019391785e-06, 'entropy': 1.0392370998859406, 'num_tokens': 38803033.0, 'mean_token_accuracy': 0.7792941004037857, 'epoch': 1.62}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1210/2235 [2:13:14<1:38:56,  5.79s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1211/2235 [2:13:19<1:38:48,  5.79s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1212/2235 [2:13:25<1:38:39,  5.79s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1213/2235 [2:13:31<1:38:31,  5.78s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1214/2235 [2:13:37<1:38:23,  5.78s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1215/2235 [2:13:43<1:38:06,  5.77s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1216/2235 [2:13:48<1:38:06,  5.78s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1217/2235 [2:13:54<1:37:54,  5.77s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1218/2235 [2:14:00<1:37:37,  5.76s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1219/2235 [2:14:06<1:37:29,  5.76s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1220/2235 [2:14:11<1:37:30,  5.76s/it]                                                       {'loss': 1.003, 'grad_norm': 0.427734375, 'learning_rate': 2.144529117894959e-06, 'entropy': 1.0154441148042679, 'num_tokens': 39123081.0, 'mean_token_accuracy': 0.7850352376699448, 'epoch': 1.64}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1220/2235 [2:14:11<1:37:30,  5.76s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1221/2235 [2:14:17<1:37:24,  5.76s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1222/2235 [2:14:23<1:37:27,  5.77s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1223/2235 [2:14:29<1:37:27,  5.78s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1224/2235 [2:14:35<1:37:28,  5.78s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1225/2235 [2:14:40<1:37:17,  5.78s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1226/2235 [2:14:46<1:37:18,  5.79s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1227/2235 [2:14:52<1:37:19,  5.79s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1228/2235 [2:14:58<1:37:20,  5.80s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1229/2235 [2:15:04<1:37:25,  5.81s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1230/2235 [2:15:09<1:36:28,  5.76s/it]                                                       {'loss': 1.0395, 'grad_norm': 0.400390625, 'learning_rate': 2.109781566907659e-06, 'entropy': 1.0566665261983872, 'num_tokens': 39444966.0, 'mean_token_accuracy': 0.7777735620737076, 'epoch': 1.65}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1230/2235 [2:15:09<1:36:28,  5.76s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1231/2235 [2:15:15<1:36:45,  5.78s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1232/2235 [2:15:21<1:37:02,  5.81s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1233/2235 [2:15:27<1:37:07,  5.82s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1234/2235 [2:15:33<1:36:57,  5.81s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1235/2235 [2:15:38<1:36:51,  5.81s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1236/2235 [2:15:44<1:36:45,  5.81s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1237/2235 [2:15:50<1:36:27,  5.80s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1238/2235 [2:15:56<1:36:11,  5.79s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1239/2235 [2:16:02<1:36:10,  5.79s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1240/2235 [2:16:07<1:35:59,  5.79s/it]                                                       {'loss': 1.0738, 'grad_norm': 0.349609375, 'learning_rate': 2.075111114312386e-06, 'entropy': 1.0899074107408524, 'num_tokens': 39765965.0, 'mean_token_accuracy': 0.7702788233757019, 'epoch': 1.66}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1240/2235 [2:16:07<1:35:59,  5.79s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1241/2235 [2:16:13<1:35:54,  5.79s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1242/2235 [2:16:19<1:35:52,  5.79s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1243/2235 [2:16:25<1:35:51,  5.80s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1244/2235 [2:16:30<1:35:47,  5.80s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1245/2235 [2:16:36<1:35:43,  5.80s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1246/2235 [2:16:42<1:35:30,  5.79s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1247/2235 [2:16:48<1:35:28,  5.80s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1248/2235 [2:16:54<1:35:28,  5.80s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1249/2235 [2:17:00<1:35:37,  5.82s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1250/2235 [2:17:05<1:35:35,  5.82s/it]                                                       {'loss': 1.0048, 'grad_norm': 0.453125, 'learning_rate': 2.0405246102113367e-06, 'entropy': 1.0214883595705033, 'num_tokens': 40086819.0, 'mean_token_accuracy': 0.7842409133911132, 'epoch': 1.68}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1250/2235 [2:17:05<1:35:35,  5.82s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1251/2235 [2:17:11<1:35:41,  5.83s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1252/2235 [2:17:17<1:35:36,  5.84s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1253/2235 [2:17:23<1:35:22,  5.83s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1254/2235 [2:17:29<1:35:15,  5.83s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1255/2235 [2:17:34<1:34:49,  5.81s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1256/2235 [2:17:40<1:34:43,  5.81s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1257/2235 [2:17:46<1:34:30,  5.80s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1258/2235 [2:17:52<1:34:25,  5.80s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1259/2235 [2:17:57<1:33:12,  5.73s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1260/2235 [2:18:03<1:33:28,  5.75s/it]                                                       {'loss': 0.9761, 'grad_norm': 0.455078125, 'learning_rate': 2.006028888120373e-06, 'entropy': 0.9861928582191467, 'num_tokens': 40406483.0, 'mean_token_accuracy': 0.7909777760505676, 'epoch': 1.69}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1260/2235 [2:18:03<1:33:28,  5.75s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1261/2235 [2:18:09<1:33:32,  5.76s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1262/2235 [2:18:15<1:33:40,  5.78s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1263/2235 [2:18:21<1:33:37,  5.78s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1264/2235 [2:18:26<1:33:42,  5.79s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1265/2235 [2:18:32<1:33:47,  5.80s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1266/2235 [2:18:38<1:33:52,  5.81s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1267/2235 [2:18:44<1:33:51,  5.82s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1268/2235 [2:18:50<1:33:59,  5.83s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1269/2235 [2:18:56<1:34:05,  5.84s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1270/2235 [2:19:01<1:33:57,  5.84s/it]                                                       {'loss': 0.9938, 'grad_norm': 0.47265625, 'learning_rate': 1.9716307636188725e-06, 'entropy': 1.005899891257286, 'num_tokens': 40726017.0, 'mean_token_accuracy': 0.7857262104749679, 'epoch': 1.7}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1270/2235 [2:19:02<1:33:57,  5.84s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1271/2235 [2:19:07<1:33:37,  5.83s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1272/2235 [2:19:13<1:33:22,  5.82s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1273/2235 [2:19:19<1:33:13,  5.81s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1274/2235 [2:19:25<1:33:03,  5.81s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1275/2235 [2:19:30<1:32:55,  5.81s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1276/2235 [2:19:36<1:32:31,  5.79s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1277/2235 [2:19:42<1:32:26,  5.79s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1278/2235 [2:19:48<1:32:21,  5.79s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1279/2235 [2:19:54<1:32:17,  5.79s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1280/2235 [2:19:59<1:32:03,  5.78s/it]                                                       {'loss': 0.9957, 'grad_norm': 0.447265625, 'learning_rate': 1.937337033003121e-06, 'entropy': 1.0103913217782974, 'num_tokens': 41046569.0, 'mean_token_accuracy': 0.7868249028921127, 'epoch': 1.72}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1280/2235 [2:19:59<1:32:03,  5.78s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1281/2235 [2:20:05<1:32:04,  5.79s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1282/2235 [2:20:11<1:32:04,  5.80s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1283/2235 [2:20:17<1:32:04,  5.80s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1284/2235 [2:20:23<1:31:59,  5.80s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1285/2235 [2:20:28<1:31:57,  5.81s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1286/2235 [2:20:34<1:32:04,  5.82s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1287/2235 [2:20:40<1:31:59,  5.82s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1288/2235 [2:20:46<1:31:59,  5.83s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1289/2235 [2:20:52<1:31:55,  5.83s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1290/2235 [2:20:58<1:31:42,  5.82s/it]                                                       {'loss': 0.9865, 'grad_norm': 0.4140625, 'learning_rate': 1.903154471943519e-06, 'entropy': 1.0014203548431397, 'num_tokens': 41367947.0, 'mean_token_accuracy': 0.7887421876192093, 'epoch': 1.73}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1290/2235 [2:20:58<1:31:42,  5.82s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1291/2235 [2:21:03<1:31:25,  5.81s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1292/2235 [2:21:09<1:31:14,  5.81s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1293/2235 [2:21:15<1:31:01,  5.80s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1294/2235 [2:21:21<1:30:49,  5.79s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1295/2235 [2:21:27<1:30:38,  5.79s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1296/2235 [2:21:32<1:30:31,  5.78s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1297/2235 [2:21:38<1:30:20,  5.78s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1298/2235 [2:21:44<1:30:13,  5.78s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1299/2235 [2:21:50<1:30:04,  5.77s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1300/2235 [2:21:55<1:29:58,  5.77s/it]                                                       {'loss': 1.061, 'grad_norm': 0.380859375, 'learning_rate': 1.8690898341458635e-06, 'entropy': 1.0778370648622513, 'num_tokens': 41689066.0, 'mean_token_accuracy': 0.7742142230272293, 'epoch': 1.74}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1300/2235 [2:21:55<1:29:58,  5.77s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1301/2235 [2:22:01<1:29:52,  5.77s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1302/2235 [2:22:07<1:29:48,  5.77s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1303/2235 [2:22:13<1:29:43,  5.78s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1304/2235 [2:22:18<1:29:39,  5.78s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1305/2235 [2:22:24<1:29:34,  5.78s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1306/2235 [2:22:30<1:29:27,  5.78s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1307/2235 [2:22:36<1:29:23,  5.78s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1308/2235 [2:22:42<1:29:09,  5.77s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1309/2235 [2:22:47<1:29:07,  5.77s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1310/2235 [2:22:53<1:29:00,  5.77s/it]                                                       {'loss': 0.977, 'grad_norm': 0.34375, 'learning_rate': 1.8351498500169618e-06, 'entropy': 0.9839703619480134, 'num_tokens': 42008654.0, 'mean_token_accuracy': 0.7900566697120667, 'epoch': 1.76}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1310/2235 [2:22:53<1:29:00,  5.77s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1311/2235 [2:22:59<1:29:00,  5.78s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1312/2235 [2:23:05<1:28:58,  5.78s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1313/2235 [2:23:11<1:28:55,  5.79s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1314/2235 [2:23:16<1:28:53,  5.79s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1315/2235 [2:23:22<1:28:53,  5.80s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1316/2235 [2:23:28<1:28:47,  5.80s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1317/2235 [2:23:34<1:27:49,  5.74s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1318/2235 [2:23:39<1:28:06,  5.76s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1319/2235 [2:23:45<1:28:16,  5.78s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1320/2235 [2:23:51<1:28:26,  5.80s/it]                                                       {'loss': 1.0275, 'grad_norm': 0.3125, 'learning_rate': 1.8013412253348613e-06, 'entropy': 1.0382455050945283, 'num_tokens': 42329118.0, 'mean_token_accuracy': 0.7792588412761688, 'epoch': 1.77}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1320/2235 [2:23:51<1:28:26,  5.80s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1321/2235 [2:23:57<1:28:33,  5.81s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1322/2235 [2:24:03<1:28:37,  5.82s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1323/2235 [2:24:09<1:28:45,  5.84s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1324/2235 [2:24:14<1:28:27,  5.83s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1325/2235 [2:24:20<1:28:22,  5.83s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1326/2235 [2:24:26<1:28:16,  5.83s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1327/2235 [2:24:32<1:28:07,  5.82s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1328/2235 [2:24:38<1:27:57,  5.82s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1329/2235 [2:24:43<1:27:37,  5.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1330/2235 [2:24:49<1:27:23,  5.79s/it]                                                       {'loss': 1.0816, 'grad_norm': 0.400390625, 'learning_rate': 1.7676706399239342e-06, 'entropy': 1.0951369881629944, 'num_tokens': 42650230.0, 'mean_token_accuracy': 0.7686380118131637, 'epoch': 1.79}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1330/2235 [2:24:49<1:27:23,  5.79s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1331/2235 [2:24:55<1:27:21,  5.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1332/2235 [2:25:01<1:27:19,  5.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1333/2235 [2:25:07<1:27:06,  5.79s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1334/2235 [2:25:12<1:27:01,  5.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1335/2235 [2:25:18<1:27:00,  5.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1336/2235 [2:25:24<1:27:01,  5.81s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1337/2235 [2:25:30<1:27:00,  5.81s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1338/2235 [2:25:36<1:27:02,  5.82s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1339/2235 [2:25:42<1:26:54,  5.82s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1340/2235 [2:25:47<1:26:53,  5.82s/it]                                                       {'loss': 1.041, 'grad_norm': 0.38671875, 'learning_rate': 1.7341447463350921e-06, 'entropy': 1.0517963588237762, 'num_tokens': 42971107.0, 'mean_token_accuracy': 0.7773200184106827, 'epoch': 1.8}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1340/2235 [2:25:47<1:26:53,  5.82s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1341/2235 [2:25:53<1:26:54,  5.83s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1342/2235 [2:25:59<1:26:53,  5.84s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1343/2235 [2:26:05<1:26:47,  5.84s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1344/2235 [2:26:11<1:26:43,  5.84s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1345/2235 [2:26:16<1:25:42,  5.78s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1346/2235 [2:26:22<1:25:56,  5.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1347/2235 [2:26:28<1:25:54,  5.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1348/2235 [2:26:34<1:25:50,  5.81s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1349/2235 [2:26:40<1:25:42,  5.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1350/2235 [2:26:45<1:25:33,  5.80s/it]                                                       {'loss': 0.9832, 'grad_norm': 0.37109375, 'learning_rate': 1.7007701685314002e-06, 'entropy': 0.9941470384597778, 'num_tokens': 43290728.0, 'mean_token_accuracy': 0.7899352014064789, 'epoch': 1.81}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1350/2235 [2:26:45<1:25:33,  5.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1351/2235 [2:26:51<1:25:13,  5.78s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1352/2235 [2:26:57<1:25:07,  5.78s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1353/2235 [2:27:03<1:25:00,  5.78s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1354/2235 [2:27:08<1:24:44,  5.77s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1355/2235 [2:27:14<1:24:39,  5.77s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1356/2235 [2:27:20<1:24:23,  5.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1357/2235 [2:27:26<1:24:17,  5.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1358/2235 [2:27:31<1:24:07,  5.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1359/2235 [2:27:37<1:24:03,  5.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1360/2235 [2:27:43<1:23:58,  5.76s/it]                                                       {'loss': 1.0097, 'grad_norm': 0.431640625, 'learning_rate': 1.6675535005793197e-06, 'entropy': 1.0233552873134613, 'num_tokens': 43612703.0, 'mean_token_accuracy': 0.7826198488473892, 'epoch': 1.83}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1360/2235 [2:27:43<1:23:58,  5.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1361/2235 [2:27:49<1:23:53,  5.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1362/2235 [2:27:55<1:23:47,  5.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1363/2235 [2:28:00<1:23:41,  5.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1364/2235 [2:28:06<1:23:30,  5.75s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1365/2235 [2:28:12<1:23:28,  5.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1366/2235 [2:28:18<1:23:25,  5.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1367/2235 [2:28:23<1:23:21,  5.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1368/2235 [2:28:29<1:23:17,  5.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1369/2235 [2:28:35<1:23:13,  5.77s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1370/2235 [2:28:40<1:22:15,  5.71s/it]                                                       {'loss': 1.0305, 'grad_norm': 0.38671875, 'learning_rate': 1.6345013053458789e-06, 'entropy': 1.04014413356781, 'num_tokens': 43933848.0, 'mean_token_accuracy': 0.7794121205806732, 'epoch': 1.84}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1370/2235 [2:28:40<1:22:15,  5.71s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1371/2235 [2:28:46<1:22:27,  5.73s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1372/2235 [2:28:52<1:22:34,  5.74s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1373/2235 [2:28:58<1:22:37,  5.75s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1374/2235 [2:29:04<1:22:29,  5.75s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1375/2235 [2:29:09<1:21:33,  5.69s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1376/2235 [2:29:15<1:21:51,  5.72s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1377/2235 [2:29:21<1:22:04,  5.74s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1378/2235 [2:29:26<1:22:04,  5.75s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1379/2235 [2:29:32<1:22:10,  5.76s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1380/2235 [2:29:38<1:22:15,  5.77s/it]                                                       {'loss': 1.0456, 'grad_norm': 0.431640625, 'learning_rate': 1.601620113201991e-06, 'entropy': 1.0578260600566864, 'num_tokens': 44255202.0, 'mean_token_accuracy': 0.7762727200984955, 'epoch': 1.85}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1380/2235 [2:29:38<1:22:15,  5.77s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1381/2235 [2:29:44<1:22:22,  5.79s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1382/2235 [2:29:50<1:22:24,  5.80s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1383/2235 [2:29:55<1:22:29,  5.81s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1384/2235 [2:30:01<1:22:32,  5.82s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1385/2235 [2:30:07<1:22:40,  5.84s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1386/2235 [2:30:13<1:22:40,  5.84s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1387/2235 [2:30:19<1:22:30,  5.84s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1388/2235 [2:30:25<1:22:11,  5.82s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1389/2235 [2:30:30<1:21:59,  5.81s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1390/2235 [2:30:36<1:21:47,  5.81s/it]                                                       {'loss': 1.0437, 'grad_norm': 0.4296875, 'learning_rate': 1.5689164207322009e-06, 'entropy': 1.0538865238428117, 'num_tokens': 44576678.0, 'mean_token_accuracy': 0.7781134098768234, 'epoch': 1.87}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1390/2235 [2:30:36<1:21:47,  5.81s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1391/2235 [2:30:42<1:21:35,  5.80s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1392/2235 [2:30:48<1:21:27,  5.80s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1393/2235 [2:30:54<1:21:18,  5.79s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1394/2235 [2:30:59<1:21:10,  5.79s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1395/2235 [2:31:05<1:21:04,  5.79s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1396/2235 [2:31:11<1:20:57,  5.79s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1397/2235 [2:31:17<1:20:46,  5.78s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1398/2235 [2:31:23<1:20:42,  5.79s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1399/2235 [2:31:28<1:20:34,  5.78s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1400/2235 [2:31:34<1:20:30,  5.78s/it]                                                       {'loss': 1.0378, 'grad_norm': 0.4140625, 'learning_rate': 1.5363966894511109e-06, 'entropy': 1.054898425936699, 'num_tokens': 44897139.0, 'mean_token_accuracy': 0.7767684161663055, 'epoch': 1.88}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1400/2235 [2:31:34<1:20:30,  5.78s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1401/2235 [2:31:40<1:20:26,  5.79s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1402/2235 [2:31:46<1:20:22,  5.79s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1403/2235 [2:31:51<1:20:22,  5.80s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1404/2235 [2:31:57<1:20:20,  5.80s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1405/2235 [2:32:03<1:20:14,  5.80s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1406/2235 [2:32:09<1:20:17,  5.81s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1407/2235 [2:32:15<1:20:12,  5.81s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1408/2235 [2:32:21<1:20:08,  5.81s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1409/2235 [2:32:26<1:20:13,  5.83s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1410/2235 [2:32:32<1:20:11,  5.83s/it]                                                       {'loss': 1.0826, 'grad_norm': 0.427734375, 'learning_rate': 1.5040673445267252e-06, 'entropy': 1.0947205752134324, 'num_tokens': 45218857.0, 'mean_token_accuracy': 0.7679863423109055, 'epoch': 1.89}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1410/2235 [2:32:32<1:20:11,  5.83s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1411/2235 [2:32:38<1:20:04,  5.83s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1412/2235 [2:32:44<1:19:55,  5.83s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1413/2235 [2:32:50<1:19:40,  5.82s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1414/2235 [2:32:55<1:19:26,  5.81s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1415/2235 [2:33:01<1:19:10,  5.79s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1416/2235 [2:33:07<1:19:01,  5.79s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1417/2235 [2:33:13<1:18:51,  5.78s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1418/2235 [2:33:19<1:18:44,  5.78s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1419/2235 [2:33:24<1:18:43,  5.79s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1420/2235 [2:33:30<1:18:42,  5.79s/it]                                                       {'loss': 1.0366, 'grad_norm': 0.337890625, 'learning_rate': 1.4719347735109808e-06, 'entropy': 1.0468161642551421, 'num_tokens': 45540867.0, 'mean_token_accuracy': 0.7782894790172576, 'epoch': 1.91}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1420/2235 [2:33:30<1:18:42,  5.79s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1421/2235 [2:33:36<1:17:49,  5.74s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1422/2235 [2:33:42<1:18:03,  5.76s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1423/2235 [2:33:47<1:17:23,  5.72s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1424/2235 [2:33:53<1:17:46,  5.75s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1425/2235 [2:33:59<1:18:02,  5.78s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1426/2235 [2:34:05<1:18:00,  5.79s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1427/2235 [2:34:11<1:18:08,  5.80s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1428/2235 [2:34:16<1:18:01,  5.80s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1429/2235 [2:34:22<1:18:04,  5.81s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1430/2235 [2:34:28<1:18:05,  5.82s/it]                                                       {'loss': 1.0479, 'grad_norm': 0.427734375, 'learning_rate': 1.4400053250777177e-06, 'entropy': 1.056394949555397, 'num_tokens': 45862335.0, 'mean_token_accuracy': 0.7750393718481063, 'epoch': 1.92}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1430/2235 [2:34:28<1:18:05,  5.82s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1431/2235 [2:34:34<1:18:06,  5.83s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1432/2235 [2:34:40<1:17:56,  5.82s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1433/2235 [2:34:46<1:17:56,  5.83s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1434/2235 [2:34:51<1:17:53,  5.83s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1435/2235 [2:34:57<1:17:42,  5.83s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1436/2235 [2:35:03<1:17:39,  5.83s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1437/2235 [2:35:09<1:17:13,  5.81s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1438/2235 [2:35:15<1:17:04,  5.80s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1439/2235 [2:35:20<1:16:54,  5.80s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1440/2235 [2:35:26<1:15:57,  5.73s/it]                                                       {'loss': 1.0128, 'grad_norm': 0.35546875, 'learning_rate': 1.408285307768314e-06, 'entropy': 1.025933027267456, 'num_tokens': 46183308.0, 'mean_token_accuracy': 0.7822956323623658, 'epoch': 1.93}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1440/2235 [2:35:26<1:15:57,  5.73s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1441/2235 [2:35:32<1:15:59,  5.74s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1442/2235 [2:35:37<1:15:59,  5.75s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1443/2235 [2:35:43<1:15:51,  5.75s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1444/2235 [2:35:49<1:15:52,  5.75s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1445/2235 [2:35:55<1:15:01,  5.70s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1446/2235 [2:36:00<1:15:14,  5.72s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1447/2235 [2:36:06<1:15:21,  5.74s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1448/2235 [2:36:12<1:15:24,  5.75s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1449/2235 [2:36:18<1:15:20,  5.75s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1450/2235 [2:36:23<1:15:16,  5.75s/it]                                                       {'loss': 1.0249, 'grad_norm': 0.412109375, 'learning_rate': 1.3767809887452682e-06, 'entropy': 1.0388693809509277, 'num_tokens': 46504061.0, 'mean_token_accuracy': 0.7802269607782364, 'epoch': 1.95}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1450/2235 [2:36:23<1:15:16,  5.75s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1451/2235 [2:36:29<1:15:17,  5.76s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1452/2235 [2:36:35<1:15:16,  5.77s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1453/2235 [2:36:41<1:15:12,  5.77s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1454/2235 [2:36:47<1:15:07,  5.77s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1455/2235 [2:36:52<1:14:59,  5.77s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1456/2235 [2:36:58<1:14:59,  5.78s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1457/2235 [2:37:04<1:14:59,  5.78s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1458/2235 [2:37:10<1:14:59,  5.79s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1459/2235 [2:37:16<1:15:01,  5.80s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1460/2235 [2:37:21<1:15:03,  5.81s/it]                                                       {'loss': 1.049, 'grad_norm': 0.32421875, 'learning_rate': 1.3454985925539444e-06, 'entropy': 1.0607287913560868, 'num_tokens': 46826532.0, 'mean_token_accuracy': 0.7750655293464661, 'epoch': 1.96}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1460/2235 [2:37:21<1:15:03,  5.81s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1461/2235 [2:37:27<1:15:08,  5.82s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1462/2235 [2:37:33<1:15:10,  5.83s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1463/2235 [2:37:39<1:15:06,  5.84s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1464/2235 [2:37:45<1:14:53,  5.83s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1465/2235 [2:37:50<1:14:39,  5.82s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1466/2235 [2:37:56<1:14:31,  5.81s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1467/2235 [2:38:02<1:14:20,  5.81s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1468/2235 [2:38:08<1:14:12,  5.80s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1469/2235 [2:38:14<1:14:03,  5.80s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1470/2235 [2:38:19<1:13:56,  5.80s/it]                                                       {'loss': 1.0953, 'grad_norm': 0.39453125, 'learning_rate': 1.3144442998927403e-06, 'entropy': 1.113349986076355, 'num_tokens': 47148135.0, 'mean_token_accuracy': 0.766430988907814, 'epoch': 1.97}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1470/2235 [2:38:19<1:13:56,  5.80s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1471/2235 [2:38:25<1:13:50,  5.80s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1472/2235 [2:38:31<1:13:45,  5.80s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1473/2235 [2:38:37<1:13:40,  5.80s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1474/2235 [2:38:43<1:13:36,  5.80s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1475/2235 [2:38:49<1:13:34,  5.81s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1476/2235 [2:38:54<1:13:30,  5.81s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1477/2235 [2:39:00<1:13:29,  5.82s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1478/2235 [2:39:06<1:13:22,  5.82s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1479/2235 [2:39:12<1:12:38,  5.76s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1480/2235 [2:39:17<1:12:55,  5.80s/it]                                                       {'loss': 1.002, 'grad_norm': 0.419921875, 'learning_rate': 1.2836242463919284e-06, 'entropy': 1.0167398750782013, 'num_tokens': 47467458.0, 'mean_token_accuracy': 0.7848323911428452, 'epoch': 1.99}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1480/2235 [2:39:17<1:12:55,  5.80s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1481/2235 [2:39:23<1:12:54,  5.80s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1482/2235 [2:39:29<1:12:53,  5.81s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1483/2235 [2:39:35<1:12:56,  5.82s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1484/2235 [2:39:41<1:12:52,  5.82s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1485/2235 [2:39:47<1:12:51,  5.83s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1486/2235 [2:39:52<1:12:32,  5.81s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1487/2235 [2:39:58<1:12:37,  5.83s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1488/2235 [2:40:04<1:12:38,  5.83s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1489/2235 [2:40:10<1:12:41,  5.85s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1490/2235 [2:40:16<1:12:58,  5.88s/it]                                                       {'loss': 1.0898, 'grad_norm': 0.35546875, 'learning_rate': 1.2530445214013817e-06, 'entropy': 1.1029660761356355, 'num_tokens': 47788684.0, 'mean_token_accuracy': 0.7673848509788513, 'epoch': 2.0}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1490/2235 [2:40:16<1:12:58,  5.88s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1491/2235 [2:40:22<1:14:23,  6.00s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1492/2235 [2:40:28<1:13:46,  5.96s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

  0%|          | 0/187 [00:00<?, ?it/s][A
  1%|          | 2/187 [00:01<02:42,  1.14it/s][A
  2%|â–         | 3/187 [00:03<03:49,  1.25s/it][A
  2%|â–         | 4/187 [00:05<04:23,  1.44s/it][A
  3%|â–Ž         | 5/187 [00:07<04:42,  1.55s/it][A
  3%|â–Ž         | 6/187 [00:08<04:53,  1.62s/it][A
  4%|â–Ž         | 7/187 [00:10<04:59,  1.67s/it][A
  4%|â–         | 8/187 [00:12<05:03,  1.70s/it][A
  5%|â–         | 9/187 [00:14<05:05,  1.72s/it][A
  5%|â–Œ         | 10/187 [00:15<05:06,  1.73s/it][A
  6%|â–Œ         | 11/187 [00:17<05:06,  1.74s/it][A
  6%|â–‹         | 12/187 [00:19<05:05,  1.75s/it][A
  7%|â–‹         | 13/187 [00:21<05:04,  1.75s/it][A
  7%|â–‹         | 14/187 [00:22<05:03,  1.75s/it][A
  8%|â–Š         | 15/187 [00:24<05:02,  1.76s/it][A
  9%|â–Š         | 16/187 [00:26<05:00,  1.76s/it][A
  9%|â–‰         | 17/187 [00:28<04:58,  1.76s/it][A
 10%|â–‰         | 18/187 [00:29<04:56,  1.76s/it][A
 10%|â–ˆ         | 19/187 [00:31<04:55,  1.76s/it][A
 11%|â–ˆ         | 20/187 [00:33<04:53,  1.76s/it][A
 11%|â–ˆ         | 21/187 [00:35<04:51,  1.76s/it][A
 12%|â–ˆâ–        | 22/187 [00:36<04:50,  1.76s/it][A
 12%|â–ˆâ–        | 23/187 [00:38<04:48,  1.76s/it][A
 13%|â–ˆâ–Ž        | 24/187 [00:40<04:46,  1.76s/it][A
 13%|â–ˆâ–Ž        | 25/187 [00:42<04:44,  1.76s/it][A
 14%|â–ˆâ–        | 26/187 [00:43<04:42,  1.76s/it][A
 14%|â–ˆâ–        | 27/187 [00:45<04:41,  1.76s/it][A
 15%|â–ˆâ–        | 28/187 [00:47<04:39,  1.76s/it][A
 16%|â–ˆâ–Œ        | 29/187 [00:49<04:37,  1.76s/it][A
 16%|â–ˆâ–Œ        | 30/187 [00:51<04:35,  1.76s/it][A
 17%|â–ˆâ–‹        | 31/187 [00:52<04:34,  1.76s/it][A
 17%|â–ˆâ–‹        | 32/187 [00:54<04:32,  1.76s/it][A
 18%|â–ˆâ–Š        | 33/187 [00:56<04:30,  1.76s/it][A
 18%|â–ˆâ–Š        | 34/187 [00:58<04:29,  1.76s/it][A
 19%|â–ˆâ–Š        | 35/187 [00:59<04:27,  1.76s/it][A
 19%|â–ˆâ–‰        | 36/187 [01:01<04:25,  1.76s/it][A
 20%|â–ˆâ–‰        | 37/187 [01:03<04:24,  1.76s/it][A
 20%|â–ˆâ–ˆ        | 38/187 [01:05<04:22,  1.76s/it][A
 21%|â–ˆâ–ˆ        | 39/187 [01:06<04:20,  1.76s/it][A
 21%|â–ˆâ–ˆâ–       | 40/187 [01:08<04:19,  1.76s/it][A
 22%|â–ˆâ–ˆâ–       | 41/187 [01:10<04:17,  1.76s/it][A
 22%|â–ˆâ–ˆâ–       | 42/187 [01:12<04:15,  1.76s/it][A
 23%|â–ˆâ–ˆâ–Ž       | 43/187 [01:13<04:13,  1.76s/it][A
 24%|â–ˆâ–ˆâ–Ž       | 44/187 [01:15<04:11,  1.76s/it][A
 24%|â–ˆâ–ˆâ–       | 45/187 [01:17<04:09,  1.76s/it][A
 25%|â–ˆâ–ˆâ–       | 46/187 [01:19<04:07,  1.76s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 47/187 [01:20<04:05,  1.76s/it][A
 26%|â–ˆâ–ˆâ–Œ       | 48/187 [01:22<04:04,  1.76s/it][A
 26%|â–ˆâ–ˆâ–Œ       | 49/187 [01:24<04:02,  1.75s/it][A
 27%|â–ˆâ–ˆâ–‹       | 50/187 [01:26<04:00,  1.75s/it][A
 27%|â–ˆâ–ˆâ–‹       | 51/187 [01:27<03:58,  1.75s/it][A
 28%|â–ˆâ–ˆâ–Š       | 52/187 [01:29<03:56,  1.75s/it][A
 28%|â–ˆâ–ˆâ–Š       | 53/187 [01:31<03:54,  1.75s/it][A
 29%|â–ˆâ–ˆâ–‰       | 54/187 [01:33<03:52,  1.75s/it][A
 29%|â–ˆâ–ˆâ–‰       | 55/187 [01:34<03:50,  1.75s/it][A
 30%|â–ˆâ–ˆâ–‰       | 56/187 [01:36<03:48,  1.75s/it][A
 30%|â–ˆâ–ˆâ–ˆ       | 57/187 [01:38<03:46,  1.75s/it][A
 31%|â–ˆâ–ˆâ–ˆ       | 58/187 [01:40<03:45,  1.75s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 59/187 [01:41<03:43,  1.74s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 60/187 [01:43<03:41,  1.74s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 61/187 [01:45<03:39,  1.74s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 62/187 [01:47<03:37,  1.74s/it][A
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 63/187 [01:48<03:35,  1.74s/it][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 64/187 [01:50<03:34,  1.74s/it][A
 35%|â–ˆâ–ˆâ–ˆâ–      | 65/187 [01:52<03:32,  1.74s/it][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 66/187 [01:54<03:30,  1.74s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 67/187 [01:55<03:28,  1.74s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 68/187 [01:57<03:27,  1.74s/it][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 69/187 [01:59<03:25,  1.74s/it][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/187 [02:01<03:23,  1.74s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 71/187 [02:02<03:21,  1.74s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 72/187 [02:04<03:20,  1.74s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 73/187 [02:06<03:18,  1.74s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 74/187 [02:08<03:16,  1.74s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 75/187 [02:09<03:14,  1.74s/it][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76/187 [02:11<03:12,  1.74s/it][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77/187 [02:13<03:11,  1.74s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/187 [02:14<03:09,  1.74s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/187 [02:16<03:07,  1.74s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 80/187 [02:18<03:06,  1.74s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 81/187 [02:20<03:04,  1.74s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/187 [02:21<03:02,  1.74s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/187 [02:23<03:00,  1.74s/it][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/187 [02:25<02:59,  1.74s/it][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 85/187 [02:27<02:57,  1.74s/it][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86/187 [02:28<02:55,  1.74s/it][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 87/187 [02:30<02:54,  1.74s/it][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88/187 [02:32<02:52,  1.74s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 89/187 [02:34<02:50,  1.74s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/187 [02:35<02:48,  1.74s/it][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91/187 [02:37<02:46,  1.74s/it][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 92/187 [02:39<02:45,  1.74s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 93/187 [02:41<02:43,  1.74s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 94/187 [02:42<02:41,  1.74s/it][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 95/187 [02:44<02:40,  1.74s/it][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 96/187 [02:46<02:38,  1.74s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/187 [02:48<02:36,  1.74s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/187 [02:49<02:34,  1.74s/it][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 99/187 [02:51<02:33,  1.74s/it][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 100/187 [02:53<02:31,  1.74s/it][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 101/187 [02:54<02:29,  1.74s/it][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/187 [02:56<02:28,  1.74s/it][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 103/187 [02:58<02:26,  1.74s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 104/187 [03:00<02:24,  1.75s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105/187 [03:01<02:23,  1.75s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 106/187 [03:03<02:21,  1.75s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 107/187 [03:05<02:19,  1.75s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 108/187 [03:07<02:18,  1.75s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 109/187 [03:08<02:16,  1.75s/it][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 110/187 [03:10<02:14,  1.75s/it][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 111/187 [03:12<02:13,  1.75s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112/187 [03:14<02:11,  1.75s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 113/187 [03:16<02:09,  1.76s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114/187 [03:17<02:08,  1.76s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 115/187 [03:19<02:06,  1.76s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/187 [03:21<02:05,  1.76s/it][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 117/187 [03:23<02:03,  1.76s/it][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 118/187 [03:24<02:01,  1.76s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 119/187 [03:26<01:59,  1.76s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120/187 [03:28<01:57,  1.76s/it][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121/187 [03:30<01:55,  1.75s/it][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 122/187 [03:31<01:53,  1.75s/it][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 123/187 [03:33<01:51,  1.75s/it][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 124/187 [03:35<01:49,  1.75s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 125/187 [03:37<01:48,  1.74s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126/187 [03:38<01:46,  1.74s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 127/187 [03:40<01:44,  1.74s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 128/187 [03:42<01:42,  1.74s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 129/187 [03:43<01:40,  1.74s/it][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/187 [03:45<01:38,  1.74s/it][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 131/187 [03:47<01:37,  1.74s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 132/187 [03:49<01:35,  1.74s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133/187 [03:50<01:33,  1.73s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 134/187 [03:52<01:31,  1.73s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/187 [03:54<01:30,  1.73s/it][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 136/187 [03:56<01:28,  1.73s/it][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 137/187 [03:57<01:26,  1.73s/it][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138/187 [03:59<01:24,  1.73s/it][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 139/187 [04:01<01:23,  1.73s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/187 [04:03<01:21,  1.73s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/187 [04:04<01:19,  1.73s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 142/187 [04:06<01:17,  1.73s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 143/187 [04:08<01:16,  1.73s/it][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 144/187 [04:09<01:14,  1.73s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 145/187 [04:11<01:12,  1.73s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 146/187 [04:13<01:10,  1.73s/it][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 147/187 [04:15<01:09,  1.73s/it][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 148/187 [04:16<01:07,  1.73s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 149/187 [04:18<01:05,  1.73s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 150/187 [04:20<01:03,  1.73s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 151/187 [04:22<01:02,  1.73s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 152/187 [04:23<01:00,  1.73s/it][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 153/187 [04:25<00:58,  1.73s/it][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 154/187 [04:27<00:56,  1.72s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 155/187 [04:28<00:55,  1.72s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 156/187 [04:30<00:53,  1.72s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 157/187 [04:32<00:51,  1.72s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 158/187 [04:34<00:49,  1.72s/it][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 159/187 [04:35<00:48,  1.72s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 160/187 [04:37<00:46,  1.72s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 161/187 [04:39<00:44,  1.72s/it][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 162/187 [04:40<00:43,  1.72s/it][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 163/187 [04:42<00:41,  1.72s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 164/187 [04:44<00:39,  1.72s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 165/187 [04:46<00:37,  1.72s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 166/187 [04:47<00:36,  1.72s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 167/187 [04:49<00:34,  1.72s/it][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 168/187 [04:51<00:32,  1.72s/it][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 169/187 [04:53<00:31,  1.73s/it][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 170/187 [04:54<00:29,  1.73s/it][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 171/187 [04:56<00:27,  1.72s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 172/187 [04:58<00:25,  1.73s/it][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 173/187 [04:59<00:24,  1.72s/it][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 174/187 [05:01<00:22,  1.72s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 175/187 [05:03<00:20,  1.72s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 176/187 [05:05<00:18,  1.72s/it][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 177/187 [05:06<00:17,  1.72s/it][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 178/187 [05:08<00:15,  1.72s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 179/187 [05:10<00:13,  1.72s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 180/187 [05:12<00:12,  1.73s/it][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 181/187 [05:13<00:10,  1.73s/it][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 182/187 [05:15<00:08,  1.73s/it][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 183/187 [05:17<00:06,  1.73s/it][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 184/187 [05:18<00:05,  1.73s/it][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 185/187 [05:20<00:03,  1.73s/it][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 186/187 [05:22<00:01,  1.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [05:24<00:00,  1.75s/it][A                                                       
                                                 [A{'eval_loss': 1.0410678386688232, 'eval_runtime': 326.3832, 'eval_samples_per_second': 18.261, 'eval_steps_per_second': 0.573, 'eval_entropy': 1.054865112916671, 'eval_num_tokens': 47852610.0, 'eval_mean_token_accuracy': 0.7768339514732361, 'epoch': 2.0}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1492/2235 [2:45:54<1:13:46,  5.96s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [05:24<00:00,  1.75s/it][A
                                                 [A/home/ubuntu/Part-II-Project/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1493/2235 [2:46:01<21:26:30, 104.03s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1494/2235 [2:46:07<15:20:44, 74.55s/it]  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1495/2235 [2:46:13<11:05:00, 53.92s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1496/2235 [2:46:18<8:05:26, 39.41s/it]  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1497/2235 [2:46:24<5:59:55, 29.26s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1498/2235 [2:46:29<4:32:49, 22.21s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1499/2235 [2:46:35<3:31:59, 17.28s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1500/2235 [2:46:41<2:49:27, 13.83s/it]                                                       {'loss': 1.0381, 'grad_norm': 0.54296875, 'learning_rate': 1.2227111667874664e-06, 'entropy': 1.045997565984726, 'num_tokens': 48108994.0, 'mean_token_accuracy': 0.7774801641702652, 'epoch': 2.01}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1500/2235 [2:46:41<2:49:27, 13.83s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1501/2235 [2:46:47<2:19:41, 11.42s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1502/2235 [2:46:53<1:58:52,  9.73s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1503/2235 [2:46:58<1:44:07,  8.53s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1504/2235 [2:47:04<1:33:53,  7.71s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1505/2235 [2:47:10<1:26:53,  7.14s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1506/2235 [2:47:16<1:21:51,  6.74s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1507/2235 [2:47:22<1:18:28,  6.47s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1508/2235 [2:47:27<1:16:07,  6.28s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1509/2235 [2:47:33<1:14:27,  6.15s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1510/2235 [2:47:39<1:13:12,  6.06s/it]                                                       {'loss': 1.0526, 'grad_norm': 0.3671875, 'learning_rate': 1.1926301757392956e-06, 'entropy': 1.06610207259655, 'num_tokens': 48430173.0, 'mean_token_accuracy': 0.7756948798894883, 'epoch': 2.03}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1510/2235 [2:47:39<1:13:12,  6.06s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1511/2235 [2:47:45<1:12:07,  5.98s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1512/2235 [2:47:51<1:11:27,  5.93s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1513/2235 [2:47:57<1:11:01,  5.90s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1514/2235 [2:48:02<1:10:41,  5.88s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1515/2235 [2:48:08<1:10:28,  5.87s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1516/2235 [2:48:14<1:10:11,  5.86s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1517/2235 [2:48:20<1:09:58,  5.85s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1518/2235 [2:48:26<1:09:43,  5.83s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1519/2235 [2:48:31<1:09:29,  5.82s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1520/2235 [2:48:37<1:09:14,  5.81s/it]                                                       {'loss': 1.0479, 'grad_norm': 0.396484375, 'learning_rate': 1.1628074915846146e-06, 'entropy': 1.058207732439041, 'num_tokens': 48750621.0, 'mean_token_accuracy': 0.7737665265798569, 'epoch': 2.04}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1520/2235 [2:48:37<1:09:14,  5.81s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1521/2235 [2:48:43<1:09:01,  5.80s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1522/2235 [2:48:49<1:08:47,  5.79s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1523/2235 [2:48:55<1:08:41,  5.79s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1524/2235 [2:49:00<1:08:35,  5.79s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1525/2235 [2:49:06<1:09:31,  5.88s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1526/2235 [2:49:12<1:09:04,  5.85s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1527/2235 [2:49:18<1:08:34,  5.81s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1528/2235 [2:49:24<1:08:21,  5.80s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1529/2235 [2:49:29<1:08:06,  5.79s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1530/2235 [2:49:35<1:07:57,  5.78s/it]                                                       {'loss': 1.0564, 'grad_norm': 0.345703125, 'learning_rate': 1.1332490066155339e-06, 'entropy': 1.065092533826828, 'num_tokens': 49070771.0, 'mean_token_accuracy': 0.7743499994277954, 'epoch': 2.05}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1530/2235 [2:49:35<1:07:57,  5.78s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1531/2235 [2:49:41<1:07:50,  5.78s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1532/2235 [2:49:47<1:07:44,  5.78s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1533/2235 [2:49:53<1:07:27,  5.77s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1534/2235 [2:49:58<1:07:23,  5.77s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1535/2235 [2:50:04<1:07:20,  5.77s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1536/2235 [2:50:10<1:07:16,  5.78s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1537/2235 [2:50:16<1:07:14,  5.78s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1538/2235 [2:50:21<1:07:12,  5.79s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1539/2235 [2:50:27<1:07:12,  5.79s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1540/2235 [2:50:33<1:07:12,  5.80s/it]                                                       {'loss': 1.0822, 'grad_norm': 0.44921875, 'learning_rate': 1.1039605609243423e-06, 'entropy': 1.0918543666601181, 'num_tokens': 49390392.0, 'mean_token_accuracy': 0.769141086935997, 'epoch': 2.07}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1540/2235 [2:50:33<1:07:12,  5.80s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1541/2235 [2:50:39<1:07:14,  5.81s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1542/2235 [2:50:45<1:07:15,  5.82s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1543/2235 [2:50:51<1:07:15,  5.83s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1544/2235 [2:50:56<1:07:14,  5.84s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1545/2235 [2:51:02<1:07:05,  5.83s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1546/2235 [2:51:08<1:06:48,  5.82s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1547/2235 [2:51:14<1:06:35,  5.81s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1548/2235 [2:51:20<1:06:10,  5.78s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1549/2235 [2:51:25<1:05:59,  5.77s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1550/2235 [2:51:31<1:05:48,  5.76s/it]                                                       {'loss': 1.0596, 'grad_norm': 0.431640625, 'learning_rate': 1.0749479412496346e-06, 'entropy': 1.0724277049303055, 'num_tokens': 49710915.0, 'mean_token_accuracy': 0.7729526311159134, 'epoch': 2.08}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1550/2235 [2:51:31<1:05:48,  5.76s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1551/2235 [2:51:37<1:05:29,  5.75s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1552/2235 [2:51:43<1:05:23,  5.74s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1553/2235 [2:51:48<1:05:17,  5.74s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1554/2235 [2:51:54<1:04:28,  5.68s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1555/2235 [2:52:00<1:04:34,  5.70s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1556/2235 [2:52:05<1:04:32,  5.70s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1557/2235 [2:52:11<1:04:31,  5.71s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1558/2235 [2:52:17<1:04:31,  5.72s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1559/2235 [2:52:22<1:04:24,  5.72s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1560/2235 [2:52:28<1:04:19,  5.72s/it]                                                       {'loss': 1.0067, 'grad_norm': 0.4296875, 'learning_rate': 1.0462168798329874e-06, 'entropy': 1.0213182479143144, 'num_tokens': 50032598.0, 'mean_token_accuracy': 0.785340639948845, 'epoch': 2.09}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1560/2235 [2:52:28<1:04:19,  5.72s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1561/2235 [2:52:34<1:04:13,  5.72s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1562/2235 [2:52:40<1:04:10,  5.72s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1563/2235 [2:52:45<1:04:07,  5.73s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1564/2235 [2:52:51<1:04:04,  5.73s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1565/2235 [2:52:57<1:03:59,  5.73s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1566/2235 [2:53:03<1:03:51,  5.73s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1567/2235 [2:53:08<1:03:06,  5.67s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1568/2235 [2:53:14<1:03:15,  5.69s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1569/2235 [2:53:20<1:03:19,  5.70s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1570/2235 [2:53:25<1:03:20,  5.72s/it]                                                       {'loss': 1.0438, 'grad_norm': 0.337890625, 'learning_rate': 1.0177730532863899e-06, 'entropy': 1.056533795595169, 'num_tokens': 50353175.0, 'mean_token_accuracy': 0.7754694670438766, 'epoch': 2.11}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1570/2235 [2:53:25<1:03:20,  5.72s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1571/2235 [2:53:31<1:03:20,  5.72s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1572/2235 [2:53:37<1:04:23,  5.83s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1573/2235 [2:53:43<1:04:00,  5.80s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1574/2235 [2:53:49<1:03:41,  5.78s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1575/2235 [2:53:54<1:03:28,  5.77s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1576/2235 [2:54:00<1:03:18,  5.76s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1577/2235 [2:54:06<1:03:08,  5.76s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1578/2235 [2:54:12<1:02:57,  5.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1579/2235 [2:54:17<1:02:51,  5.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1580/2235 [2:54:23<1:02:46,  5.75s/it]                                                       {'loss': 0.9291, 'grad_norm': 0.431640625, 'learning_rate': 9.896220814706803e-07, 'entropy': 0.9402393937110901, 'num_tokens': 50673450.0, 'mean_token_accuracy': 0.8010907620191574, 'epoch': 2.12}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1580/2235 [2:54:23<1:02:46,  5.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1581/2235 [2:54:29<1:02:39,  5.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1582/2235 [2:54:35<1:02:34,  5.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1583/2235 [2:54:40<1:02:28,  5.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1584/2235 [2:54:46<1:02:24,  5.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1585/2235 [2:54:52<1:02:19,  5.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1586/2235 [2:54:58<1:02:15,  5.76s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1587/2235 [2:55:03<1:02:11,  5.76s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1588/2235 [2:55:09<1:02:06,  5.76s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1589/2235 [2:55:15<1:01:58,  5.76s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1590/2235 [2:55:21<1:01:46,  5.75s/it]                                                       {'loss': 1.0059, 'grad_norm': 0.34375, 'learning_rate': 9.617695263851809e-07, 'entropy': 1.015205329656601, 'num_tokens': 50993281.0, 'mean_token_accuracy': 0.7844317048788071, 'epoch': 2.13}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1590/2235 [2:55:21<1:01:46,  5.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1591/2235 [2:55:26<1:01:46,  5.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1592/2235 [2:55:32<1:01:44,  5.76s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1593/2235 [2:55:38<1:01:41,  5.77s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1594/2235 [2:55:44<1:01:26,  5.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1595/2235 [2:55:49<1:01:25,  5.76s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1596/2235 [2:55:55<1:01:16,  5.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1597/2235 [2:56:01<1:01:17,  5.76s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1598/2235 [2:56:07<1:01:16,  5.77s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1599/2235 [2:56:13<1:01:17,  5.78s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1600/2235 [2:56:18<1:01:17,  5.79s/it]                                                       {'loss': 1.0257, 'grad_norm': 0.373046875, 'learning_rate': 9.342208910687781e-07, 'entropy': 1.0336619555950164, 'num_tokens': 51314458.0, 'mean_token_accuracy': 0.7796560257673264, 'epoch': 2.15}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1600/2235 [2:56:18<1:01:17,  5.79s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1601/2235 [2:56:24<1:01:18,  5.80s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1602/2235 [2:56:30<1:01:18,  5.81s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1603/2235 [2:56:36<1:01:17,  5.82s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1604/2235 [2:56:42<1:01:04,  5.81s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1605/2235 [2:56:47<1:01:11,  5.83s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1606/2235 [2:56:53<1:01:12,  5.84s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1607/2235 [2:56:59<1:01:04,  5.84s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1608/2235 [2:57:05<1:00:51,  5.82s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1609/2235 [2:57:11<1:00:27,  5.79s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1610/2235 [2:57:16<1:00:14,  5.78s/it]                                                       {'loss': 1.002, 'grad_norm': 0.40234375, 'learning_rate': 9.069816185126404e-07, 'entropy': 1.0145479589700699, 'num_tokens': 51636042.0, 'mean_token_accuracy': 0.7855410993099212, 'epoch': 2.16}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1610/2235 [2:57:16<1:00:14,  5.78s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1611/2235 [2:57:22<1:00:07,  5.78s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1612/2235 [2:57:28<1:00:01,  5.78s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1613/2235 [2:57:34<59:54,  5.78s/it]   72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1614/2235 [2:57:40<59:48,  5.78s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1615/2235 [2:57:45<59:41,  5.78s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1616/2235 [2:57:51<59:34,  5.77s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1617/2235 [2:57:57<59:23,  5.77s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1618/2235 [2:58:03<59:18,  5.77s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1619/2235 [2:58:08<59:13,  5.77s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1620/2235 [2:58:14<59:08,  5.77s/it]                                                     {'loss': 1.0326, 'grad_norm': 0.388671875, 'learning_rate': 8.80057090584808e-07, 'entropy': 1.0449515014886857, 'num_tokens': 51956828.0, 'mean_token_accuracy': 0.7801831364631653, 'epoch': 2.17}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1620/2235 [2:58:14<59:08,  5.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1621/2235 [2:58:20<59:03,  5.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1622/2235 [2:58:26<58:58,  5.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1623/2235 [2:58:31<58:50,  5.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1624/2235 [2:58:37<58:45,  5.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1625/2235 [2:58:43<58:40,  5.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1626/2235 [2:58:49<58:35,  5.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1627/2235 [2:58:55<58:30,  5.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1628/2235 [2:59:00<58:26,  5.78s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1629/2235 [2:59:06<58:17,  5.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1630/2235 [2:59:12<58:09,  5.77s/it]                                                     {'loss': 1.0677, 'grad_norm': 0.361328125, 'learning_rate': 8.534526269668592e-07, 'entropy': 1.0780100733041764, 'num_tokens': 52276913.0, 'mean_token_accuracy': 0.7705326110124588, 'epoch': 2.19}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1630/2235 [2:59:12<58:09,  5.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1631/2235 [2:59:18<58:07,  5.77s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1632/2235 [2:59:23<58:04,  5.78s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1633/2235 [2:59:29<58:00,  5.78s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1634/2235 [2:59:35<57:54,  5.78s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1635/2235 [2:59:41<57:47,  5.78s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1636/2235 [2:59:47<57:46,  5.79s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1637/2235 [2:59:52<57:46,  5.80s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1638/2235 [2:59:58<57:45,  5.81s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1639/2235 [3:00:04<57:48,  5.82s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1640/2235 [3:00:10<57:49,  5.83s/it]                                                     {'loss': 1.0492, 'grad_norm': 0.365234375, 'learning_rate': 8.271734841028553e-07, 'entropy': 1.0636173993349076, 'num_tokens': 52597415.0, 'mean_token_accuracy': 0.7764111667871475, 'epoch': 2.2}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1640/2235 [3:00:10<57:49,  5.83s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1641/2235 [3:00:16<57:46,  5.84s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1642/2235 [3:00:22<57:33,  5.82s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1643/2235 [3:00:27<57:25,  5.82s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1644/2235 [3:00:33<57:17,  5.82s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1645/2235 [3:00:39<57:09,  5.81s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1646/2235 [3:00:45<56:58,  5.80s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1647/2235 [3:00:51<56:46,  5.79s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1648/2235 [3:00:56<56:42,  5.80s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1649/2235 [3:01:02<56:28,  5.78s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1650/2235 [3:01:08<56:25,  5.79s/it]                                                     {'loss': 0.9585, 'grad_norm': 0.32421875, 'learning_rate': 8.01224854160789e-07, 'entropy': 0.9734296709299087, 'num_tokens': 52919500.0, 'mean_token_accuracy': 0.7956093370914459, 'epoch': 2.21}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1650/2235 [3:01:08<56:25,  5.79s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1651/2235 [3:01:14<56:22,  5.79s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1652/2235 [3:01:20<56:16,  5.79s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1653/2235 [3:01:25<56:14,  5.80s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1654/2235 [3:01:31<56:13,  5.81s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1655/2235 [3:01:37<56:07,  5.81s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1656/2235 [3:01:43<56:08,  5.82s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1657/2235 [3:01:49<56:07,  5.83s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1658/2235 [3:01:54<56:06,  5.83s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1659/2235 [3:02:00<55:56,  5.83s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1660/2235 [3:02:06<55:46,  5.82s/it]                                                     {'loss': 1.0813, 'grad_norm': 0.408203125, 'learning_rate': 7.756118640067298e-07, 'entropy': 1.0897071272134782, 'num_tokens': 53241688.0, 'mean_token_accuracy': 0.7669689923524856, 'epoch': 2.23}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1660/2235 [3:02:06<55:46,  5.82s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1661/2235 [3:02:12<55:43,  5.82s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1662/2235 [3:02:18<55:39,  5.83s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1663/2235 [3:02:24<55:35,  5.83s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1664/2235 [3:02:29<55:31,  5.83s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1665/2235 [3:02:35<55:27,  5.84s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1666/2235 [3:02:41<55:26,  5.85s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1667/2235 [3:02:47<55:22,  5.85s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1668/2235 [3:02:53<55:16,  5.85s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1669/2235 [3:02:59<55:02,  5.83s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1670/2235 [3:03:05<54:54,  5.83s/it]                                                     {'loss': 1.0054, 'grad_norm': 0.423828125, 'learning_rate': 7.503395741918662e-07, 'entropy': 1.0164101034402848, 'num_tokens': 53561943.0, 'mean_token_accuracy': 0.7856494724750519, 'epoch': 2.24}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1670/2235 [3:03:05<54:54,  5.83s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1671/2235 [3:03:10<54:46,  5.83s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1672/2235 [3:03:16<54:37,  5.82s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1673/2235 [3:03:22<54:27,  5.81s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1674/2235 [3:03:28<54:19,  5.81s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1675/2235 [3:03:34<54:11,  5.81s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1676/2235 [3:03:39<53:57,  5.79s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1677/2235 [3:03:45<53:53,  5.80s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1678/2235 [3:03:51<53:42,  5.79s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1679/2235 [3:03:57<53:37,  5.79s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1680/2235 [3:04:02<53:35,  5.79s/it]                                                     {'loss': 0.9839, 'grad_norm': 0.345703125, 'learning_rate': 7.254129779526598e-07, 'entropy': 0.9920285224914551, 'num_tokens': 53883024.0, 'mean_token_accuracy': 0.7893822520971299, 'epoch': 2.26}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1680/2235 [3:04:02<53:35,  5.79s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1681/2235 [3:04:08<53:33,  5.80s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1682/2235 [3:04:14<53:32,  5.81s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1683/2235 [3:04:20<53:31,  5.82s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1684/2235 [3:04:26<53:29,  5.83s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1685/2235 [3:04:32<53:27,  5.83s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1686/2235 [3:04:37<53:22,  5.83s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1687/2235 [3:04:43<53:14,  5.83s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1688/2235 [3:04:49<53:02,  5.82s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1689/2235 [3:04:55<52:52,  5.81s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1690/2235 [3:05:01<52:43,  5.80s/it]                                                     {'loss': 1.0833, 'grad_norm': 0.400390625, 'learning_rate': 7.008370002242887e-07, 'entropy': 1.0950720340013504, 'num_tokens': 54202076.0, 'mean_token_accuracy': 0.7687008708715439, 'epoch': 2.27}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1690/2235 [3:05:01<52:43,  5.80s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1691/2235 [3:05:06<52:35,  5.80s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1692/2235 [3:05:12<52:26,  5.79s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1693/2235 [3:05:18<52:14,  5.78s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1694/2235 [3:05:24<52:03,  5.77s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1695/2235 [3:05:29<51:52,  5.76s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1696/2235 [3:05:35<51:51,  5.77s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1697/2235 [3:05:41<51:48,  5.78s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1698/2235 [3:05:47<51:47,  5.79s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1699/2235 [3:05:53<51:42,  5.79s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1700/2235 [3:05:58<51:38,  5.79s/it]                                                     {'loss': 1.0402, 'grad_norm': 0.310546875, 'learning_rate': 6.766164966675951e-07, 'entropy': 1.0561422169208527, 'num_tokens': 54522944.0, 'mean_token_accuracy': 0.7773045688867569, 'epoch': 2.28}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1700/2235 [3:05:58<51:38,  5.79s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1701/2235 [3:06:04<51:32,  5.79s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1702/2235 [3:06:10<51:28,  5.79s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1703/2235 [3:06:16<51:28,  5.80s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1704/2235 [3:06:22<51:24,  5.81s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1705/2235 [3:06:28<51:21,  5.81s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1706/2235 [3:06:33<51:23,  5.83s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1707/2235 [3:06:39<51:20,  5.83s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1708/2235 [3:06:45<51:08,  5.82s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1709/2235 [3:06:51<50:56,  5.81s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1710/2235 [3:06:57<50:51,  5.81s/it]                                                     {'loss': 1.0536, 'grad_norm': 0.408203125, 'learning_rate': 6.527562527097151e-07, 'entropy': 1.0645053684711456, 'num_tokens': 54844472.0, 'mean_token_accuracy': 0.773359215259552, 'epoch': 2.3}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1710/2235 [3:06:57<50:51,  5.81s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1711/2235 [3:07:02<50:12,  5.75s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1712/2235 [3:07:08<50:14,  5.76s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1713/2235 [3:07:14<50:15,  5.78s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1714/2235 [3:07:20<50:09,  5.78s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1715/2235 [3:07:25<50:07,  5.78s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1716/2235 [3:07:31<50:05,  5.79s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1717/2235 [3:07:37<50:02,  5.80s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1718/2235 [3:07:43<49:59,  5.80s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1719/2235 [3:07:49<49:56,  5.81s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1720/2235 [3:07:54<49:50,  5.81s/it]                                                     {'loss': 1.0396, 'grad_norm': 0.34765625, 'learning_rate': 6.292609825985824e-07, 'entropy': 1.0562781006097794, 'num_tokens': 55165809.0, 'mean_token_accuracy': 0.7768844991922379, 'epoch': 2.31}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1720/2235 [3:07:54<49:50,  5.81s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1721/2235 [3:08:00<49:49,  5.82s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1722/2235 [3:08:06<49:49,  5.83s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1723/2235 [3:08:12<49:45,  5.83s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1724/2235 [3:08:18<49:40,  5.83s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1725/2235 [3:08:23<49:01,  5.77s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1726/2235 [3:08:29<49:01,  5.78s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1727/2235 [3:08:35<48:58,  5.78s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1728/2235 [3:08:41<48:53,  5.79s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1729/2235 [3:08:47<48:46,  5.78s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1730/2235 [3:08:52<48:40,  5.78s/it]                                                     {'loss': 1.0352, 'grad_norm': 0.447265625, 'learning_rate': 6.061353284715046e-07, 'entropy': 1.0445455223321916, 'num_tokens': 55486770.0, 'mean_token_accuracy': 0.7769060969352722, 'epoch': 2.32}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1730/2235 [3:08:52<48:40,  5.78s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1731/2235 [3:08:58<48:30,  5.78s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1732/2235 [3:09:04<48:20,  5.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1733/2235 [3:09:10<48:14,  5.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1734/2235 [3:09:15<48:09,  5.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1735/2235 [3:09:21<48:04,  5.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1736/2235 [3:09:27<47:58,  5.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1737/2235 [3:09:33<47:53,  5.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1738/2235 [3:09:39<47:49,  5.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1739/2235 [3:09:44<47:44,  5.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1740/2235 [3:09:50<47:31,  5.76s/it]                                                     {'loss': 1.0195, 'grad_norm': 0.3671875, 'learning_rate': 5.83383859437976e-07, 'entropy': 1.0325191766023636, 'num_tokens': 55807536.0, 'mean_token_accuracy': 0.7820926159620285, 'epoch': 2.34}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1740/2235 [3:09:50<47:31,  5.76s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1741/2235 [3:09:56<47:29,  5.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1742/2235 [3:10:02<47:18,  5.76s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1743/2235 [3:10:07<47:16,  5.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1744/2235 [3:10:13<47:07,  5.76s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1745/2235 [3:10:19<47:04,  5.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1746/2235 [3:10:25<47:02,  5.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1747/2235 [3:10:30<46:59,  5.78s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1748/2235 [3:10:36<46:55,  5.78s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1749/2235 [3:10:42<46:47,  5.78s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1750/2235 [3:10:48<46:45,  5.79s/it]                                                     {'loss': 1.0277, 'grad_norm': 0.421875, 'learning_rate': 5.610110706769262e-07, 'entropy': 1.0406324028968812, 'num_tokens': 56128537.0, 'mean_token_accuracy': 0.779881727695465, 'epoch': 2.35}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1750/2235 [3:10:48<46:45,  5.79s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1751/2235 [3:10:54<46:40,  5.79s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1752/2235 [3:10:59<46:40,  5.80s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1753/2235 [3:11:05<46:41,  5.81s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1754/2235 [3:11:11<46:41,  5.82s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1755/2235 [3:11:17<46:37,  5.83s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1756/2235 [3:11:23<46:33,  5.83s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1757/2235 [3:11:29<46:26,  5.83s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1758/2235 [3:11:34<46:18,  5.82s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1759/2235 [3:11:40<46:10,  5.82s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1760/2235 [3:11:46<45:59,  5.81s/it]                                                     {'loss': 1.0491, 'grad_norm': 0.53125, 'learning_rate': 5.390213825485765e-07, 'entropy': 1.0604758858680725, 'num_tokens': 56448738.0, 'mean_token_accuracy': 0.7765675842761993, 'epoch': 2.36}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1760/2235 [3:11:46<45:59,  5.81s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1761/2235 [3:11:52<45:50,  5.80s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1762/2235 [3:11:58<45:37,  5.79s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1763/2235 [3:12:03<45:27,  5.78s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1764/2235 [3:12:09<45:21,  5.78s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1765/2235 [3:12:15<45:16,  5.78s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1766/2235 [3:12:21<45:14,  5.79s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1767/2235 [3:12:27<45:11,  5.79s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1768/2235 [3:12:32<45:09,  5.80s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1769/2235 [3:12:38<45:02,  5.80s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1770/2235 [3:12:44<45:00,  5.81s/it]                                                     {'loss': 1.038, 'grad_norm': 0.40625, 'learning_rate': 5.174191397210703e-07, 'entropy': 1.0530858039855957, 'num_tokens': 56769929.0, 'mean_token_accuracy': 0.777837011218071, 'epoch': 2.38}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1770/2235 [3:12:44<45:00,  5.81s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1771/2235 [3:12:50<44:58,  5.82s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1772/2235 [3:12:56<44:53,  5.82s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1773/2235 [3:13:01<44:55,  5.83s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1774/2235 [3:13:07<44:51,  5.84s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1775/2235 [3:13:13<44:44,  5.83s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1776/2235 [3:13:19<44:36,  5.83s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1777/2235 [3:13:25<44:28,  5.83s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1778/2235 [3:13:31<44:16,  5.81s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1779/2235 [3:13:36<44:08,  5.81s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1780/2235 [3:13:42<44:01,  5.81s/it]                                                     {'loss': 1.0214, 'grad_norm': 0.353515625, 'learning_rate': 4.962086103120714e-07, 'entropy': 1.0331870675086976, 'num_tokens': 57090996.0, 'mean_token_accuracy': 0.7815848231315613, 'epoch': 2.39}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1780/2235 [3:13:42<44:01,  5.81s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1781/2235 [3:13:48<43:52,  5.80s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1782/2235 [3:13:54<43:46,  5.80s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1783/2235 [3:14:00<43:36,  5.79s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1784/2235 [3:14:05<43:32,  5.79s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1785/2235 [3:14:11<43:28,  5.80s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1786/2235 [3:14:17<43:25,  5.80s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1787/2235 [3:14:23<43:22,  5.81s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1788/2235 [3:14:29<43:19,  5.82s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1789/2235 [3:14:34<43:17,  5.82s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1790/2235 [3:14:40<43:16,  5.83s/it]                                                     {'loss': 1.0401, 'grad_norm': 0.345703125, 'learning_rate': 4.7539398504547225e-07, 'entropy': 1.0478981018066407, 'num_tokens': 57412538.0, 'mean_token_accuracy': 0.7771336555480957, 'epoch': 2.4}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1790/2235 [3:14:40<43:16,  5.83s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1791/2235 [3:14:46<43:15,  5.85s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1792/2235 [3:14:52<43:07,  5.84s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1793/2235 [3:14:58<42:54,  5.82s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1794/2235 [3:15:04<42:45,  5.82s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1795/2235 [3:15:09<42:33,  5.80s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1796/2235 [3:15:15<42:23,  5.79s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1797/2235 [3:15:21<42:14,  5.79s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1798/2235 [3:15:27<42:02,  5.77s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1799/2235 [3:15:32<41:55,  5.77s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1800/2235 [3:15:38<41:48,  5.77s/it]                                                     {'loss': 1.089, 'grad_norm': 0.41015625, 'learning_rate': 4.549793764234062e-07, 'entropy': 1.1045214593410493, 'num_tokens': 57732379.0, 'mean_token_accuracy': 0.7667473167181015, 'epoch': 2.42}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1800/2235 [3:15:38<41:48,  5.77s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1801/2235 [3:15:44<41:41,  5.76s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1802/2235 [3:15:50<41:34,  5.76s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1803/2235 [3:15:55<41:27,  5.76s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1804/2235 [3:16:01<41:20,  5.76s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1805/2235 [3:16:07<41:13,  5.75s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1806/2235 [3:16:13<41:01,  5.74s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1807/2235 [3:16:18<40:55,  5.74s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1808/2235 [3:16:24<40:51,  5.74s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1809/2235 [3:16:30<40:46,  5.74s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1810/2235 [3:16:36<40:40,  5.74s/it]                                                     {'loss': 1.0532, 'grad_norm': 0.361328125, 'learning_rate': 4.349688179137099e-07, 'entropy': 1.0656299084424972, 'num_tokens': 58053666.0, 'mean_token_accuracy': 0.7736766308546066, 'epoch': 2.43}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1810/2235 [3:16:36<40:40,  5.74s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1811/2235 [3:16:41<40:31,  5.73s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1812/2235 [3:16:47<40:26,  5.74s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1813/2235 [3:16:53<40:23,  5.74s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1814/2235 [3:16:59<40:14,  5.74s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1815/2235 [3:17:04<40:06,  5.73s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1816/2235 [3:17:10<40:01,  5.73s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1817/2235 [3:17:16<39:59,  5.74s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1818/2235 [3:17:22<39:56,  5.75s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1819/2235 [3:17:27<39:52,  5.75s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1820/2235 [3:17:33<39:47,  5.75s/it]                                                     {'loss': 1.0784, 'grad_norm': 0.408203125, 'learning_rate': 4.153662631529967e-07, 'entropy': 1.0868716776371001, 'num_tokens': 58374958.0, 'mean_token_accuracy': 0.7708169102668763, 'epoch': 2.44}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1820/2235 [3:17:33<39:47,  5.75s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1821/2235 [3:17:39<39:43,  5.76s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1822/2235 [3:17:45<39:39,  5.76s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1823/2235 [3:17:50<39:32,  5.76s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1824/2235 [3:17:56<39:28,  5.76s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1825/2235 [3:18:02<39:21,  5.76s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1826/2235 [3:18:08<39:18,  5.77s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1827/2235 [3:18:13<39:13,  5.77s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1828/2235 [3:18:19<39:09,  5.77s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1829/2235 [3:18:25<39:04,  5.78s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1830/2235 [3:18:31<39:01,  5.78s/it]                                                     {'loss': 1.0377, 'grad_norm': 0.33984375, 'learning_rate': 3.961755851655108e-07, 'entropy': 1.0533162385225296, 'num_tokens': 58696281.0, 'mean_token_accuracy': 0.7753169387578964, 'epoch': 2.46}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1830/2235 [3:18:31<39:01,  5.78s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1831/2235 [3:18:37<38:57,  5.79s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1832/2235 [3:18:42<38:52,  5.79s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1833/2235 [3:18:48<38:50,  5.80s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1834/2235 [3:18:54<38:48,  5.81s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1835/2235 [3:19:00<38:47,  5.82s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1836/2235 [3:19:06<38:46,  5.83s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1837/2235 [3:19:12<38:42,  5.83s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1838/2235 [3:19:17<38:34,  5.83s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1839/2235 [3:19:23<38:24,  5.82s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1840/2235 [3:19:29<38:13,  5.81s/it]                                                     {'loss': 1.0005, 'grad_norm': 0.515625, 'learning_rate': 3.774005755979024e-07, 'entropy': 1.0087847262620926, 'num_tokens': 59015897.0, 'mean_token_accuracy': 0.7854361355304718, 'epoch': 2.47}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1840/2235 [3:19:29<38:13,  5.81s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1841/2235 [3:19:35<38:05,  5.80s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1842/2235 [3:19:40<37:54,  5.79s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1843/2235 [3:19:46<37:48,  5.79s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1844/2235 [3:19:52<37:42,  5.79s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1845/2235 [3:19:58<37:33,  5.78s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1846/2235 [3:20:04<37:25,  5.77s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1847/2235 [3:20:09<37:20,  5.77s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1848/2235 [3:20:15<36:51,  5.71s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1849/2235 [3:20:21<36:53,  5.73s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1850/2235 [3:20:26<36:52,  5.75s/it]                                                     {'loss': 1.0315, 'grad_norm': 0.396484375, 'learning_rate': 3.590449439700838e-07, 'entropy': 1.0423811286687852, 'num_tokens': 59337430.0, 'mean_token_accuracy': 0.7796726703643799, 'epoch': 2.48}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1850/2235 [3:20:26<36:52,  5.75s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1851/2235 [3:20:32<36:51,  5.76s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1852/2235 [3:20:38<36:47,  5.76s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1853/2235 [3:20:44<36:41,  5.76s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1854/2235 [3:20:50<36:32,  5.75s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1855/2235 [3:20:55<36:29,  5.76s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1856/2235 [3:21:01<36:23,  5.76s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1857/2235 [3:21:07<36:18,  5.76s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1858/2235 [3:21:13<36:10,  5.76s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1859/2235 [3:21:18<36:09,  5.77s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1860/2235 [3:21:24<36:05,  5.78s/it]                                                     {'loss': 1.0122, 'grad_norm': 0.369140625, 'learning_rate': 3.4111231694231335e-07, 'entropy': 1.0249922424554825, 'num_tokens': 59657833.0, 'mean_token_accuracy': 0.7830858111381531, 'epoch': 2.5}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1860/2235 [3:21:24<36:05,  5.78s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1861/2235 [3:21:30<36:03,  5.79s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1862/2235 [3:21:36<35:57,  5.78s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1863/2235 [3:21:42<35:56,  5.80s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1864/2235 [3:21:47<35:55,  5.81s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1865/2235 [3:21:53<35:50,  5.81s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

  0%|          | 0/187 [00:00<?, ?it/s][A
  1%|          | 2/187 [00:01<02:43,  1.13it/s][A
  2%|â–         | 3/187 [00:03<03:50,  1.25s/it][A
  2%|â–         | 4/187 [00:05<04:24,  1.44s/it][A
  3%|â–Ž         | 5/187 [00:07<04:42,  1.55s/it][A
  3%|â–Ž         | 6/187 [00:08<04:53,  1.62s/it][A
  4%|â–Ž         | 7/187 [00:10<04:59,  1.66s/it][A
  4%|â–         | 8/187 [00:12<05:02,  1.69s/it][A
  5%|â–         | 9/187 [00:14<05:04,  1.71s/it][A
  5%|â–Œ         | 10/187 [00:15<05:04,  1.72s/it][A
  6%|â–Œ         | 11/187 [00:17<05:04,  1.73s/it][A
  6%|â–‹         | 12/187 [00:19<05:03,  1.73s/it][A
  7%|â–‹         | 13/187 [00:21<05:02,  1.74s/it][A
  7%|â–‹         | 14/187 [00:22<05:00,  1.74s/it][A
  8%|â–Š         | 15/187 [00:24<04:58,  1.74s/it][A
  9%|â–Š         | 16/187 [00:26<04:57,  1.74s/it][A
  9%|â–‰         | 17/187 [00:28<04:55,  1.74s/it][A
 10%|â–‰         | 18/187 [00:29<04:53,  1.74s/it][A
 10%|â–ˆ         | 19/187 [00:31<04:51,  1.74s/it][A
 11%|â–ˆ         | 20/187 [00:33<04:50,  1.74s/it][A
 11%|â–ˆ         | 21/187 [00:34<04:48,  1.74s/it][A
 12%|â–ˆâ–        | 22/187 [00:36<04:46,  1.74s/it][A
 12%|â–ˆâ–        | 23/187 [00:38<04:44,  1.73s/it][A
 13%|â–ˆâ–Ž        | 24/187 [00:40<04:42,  1.73s/it][A
 13%|â–ˆâ–Ž        | 25/187 [00:41<04:40,  1.73s/it][A
 14%|â–ˆâ–        | 26/187 [00:43<04:39,  1.73s/it][A
 14%|â–ˆâ–        | 27/187 [00:45<04:37,  1.73s/it][A
 15%|â–ˆâ–        | 28/187 [00:47<04:35,  1.73s/it][A
 16%|â–ˆâ–Œ        | 29/187 [00:48<04:33,  1.73s/it][A
 16%|â–ˆâ–Œ        | 30/187 [00:50<04:32,  1.73s/it][A
 17%|â–ˆâ–‹        | 31/187 [00:52<04:30,  1.73s/it][A
 17%|â–ˆâ–‹        | 32/187 [00:54<04:28,  1.73s/it][A
 18%|â–ˆâ–Š        | 33/187 [00:55<04:27,  1.73s/it][A
 18%|â–ˆâ–Š        | 34/187 [00:57<04:25,  1.73s/it][A
 19%|â–ˆâ–Š        | 35/187 [00:59<04:23,  1.73s/it][A
 19%|â–ˆâ–‰        | 36/187 [01:00<04:22,  1.74s/it][A
 20%|â–ˆâ–‰        | 37/187 [01:02<04:20,  1.74s/it][A
 20%|â–ˆâ–ˆ        | 38/187 [01:04<04:18,  1.74s/it][A
 21%|â–ˆâ–ˆ        | 39/187 [01:06<04:17,  1.74s/it][A
 21%|â–ˆâ–ˆâ–       | 40/187 [01:07<04:15,  1.74s/it][A
 22%|â–ˆâ–ˆâ–       | 41/187 [01:09<04:13,  1.74s/it][A
 22%|â–ˆâ–ˆâ–       | 42/187 [01:11<04:11,  1.74s/it][A
 23%|â–ˆâ–ˆâ–Ž       | 43/187 [01:13<04:09,  1.73s/it][A
 24%|â–ˆâ–ˆâ–Ž       | 44/187 [01:14<04:08,  1.73s/it][A
 24%|â–ˆâ–ˆâ–       | 45/187 [01:16<04:06,  1.73s/it][A
 25%|â–ˆâ–ˆâ–       | 46/187 [01:18<04:04,  1.73s/it][A
 25%|â–ˆâ–ˆâ–Œ       | 47/187 [01:20<04:02,  1.73s/it][A
 26%|â–ˆâ–ˆâ–Œ       | 48/187 [01:21<04:01,  1.74s/it][A
 26%|â–ˆâ–ˆâ–Œ       | 49/187 [01:23<03:59,  1.74s/it][A
 27%|â–ˆâ–ˆâ–‹       | 50/187 [01:25<03:57,  1.74s/it][A
 27%|â–ˆâ–ˆâ–‹       | 51/187 [01:26<03:56,  1.74s/it][A
 28%|â–ˆâ–ˆâ–Š       | 52/187 [01:28<03:54,  1.74s/it][A
 28%|â–ˆâ–ˆâ–Š       | 53/187 [01:30<03:52,  1.74s/it][A
 29%|â–ˆâ–ˆâ–‰       | 54/187 [01:32<03:51,  1.74s/it][A
 29%|â–ˆâ–ˆâ–‰       | 55/187 [01:33<03:49,  1.74s/it][A
 30%|â–ˆâ–ˆâ–‰       | 56/187 [01:35<03:47,  1.74s/it][A
 30%|â–ˆâ–ˆâ–ˆ       | 57/187 [01:37<03:46,  1.74s/it][A
 31%|â–ˆâ–ˆâ–ˆ       | 58/187 [01:39<03:44,  1.74s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 59/187 [01:40<03:42,  1.74s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 60/187 [01:42<03:41,  1.74s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 61/187 [01:44<03:39,  1.74s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 62/187 [01:46<03:37,  1.74s/it][A
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 63/187 [01:47<03:36,  1.74s/it][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 64/187 [01:49<03:34,  1.75s/it][A
 35%|â–ˆâ–ˆâ–ˆâ–      | 65/187 [01:51<03:33,  1.75s/it][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 66/187 [01:53<03:31,  1.75s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 67/187 [01:54<03:30,  1.75s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 68/187 [01:56<03:28,  1.75s/it][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 69/187 [01:58<03:26,  1.75s/it][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/187 [02:00<03:25,  1.75s/it][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 71/187 [02:01<03:23,  1.76s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 72/187 [02:03<03:22,  1.76s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 73/187 [02:05<03:20,  1.76s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 74/187 [02:07<03:19,  1.76s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 75/187 [02:08<03:17,  1.76s/it][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76/187 [02:10<03:15,  1.76s/it][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77/187 [02:12<03:14,  1.76s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/187 [02:14<03:12,  1.76s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/187 [02:16<03:10,  1.76s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 80/187 [02:17<03:08,  1.76s/it][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 81/187 [02:19<03:06,  1.76s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/187 [02:21<03:04,  1.76s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/187 [02:23<03:02,  1.75s/it][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/187 [02:24<03:00,  1.75s/it][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 85/187 [02:26<02:58,  1.75s/it][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86/187 [02:28<02:56,  1.75s/it][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 87/187 [02:30<02:54,  1.75s/it][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88/187 [02:31<02:53,  1.75s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 89/187 [02:33<02:51,  1.75s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/187 [02:35<02:49,  1.75s/it][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91/187 [02:37<02:47,  1.75s/it][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 92/187 [02:38<02:45,  1.75s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 93/187 [02:40<02:43,  1.74s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 94/187 [02:42<02:42,  1.74s/it][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 95/187 [02:43<02:40,  1.74s/it][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 96/187 [02:45<02:38,  1.74s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/187 [02:47<02:36,  1.74s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/187 [02:49<02:35,  1.74s/it][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 99/187 [02:50<02:33,  1.74s/it][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 100/187 [02:52<02:31,  1.74s/it][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 101/187 [02:54<02:29,  1.74s/it][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/187 [02:56<02:28,  1.74s/it][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 103/187 [02:57<02:26,  1.74s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 104/187 [02:59<02:24,  1.74s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105/187 [03:01<02:22,  1.74s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 106/187 [03:03<02:21,  1.74s/it][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 107/187 [03:04<02:19,  1.74s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 108/187 [03:06<02:17,  1.74s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 109/187 [03:08<02:15,  1.74s/it][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 110/187 [03:10<02:14,  1.74s/it][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 111/187 [03:11<02:12,  1.74s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112/187 [03:13<02:10,  1.74s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 113/187 [03:15<02:08,  1.74s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114/187 [03:17<02:07,  1.74s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 115/187 [03:18<02:05,  1.74s/it][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/187 [03:20<02:03,  1.74s/it][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 117/187 [03:22<02:02,  1.74s/it][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 118/187 [03:24<02:00,  1.75s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 119/187 [03:25<01:58,  1.74s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120/187 [03:27<01:56,  1.75s/it][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121/187 [03:29<01:55,  1.75s/it][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 122/187 [03:31<01:53,  1.75s/it][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 123/187 [03:32<01:51,  1.75s/it][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 124/187 [03:34<01:50,  1.75s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 125/187 [03:36<01:48,  1.75s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126/187 [03:38<01:46,  1.75s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 127/187 [03:39<01:45,  1.75s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 128/187 [03:41<01:43,  1.75s/it][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 129/187 [03:43<01:41,  1.75s/it][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/187 [03:45<01:40,  1.76s/it][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 131/187 [03:46<01:38,  1.76s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 132/187 [03:48<01:36,  1.76s/it][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133/187 [03:50<01:34,  1.76s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 134/187 [03:52<01:33,  1.76s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/187 [03:53<01:31,  1.76s/it][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 136/187 [03:55<01:29,  1.76s/it][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 137/187 [03:57<01:28,  1.77s/it][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138/187 [03:59<01:26,  1.77s/it][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 139/187 [04:00<01:24,  1.77s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/187 [04:02<01:23,  1.77s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/187 [04:04<01:21,  1.77s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 142/187 [04:06<01:19,  1.77s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 143/187 [04:08<01:17,  1.77s/it][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 144/187 [04:09<01:15,  1.76s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 145/187 [04:11<01:13,  1.76s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 146/187 [04:13<01:11,  1.75s/it][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 147/187 [04:15<01:09,  1.75s/it][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 148/187 [04:16<01:07,  1.74s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 149/187 [04:18<01:06,  1.74s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 150/187 [04:20<01:04,  1.74s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 151/187 [04:21<01:02,  1.74s/it][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 152/187 [04:23<01:00,  1.73s/it][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 153/187 [04:25<00:58,  1.73s/it][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 154/187 [04:27<00:57,  1.73s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 155/187 [04:28<00:55,  1.73s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 156/187 [04:30<00:53,  1.72s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 157/187 [04:32<00:51,  1.72s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 158/187 [04:34<00:49,  1.72s/it][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 159/187 [04:35<00:48,  1.72s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 160/187 [04:37<00:46,  1.72s/it][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 161/187 [04:39<00:44,  1.72s/it][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 162/187 [04:40<00:43,  1.72s/it][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 163/187 [04:42<00:41,  1.72s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 164/187 [04:44<00:39,  1.72s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 165/187 [04:46<00:37,  1.72s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 166/187 [04:47<00:36,  1.72s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 167/187 [04:49<00:34,  1.72s/it][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 168/187 [04:51<00:32,  1.72s/it][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 169/187 [04:52<00:30,  1.72s/it][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 170/187 [04:54<00:29,  1.72s/it][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 171/187 [04:56<00:27,  1.72s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 172/187 [04:58<00:25,  1.72s/it][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 173/187 [04:59<00:24,  1.72s/it][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 174/187 [05:01<00:22,  1.72s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 175/187 [05:03<00:20,  1.72s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 176/187 [05:04<00:18,  1.71s/it][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 177/187 [05:06<00:17,  1.71s/it][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 178/187 [05:08<00:15,  1.71s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 179/187 [05:10<00:13,  1.71s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 180/187 [05:11<00:11,  1.71s/it][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 181/187 [05:13<00:10,  1.71s/it][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 182/187 [05:15<00:08,  1.71s/it][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 183/187 [05:16<00:06,  1.71s/it][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 184/187 [05:18<00:05,  1.71s/it][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 185/187 [05:20<00:03,  1.71s/it][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 186/187 [05:22<00:01,  1.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [05:23<00:00,  1.74s/it][A                                                     
                                                 [A{'eval_loss': 1.040858507156372, 'eval_runtime': 326.0633, 'eval_samples_per_second': 18.279, 'eval_steps_per_second': 0.574, 'eval_entropy': 1.053400737397811, 'eval_num_tokens': 59818697.0, 'eval_mean_token_accuracy': 0.7768597816400987, 'epoch': 2.5}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1865/2235 [3:27:19<35:50,  5.81s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [05:23<00:00,  1.74s/it][A
                                                 [A/home/ubuntu/Part-II-Project/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1866/2235 [3:27:26<10:38:28, 103.82s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1867/2235 [3:27:31<7:36:14, 74.39s/it]   84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1868/2235 [3:27:37<5:29:01, 53.79s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1869/2235 [3:27:43<4:00:09, 39.37s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1870/2235 [3:27:49<2:58:07, 29.28s/it]                                                       {'loss': 1.122, 'grad_norm': 0.412109375, 'learning_rate': 3.2360623759864734e-07, 'entropy': 1.1313650131225585, 'num_tokens': 59978876.0, 'mean_token_accuracy': 0.7593303412199021, 'epoch': 2.51}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1870/2235 [3:27:49<2:58:07, 29.28s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1871/2235 [3:27:54<2:14:45, 22.21s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1872/2235 [3:28:00<1:44:29, 17.27s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1873/2235 [3:28:06<1:23:19, 13.81s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1874/2235 [3:28:12<1:08:30, 11.39s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1875/2235 [3:28:17<57:46,  9.63s/it]   84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1876/2235 [3:28:23<50:32,  8.45s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1877/2235 [3:28:29<45:33,  7.63s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1878/2235 [3:28:34<42:00,  7.06s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1879/2235 [3:28:40<39:32,  6.66s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1880/2235 [3:28:46<37:46,  6.38s/it]                                                     {'loss': 1.1094, 'grad_norm': 0.365234375, 'learning_rate': 3.065301647469057e-07, 'entropy': 1.1233368545770646, 'num_tokens': 60300789.0, 'mean_token_accuracy': 0.7616755425930023, 'epoch': 2.52}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1880/2235 [3:28:46<37:46,  6.38s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1881/2235 [3:28:51<36:31,  6.19s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1882/2235 [3:28:57<35:37,  6.05s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1883/2235 [3:29:03<34:56,  5.95s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1884/2235 [3:29:09<34:27,  5.89s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1885/2235 [3:29:14<34:04,  5.84s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1886/2235 [3:29:20<33:46,  5.81s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1887/2235 [3:29:26<33:34,  5.79s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1888/2235 [3:29:31<33:02,  5.71s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1889/2235 [3:29:37<33:00,  5.72s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1890/2235 [3:29:43<32:57,  5.73s/it]                                                     {'loss': 1.0843, 'grad_norm': 0.4140625, 'learning_rate': 2.8988747223529125e-07, 'entropy': 1.096329429745674, 'num_tokens': 60622429.0, 'mean_token_accuracy': 0.7681317329406738, 'epoch': 2.54}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1890/2235 [3:29:43<32:57,  5.73s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1891/2235 [3:29:49<32:51,  5.73s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1892/2235 [3:29:54<32:47,  5.74s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1893/2235 [3:30:00<32:38,  5.73s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1894/2235 [3:30:06<32:36,  5.74s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1895/2235 [3:30:12<32:26,  5.73s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1896/2235 [3:30:17<32:23,  5.73s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1897/2235 [3:30:23<32:21,  5.74s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1898/2235 [3:30:29<32:18,  5.75s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1899/2235 [3:30:35<32:13,  5.76s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1900/2235 [3:30:40<32:10,  5.76s/it]                                                     {'loss': 1.0677, 'grad_norm': 0.375, 'learning_rate': 2.736814482857911e-07, 'entropy': 1.0742446720600127, 'num_tokens': 60941990.0, 'mean_token_accuracy': 0.7717767119407654, 'epoch': 2.55}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1900/2235 [3:30:40<32:10,  5.76s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1901/2235 [3:30:46<32:06,  5.77s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1902/2235 [3:30:52<32:02,  5.77s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1903/2235 [3:30:58<31:56,  5.77s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1904/2235 [3:31:03<31:52,  5.78s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1905/2235 [3:31:09<31:47,  5.78s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1906/2235 [3:31:15<31:43,  5.79s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1907/2235 [3:31:21<31:40,  5.79s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1908/2235 [3:31:27<31:36,  5.80s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1909/2235 [3:31:33<31:33,  5.81s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1910/2235 [3:31:38<31:29,  5.81s/it]                                                     {'loss': 1.0287, 'grad_norm': 0.40234375, 'learning_rate': 2.579152948444999e-07, 'entropy': 1.0452507734298706, 'num_tokens': 61261833.0, 'mean_token_accuracy': 0.781396621465683, 'epoch': 2.56}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1910/2235 [3:31:38<31:29,  5.81s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1911/2235 [3:31:44<31:27,  5.83s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1912/2235 [3:31:50<31:18,  5.82s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1913/2235 [3:31:56<31:12,  5.82s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1914/2235 [3:32:02<31:05,  5.81s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1915/2235 [3:32:07<30:57,  5.81s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1916/2235 [3:32:13<30:48,  5.79s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1917/2235 [3:32:19<30:42,  5.79s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1918/2235 [3:32:25<30:35,  5.79s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1919/2235 [3:32:31<30:29,  5.79s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1920/2235 [3:32:36<30:19,  5.78s/it]                                                     {'loss': 1.0477, 'grad_norm': 0.38671875, 'learning_rate': 2.4259212694898505e-07, 'entropy': 1.0585427224636077, 'num_tokens': 61583452.0, 'mean_token_accuracy': 0.7752816200256347, 'epoch': 2.58}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1920/2235 [3:32:36<30:19,  5.78s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1921/2235 [3:32:42<30:14,  5.78s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1922/2235 [3:32:48<30:09,  5.78s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1923/2235 [3:32:54<30:02,  5.78s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1924/2235 [3:32:59<29:56,  5.78s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1925/2235 [3:33:05<29:51,  5.78s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1926/2235 [3:33:11<29:46,  5.78s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1927/2235 [3:33:17<29:41,  5.78s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1928/2235 [3:33:23<29:36,  5.79s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1929/2235 [3:33:28<29:29,  5.78s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1930/2235 [3:33:34<29:25,  5.79s/it]                                                     {'loss': 0.9872, 'grad_norm': 0.337890625, 'learning_rate': 2.2771497211282633e-07, 'entropy': 1.0001346528530122, 'num_tokens': 61904058.0, 'mean_token_accuracy': 0.7877995759248734, 'epoch': 2.59}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1930/2235 [3:33:34<29:25,  5.79s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1931/2235 [3:33:40<29:22,  5.80s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1932/2235 [3:33:46<29:17,  5.80s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1933/2235 [3:33:52<29:15,  5.81s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1934/2235 [3:33:57<29:12,  5.82s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1935/2235 [3:34:03<29:10,  5.84s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1936/2235 [3:34:09<29:04,  5.84s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1937/2235 [3:34:15<28:55,  5.82s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1938/2235 [3:34:21<28:49,  5.82s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1939/2235 [3:34:27<28:42,  5.82s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1940/2235 [3:34:32<28:35,  5.82s/it]                                                     {'loss': 1.0039, 'grad_norm': 0.408203125, 'learning_rate': 2.1328676972744932e-07, 'entropy': 1.018637791275978, 'num_tokens': 62223040.0, 'mean_token_accuracy': 0.7829928576946259, 'epoch': 2.6}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1940/2235 [3:34:32<28:35,  5.82s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1941/2235 [3:34:38<28:29,  5.81s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1942/2235 [3:34:44<28:20,  5.80s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1943/2235 [3:34:50<27:56,  5.74s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1944/2235 [3:34:55<27:56,  5.76s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1945/2235 [3:35:01<27:54,  5.77s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1946/2235 [3:35:07<27:52,  5.79s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1947/2235 [3:35:13<27:47,  5.79s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1948/2235 [3:35:19<27:44,  5.80s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1949/2235 [3:35:24<27:41,  5.81s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1950/2235 [3:35:30<27:37,  5.82s/it]                                                     {'loss': 1.0542, 'grad_norm': 0.353515625, 'learning_rate': 1.9931037048136342e-07, 'entropy': 1.0660248100757599, 'num_tokens': 62545240.0, 'mean_token_accuracy': 0.7728548973798752, 'epoch': 2.62}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1950/2235 [3:35:30<27:37,  5.82s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1951/2235 [3:35:36<27:33,  5.82s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1952/2235 [3:35:42<27:29,  5.83s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1953/2235 [3:35:48<27:26,  5.84s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1954/2235 [3:35:54<27:21,  5.84s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1955/2235 [3:36:00<27:16,  5.85s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1956/2235 [3:36:05<27:10,  5.84s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1957/2235 [3:36:11<27:02,  5.84s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1958/2235 [3:36:17<26:57,  5.84s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1959/2235 [3:36:23<26:51,  5.84s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1960/2235 [3:36:29<26:45,  5.84s/it]                                                     {'loss': 1.0835, 'grad_norm': 0.4140625, 'learning_rate': 1.8578853579693357e-07, 'entropy': 1.0958710372447968, 'num_tokens': 62866851.0, 'mean_token_accuracy': 0.7673978805541992, 'epoch': 2.63}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1960/2235 [3:36:29<26:45,  5.84s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1961/2235 [3:36:35<26:39,  5.84s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1962/2235 [3:36:40<26:30,  5.83s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1963/2235 [3:36:46<26:26,  5.83s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1964/2235 [3:36:52<26:20,  5.83s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1965/2235 [3:36:58<26:16,  5.84s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1966/2235 [3:37:04<26:09,  5.84s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1967/2235 [3:37:10<26:05,  5.84s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1968/2235 [3:37:15<26:00,  5.84s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1969/2235 [3:37:21<25:54,  5.84s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1970/2235 [3:37:27<25:45,  5.83s/it]                                                     {'loss': 1.0059, 'grad_norm': 0.40234375, 'learning_rate': 1.7272393728478358e-07, 'entropy': 1.0219996660947799, 'num_tokens': 63187270.0, 'mean_token_accuracy': 0.7843628823757172, 'epoch': 2.64}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1970/2235 [3:37:27<25:45,  5.83s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1971/2235 [3:37:33<25:24,  5.77s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1972/2235 [3:37:39<25:24,  5.80s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1973/2235 [3:37:44<25:22,  5.81s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1974/2235 [3:37:50<25:19,  5.82s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1975/2235 [3:37:56<25:14,  5.83s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1976/2235 [3:38:02<25:10,  5.83s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1977/2235 [3:38:08<25:04,  5.83s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1978/2235 [3:38:14<24:59,  5.84s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1979/2235 [3:38:19<24:54,  5.84s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1980/2235 [3:38:25<24:50,  5.84s/it]                                                     {'loss': 1.012, 'grad_norm': 0.36328125, 'learning_rate': 1.6011915621594543e-07, 'entropy': 1.0276603698730469, 'num_tokens': 63508660.0, 'mean_token_accuracy': 0.7830500602722168, 'epoch': 2.66}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1980/2235 [3:38:25<24:50,  5.84s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1981/2235 [3:38:31<24:45,  5.85s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1982/2235 [3:38:37<24:37,  5.84s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1983/2235 [3:38:43<24:28,  5.83s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1984/2235 [3:38:49<24:17,  5.81s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1985/2235 [3:38:54<24:09,  5.80s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1986/2235 [3:39:00<24:01,  5.79s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1987/2235 [3:39:06<23:54,  5.78s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1988/2235 [3:39:12<23:46,  5.77s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1989/2235 [3:39:17<23:39,  5.77s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1990/2235 [3:39:23<23:33,  5.77s/it]                                                     {'loss': 1.0086, 'grad_norm': 0.392578125, 'learning_rate': 1.4797668301186062e-07, 'entropy': 1.022437059879303, 'num_tokens': 63829085.0, 'mean_token_accuracy': 0.7844369769096374, 'epoch': 2.67}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1990/2235 [3:39:23<23:33,  5.77s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1991/2235 [3:39:29<23:26,  5.77s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1992/2235 [3:39:35<23:20,  5.76s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1993/2235 [3:39:40<23:13,  5.76s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1994/2235 [3:39:46<23:07,  5.76s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1995/2235 [3:39:52<23:00,  5.75s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1996/2235 [3:39:58<22:54,  5.75s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1997/2235 [3:40:03<22:48,  5.75s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1998/2235 [3:40:09<22:41,  5.75s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1999/2235 [3:40:15<22:36,  5.75s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2000/2235 [3:40:21<22:30,  5.75s/it]                                                     {'loss': 1.0635, 'grad_norm': 0.39453125, 'learning_rate': 1.3629891675232514e-07, 'entropy': 1.0796937704086305, 'num_tokens': 64150551.0, 'mean_token_accuracy': 0.7720254361629486, 'epoch': 2.68}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2000/2235 [3:40:21<22:30,  5.75s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2001/2235 [3:40:26<22:25,  5.75s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2002/2235 [3:40:32<22:20,  5.75s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2003/2235 [3:40:38<22:14,  5.75s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2004/2235 [3:40:44<22:09,  5.76s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2005/2235 [3:40:49<22:03,  5.75s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2006/2235 [3:40:55<21:57,  5.75s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2007/2235 [3:41:01<21:38,  5.70s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2008/2235 [3:41:06<21:38,  5.72s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2009/2235 [3:41:12<21:33,  5.72s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2010/2235 [3:41:18<21:30,  5.73s/it]                                                     {'loss': 1.0467, 'grad_norm': 0.43359375, 'learning_rate': 1.2508816470148955e-07, 'entropy': 1.058793193101883, 'num_tokens': 64471980.0, 'mean_token_accuracy': 0.7745562344789505, 'epoch': 2.7}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2010/2235 [3:41:18<21:30,  5.73s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2011/2235 [3:41:24<21:27,  5.75s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2012/2235 [3:41:30<21:22,  5.75s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2013/2235 [3:41:35<21:18,  5.76s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2014/2235 [3:41:41<21:14,  5.77s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2015/2235 [3:41:47<21:11,  5.78s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2016/2235 [3:41:53<21:04,  5.77s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2017/2235 [3:41:58<21:00,  5.78s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2018/2235 [3:42:04<20:57,  5.79s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2019/2235 [3:42:10<20:53,  5.81s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2020/2235 [3:42:16<20:49,  5.81s/it]                                                     {'loss': 0.9891, 'grad_norm': 0.37890625, 'learning_rate': 1.1434664185199062e-07, 'entropy': 1.0015636503696441, 'num_tokens': 64791743.0, 'mean_token_accuracy': 0.7880658447742462, 'epoch': 2.71}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2020/2235 [3:42:16<20:49,  5.81s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2021/2235 [3:42:22<20:45,  5.82s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2022/2235 [3:42:28<20:42,  5.83s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2023/2235 [3:42:33<20:35,  5.83s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2024/2235 [3:42:39<20:27,  5.82s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2025/2235 [3:42:45<20:19,  5.81s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2026/2235 [3:42:51<20:10,  5.79s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2027/2235 [3:42:57<20:04,  5.79s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2028/2235 [3:43:02<19:57,  5.78s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2029/2235 [3:43:08<19:42,  5.74s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2030/2235 [3:43:14<19:38,  5.75s/it]                                                     {'loss': 1.0561, 'grad_norm': 0.390625, 'learning_rate': 1.0407647048732345e-07, 'entropy': 1.0702348619699478, 'num_tokens': 65114028.0, 'mean_token_accuracy': 0.7726138353347778, 'epoch': 2.72}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2030/2235 [3:43:14<19:38,  5.75s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2031/2235 [3:43:20<19:33,  5.75s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2032/2235 [3:43:25<19:28,  5.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2033/2235 [3:43:31<19:23,  5.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2034/2235 [3:43:37<19:17,  5.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2035/2235 [3:43:43<19:12,  5.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2036/2235 [3:43:48<19:06,  5.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2037/2235 [3:43:54<19:00,  5.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2038/2235 [3:44:00<18:55,  5.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2039/2235 [3:44:06<18:49,  5.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2040/2235 [3:44:11<18:44,  5.77s/it]                                                     {'loss': 1.027, 'grad_norm': 0.365234375, 'learning_rate': 9.427967976252173e-08, 'entropy': 1.0388746589422226, 'num_tokens': 65434020.0, 'mean_token_accuracy': 0.7785505950450897, 'epoch': 2.74}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2040/2235 [3:44:11<18:44,  5.77s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2041/2235 [3:44:17<18:38,  5.77s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2042/2235 [3:44:23<18:32,  5.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2043/2235 [3:44:29<18:25,  5.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2044/2235 [3:44:34<18:19,  5.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2045/2235 [3:44:40<18:14,  5.76s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2046/2235 [3:44:46<18:09,  5.76s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2047/2235 [3:44:52<18:03,  5.77s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2048/2235 [3:44:57<17:57,  5.76s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2049/2235 [3:45:03<17:52,  5.77s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2050/2235 [3:45:09<17:47,  5.77s/it]                                                     {'loss': 0.9443, 'grad_norm': 0.3828125, 'learning_rate': 8.495820530324522e-08, 'entropy': 0.9544067621231079, 'num_tokens': 65752899.0, 'mean_token_accuracy': 0.7975385189056396, 'epoch': 2.75}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2050/2235 [3:45:09<17:47,  5.77s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2051/2235 [3:45:15<17:42,  5.77s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2052/2235 [3:45:21<17:36,  5.77s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2053/2235 [3:45:26<17:31,  5.78s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2054/2235 [3:45:32<17:27,  5.79s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2055/2235 [3:45:38<17:24,  5.80s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2056/2235 [3:45:44<17:20,  5.81s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2057/2235 [3:45:50<17:17,  5.83s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2058/2235 [3:45:56<17:12,  5.83s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2059/2235 [3:46:01<17:05,  5.83s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2060/2235 [3:46:07<16:57,  5.81s/it]                                                     {'loss': 1.0328, 'grad_norm': 0.376953125, 'learning_rate': 7.611388882334286e-08, 'entropy': 1.0449463218450545, 'num_tokens': 66073374.0, 'mean_token_accuracy': 0.7785759270191193, 'epoch': 2.77}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2060/2235 [3:46:07<16:57,  5.81s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2061/2235 [3:46:13<16:50,  5.81s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2062/2235 [3:46:19<16:43,  5.80s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2063/2235 [3:46:25<16:36,  5.80s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2064/2235 [3:46:30<16:30,  5.79s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2065/2235 [3:46:36<16:23,  5.79s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2066/2235 [3:46:42<16:17,  5.78s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2067/2235 [3:46:48<16:11,  5.78s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2068/2235 [3:46:53<16:05,  5.78s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2069/2235 [3:46:59<15:59,  5.78s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2070/2235 [3:47:05<15:52,  5.77s/it]                                                     {'loss': 0.9631, 'grad_norm': 0.384765625, 'learning_rate': 6.77484777609702e-08, 'entropy': 0.9721237242221832, 'num_tokens': 66393301.0, 'mean_token_accuracy': 0.7913003206253052, 'epoch': 2.78}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2070/2235 [3:47:05<15:52,  5.77s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2071/2235 [3:47:11<15:47,  5.78s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2072/2235 [3:47:17<15:41,  5.77s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2073/2235 [3:47:22<15:35,  5.77s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2074/2235 [3:47:28<15:30,  5.78s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2075/2235 [3:47:34<15:22,  5.77s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2076/2235 [3:47:40<15:16,  5.76s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2077/2235 [3:47:45<15:11,  5.77s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2078/2235 [3:47:51<15:05,  5.77s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2079/2235 [3:47:57<15:00,  5.77s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2080/2235 [3:48:03<14:56,  5.78s/it]                                                     {'loss': 1.0203, 'grad_norm': 0.390625, 'learning_rate': 5.986362493333592e-08, 'entropy': 1.033627423644066, 'num_tokens': 66713618.0, 'mean_token_accuracy': 0.7814078271389008, 'epoch': 2.79}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2080/2235 [3:48:03<14:56,  5.78s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2081/2235 [3:48:09<14:51,  5.79s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2082/2235 [3:48:14<14:47,  5.80s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2083/2235 [3:48:20<14:42,  5.81s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2084/2235 [3:48:26<14:36,  5.81s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2085/2235 [3:48:32<14:33,  5.82s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2086/2235 [3:48:38<14:29,  5.83s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2087/2235 [3:48:44<14:22,  5.83s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2088/2235 [3:48:49<14:16,  5.82s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2089/2235 [3:48:55<14:09,  5.82s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2090/2235 [3:49:01<14:01,  5.80s/it]                                                     {'loss': 1.0011, 'grad_norm': 0.3359375, 'learning_rate': 5.246088821014378e-08, 'entropy': 1.0122787177562713, 'num_tokens': 67034421.0, 'mean_token_accuracy': 0.7872266083955765, 'epoch': 2.81}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2090/2235 [3:49:01<14:01,  5.80s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2091/2235 [3:49:07<13:55,  5.80s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2092/2235 [3:49:12<13:49,  5.80s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2093/2235 [3:49:18<13:43,  5.80s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2094/2235 [3:49:24<13:36,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2095/2235 [3:49:30<13:30,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2096/2235 [3:49:36<13:24,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2097/2235 [3:49:41<13:18,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2098/2235 [3:49:47<13:12,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2099/2235 [3:49:53<13:06,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2100/2235 [3:49:59<13:01,  5.79s/it]                                                     {'loss': 1.0014, 'grad_norm': 0.41015625, 'learning_rate': 4.5541730205789505e-08, 'entropy': 1.0140005618333816, 'num_tokens': 67354426.0, 'mean_token_accuracy': 0.7866132497787476, 'epoch': 2.82}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2100/2235 [3:49:59<13:01,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2101/2235 [3:50:05<12:55,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2102/2235 [3:50:10<12:50,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2103/2235 [3:50:16<12:43,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2104/2235 [3:50:22<12:38,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2105/2235 [3:50:28<12:32,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2106/2235 [3:50:33<12:26,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2107/2235 [3:50:39<12:21,  5.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2108/2235 [3:50:45<12:16,  5.80s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2109/2235 [3:50:51<12:12,  5.81s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2110/2235 [3:50:57<12:07,  5.82s/it]                                                     {'loss': 1.0801, 'grad_norm': 0.46875, 'learning_rate': 3.910751799038326e-08, 'entropy': 1.09088596701622, 'num_tokens': 67674453.0, 'mean_token_accuracy': 0.7693911880254746, 'epoch': 2.83}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2110/2235 [3:50:57<12:07,  5.82s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2111/2235 [3:51:03<12:03,  5.84s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2112/2235 [3:51:09<11:57,  5.84s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2113/2235 [3:51:14<11:50,  5.82s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2114/2235 [3:51:20<11:41,  5.80s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2115/2235 [3:51:26<11:35,  5.79s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2116/2235 [3:51:32<11:29,  5.79s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2117/2235 [3:51:37<11:23,  5.79s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2118/2235 [3:51:43<11:16,  5.78s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2119/2235 [3:51:49<11:08,  5.77s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2120/2235 [3:51:55<11:02,  5.76s/it]                                                     {'loss': 1.05, 'grad_norm': 0.38671875, 'learning_rate': 3.315952281964713e-08, 'entropy': 1.061597967147827, 'num_tokens': 67997411.0, 'mean_token_accuracy': 0.7737174034118652, 'epoch': 2.85}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2120/2235 [3:51:55<11:02,  5.76s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2121/2235 [3:52:00<10:56,  5.76s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2122/2235 [3:52:06<10:50,  5.76s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2123/2235 [3:52:12<10:45,  5.76s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2124/2235 [3:52:18<10:39,  5.76s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2125/2235 [3:52:23<10:33,  5.76s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2126/2235 [3:52:29<10:27,  5.76s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2127/2235 [3:52:35<10:21,  5.76s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2128/2235 [3:52:41<10:15,  5.75s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2129/2235 [3:52:46<10:10,  5.75s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2130/2235 [3:52:52<10:04,  5.76s/it]                                                     {'loss': 1.0461, 'grad_norm': 0.341796875, 'learning_rate': 2.7698919883742127e-08, 'entropy': 1.0633781105279922, 'num_tokens': 68316635.0, 'mean_token_accuracy': 0.7755333751440048, 'epoch': 2.86}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2130/2235 [3:52:52<10:04,  5.76s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2131/2235 [3:52:58<09:57,  5.75s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2132/2235 [3:53:04<09:52,  5.75s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2133/2235 [3:53:09<09:47,  5.76s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2134/2235 [3:53:15<09:42,  5.76s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2135/2235 [3:53:21<09:36,  5.77s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2136/2235 [3:53:27<09:31,  5.77s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2137/2235 [3:53:33<09:25,  5.78s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2138/2235 [3:53:38<09:20,  5.78s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2139/2235 [3:53:44<09:13,  5.77s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2140/2235 [3:53:50<09:08,  5.77s/it]                                                     {'loss': 0.9794, 'grad_norm': 0.419921875, 'learning_rate': 2.272678807507922e-08, 'entropy': 0.9906397938728333, 'num_tokens': 68635854.0, 'mean_token_accuracy': 0.7898510515689849, 'epoch': 2.87}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2140/2235 [3:53:50<09:08,  5.77s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2141/2235 [3:53:56<09:03,  5.78s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2142/2235 [3:54:02<08:58,  5.79s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2143/2235 [3:54:07<08:53,  5.80s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2144/2235 [3:54:13<08:48,  5.81s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2145/2235 [3:54:19<08:43,  5.82s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2146/2235 [3:54:25<08:39,  5.83s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2147/2235 [3:54:31<08:33,  5.83s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2148/2235 [3:54:36<08:26,  5.82s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2149/2235 [3:54:42<08:20,  5.81s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2150/2235 [3:54:48<08:13,  5.81s/it]                                                     {'loss': 0.9932, 'grad_norm': 0.3984375, 'learning_rate': 1.824410977515234e-08, 'entropy': 1.009441262483597, 'num_tokens': 68956393.0, 'mean_token_accuracy': 0.7868784755468369, 'epoch': 2.89}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2150/2235 [3:54:48<08:13,  5.81s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2151/2235 [3:54:54<08:07,  5.80s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2152/2235 [3:55:00<08:00,  5.79s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2153/2235 [3:55:05<07:54,  5.79s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2154/2235 [3:55:11<07:48,  5.79s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2155/2235 [3:55:17<07:42,  5.78s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2156/2235 [3:55:23<07:35,  5.77s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2157/2235 [3:55:28<07:29,  5.77s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2158/2235 [3:55:34<07:24,  5.77s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2159/2235 [3:55:40<07:18,  5.77s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2160/2235 [3:55:46<07:13,  5.78s/it]                                                     {'loss': 1.0316, 'grad_norm': 0.38671875, 'learning_rate': 1.4251770660443087e-08, 'entropy': 1.03691286444664, 'num_tokens': 69278593.0, 'mean_token_accuracy': 0.7784869134426117, 'epoch': 2.9}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2160/2235 [3:55:46<07:13,  5.78s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2161/2235 [3:55:52<07:06,  5.76s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2162/2235 [3:55:57<07:01,  5.77s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2163/2235 [3:56:03<06:55,  5.78s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2164/2235 [3:56:09<06:50,  5.78s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2165/2235 [3:56:15<06:45,  5.79s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2166/2235 [3:56:21<06:39,  5.79s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2167/2235 [3:56:26<06:33,  5.79s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2168/2235 [3:56:32<06:28,  5.80s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2169/2235 [3:56:38<06:19,  5.75s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2170/2235 [3:56:44<06:15,  5.78s/it]                                                     {'loss': 1.0743, 'grad_norm': 0.341796875, 'learning_rate': 1.0750559527430982e-08, 'entropy': 1.0870393365621567, 'num_tokens': 69599301.0, 'mean_token_accuracy': 0.7692572891712188, 'epoch': 2.91}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2170/2235 [3:56:44<06:15,  5.78s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2171/2235 [3:56:49<06:11,  5.80s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2172/2235 [3:56:55<06:06,  5.81s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2173/2235 [3:57:01<06:00,  5.81s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2174/2235 [3:57:07<05:53,  5.80s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2175/2235 [3:57:13<05:47,  5.79s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2176/2235 [3:57:18<05:41,  5.79s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2177/2235 [3:57:24<05:35,  5.78s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2178/2235 [3:57:30<05:29,  5.78s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2179/2235 [3:57:36<05:23,  5.78s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2180/2235 [3:57:42<05:17,  5.77s/it]                                                     {'loss': 1.0311, 'grad_norm': 0.40234375, 'learning_rate': 7.741168136744248e-09, 'entropy': 1.044893518090248, 'num_tokens': 69920427.0, 'mean_token_accuracy': 0.7772417515516281, 'epoch': 2.93}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2180/2235 [3:57:42<05:17,  5.77s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2181/2235 [3:57:47<05:11,  5.77s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2182/2235 [3:57:53<05:06,  5.77s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2183/2235 [3:57:59<05:00,  5.78s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2184/2235 [3:58:05<04:54,  5.78s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2185/2235 [3:58:10<04:48,  5.78s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2186/2235 [3:58:16<04:42,  5.77s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2187/2235 [3:58:22<04:37,  5.77s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2188/2235 [3:58:28<04:30,  5.76s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2189/2235 [3:58:33<04:25,  5.76s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2190/2235 [3:58:39<04:16,  5.70s/it]                                                     {'loss': 1.0087, 'grad_norm': 0.447265625, 'learning_rate': 5.224191076484142e-09, 'entropy': 1.0216041058301926, 'num_tokens': 70240871.0, 'mean_token_accuracy': 0.7846454352140426, 'epoch': 2.94}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2190/2235 [3:58:39<04:16,  5.70s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2191/2235 [3:58:45<04:12,  5.73s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2192/2235 [3:58:51<04:07,  5.75s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2193/2235 [3:58:56<04:01,  5.76s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2194/2235 [3:59:02<03:56,  5.77s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2195/2235 [3:59:08<03:50,  5.77s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2196/2235 [3:59:14<03:45,  5.78s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2197/2235 [3:59:20<03:39,  5.78s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2198/2235 [3:59:25<03:34,  5.79s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2199/2235 [3:59:31<03:29,  5.81s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2200/2235 [3:59:37<03:21,  5.76s/it]                                                     {'loss': 0.9912, 'grad_norm': 0.419921875, 'learning_rate': 3.200125644747809e-09, 'entropy': 1.0056394070386887, 'num_tokens': 70560687.0, 'mean_token_accuracy': 0.7876120388507843, 'epoch': 2.95}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2200/2235 [3:59:37<03:21,  5.76s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2201/2235 [3:59:43<03:16,  5.79s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2202/2235 [3:59:48<03:11,  5.80s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2203/2235 [3:59:54<03:05,  5.79s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2204/2235 [4:00:00<02:59,  5.79s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2205/2235 [4:00:06<02:53,  5.79s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2206/2235 [4:00:12<02:47,  5.78s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2207/2235 [4:00:17<02:41,  5.78s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2208/2235 [4:00:23<02:35,  5.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2209/2235 [4:00:29<02:30,  5.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2210/2235 [4:00:35<02:24,  5.78s/it]                                                     {'loss': 0.9937, 'grad_norm': 0.359375, 'learning_rate': 1.6693717513721619e-09, 'entropy': 1.0065648406744003, 'num_tokens': 70881479.0, 'mean_token_accuracy': 0.7865663051605225, 'epoch': 2.97}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2210/2235 [4:00:35<02:24,  5.78s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2211/2235 [4:00:40<02:18,  5.78s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2212/2235 [4:00:46<02:12,  5.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2213/2235 [4:00:52<02:07,  5.78s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2214/2235 [4:00:58<02:01,  5.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2215/2235 [4:01:04<01:55,  5.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2216/2235 [4:01:09<01:49,  5.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2217/2235 [4:01:15<01:43,  5.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2218/2235 [4:01:21<01:38,  5.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2219/2235 [4:01:27<01:32,  5.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2220/2235 [4:01:32<01:26,  5.77s/it]                                                     {'loss': 1.0089, 'grad_norm': 0.44921875, 'learning_rate': 6.322318389226368e-10, 'entropy': 1.0206875443458556, 'num_tokens': 71201312.0, 'mean_token_accuracy': 0.7837065637111664, 'epoch': 2.98}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2220/2235 [4:01:32<01:26,  5.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2221/2235 [4:01:38<01:20,  5.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2222/2235 [4:01:44<01:14,  5.77s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2223/2235 [4:01:50<01:09,  5.77s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2224/2235 [4:01:56<01:03,  5.78s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2225/2235 [4:02:01<00:57,  5.78s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2226/2235 [4:02:07<00:51,  5.77s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2227/2235 [4:02:13<00:45,  5.72s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2228/2235 [4:02:18<00:40,  5.74s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2229/2235 [4:02:24<00:34,  5.76s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2230/2235 [4:02:30<00:28,  5.77s/it]                                                     {'loss': 1.0393, 'grad_norm': 0.416015625, 'learning_rate': 8.891082293516206e-11, 'entropy': 1.057813972234726, 'num_tokens': 71521949.0, 'mean_token_accuracy': 0.7777211278676986, 'epoch': 2.99}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2230/2235 [4:02:30<00:28,  5.77s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2231/2235 [4:02:36<00:23,  5.79s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2232/2235 [4:02:42<00:17,  5.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2233/2235 [4:02:48<00:11,  5.83s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2234/2235 [4:02:53<00:05,  5.83s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2235/2235 [4:02:59<00:00,  5.79s/it]Skipping save on non-main process (rank 3)Skipping save on non-main process (rank 1)

Skipping save on non-main process (rank 2)                                                     
{'train_runtime': 14581.9402, 'train_samples_per_second': 4.905, 'train_steps_per_second': 0.153, 'train_loss': 1.1223766241564312, 'epoch': 3.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2235/2235 [4:03:00<00:00,  5.79s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2235/2235 [4:03:00<00:00,  6.52s/it]

Training completed all 3 epochs.
Model saved.
wandb: updating run metadata
wandb: uploading history steps 228-228, summary, console lines 251-254
wandb: 
wandb: Run history:
wandb:             eval/entropy â–ˆâ–‚â–â–â–
wandb:                eval/loss â–ˆâ–‚â–â–â–
wandb: eval/mean_token_accuracy â–â–‡â–ˆâ–ˆâ–ˆ
wandb:          eval/num_tokens â–â–ƒâ–„â–†â–ˆ
wandb:             eval/runtime â–â–ˆâ–‡â–‡â–…
wandb:  eval/samples_per_second â–ˆâ–â–‚â–‚â–„
wandb:    eval/steps_per_second â–ˆâ–â–ƒâ–ƒâ–…
wandb:            train/entropy â–ˆâ–‡â–‡â–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:              train/epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:        train/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                       +5 ...
wandb: 
wandb: Run summary:
wandb:             eval/entropy 1.0534
wandb:                eval/loss 1.04086
wandb: eval/mean_token_accuracy 0.77686
wandb:          eval/num_tokens 59818697.0
wandb:             eval/runtime 326.0633
wandb:  eval/samples_per_second 18.279
wandb:    eval/steps_per_second 0.574
wandb:               total_flos 3.306648899036381e+18
wandb:            train/entropy 1.05781
wandb:              train/epoch 3
wandb:                      +10 ...
wandb: 
wandb: ðŸš€ View run finetuned_llama_8b_lr5e6 at: https://wandb.ai/av670-university-of-cambridge/part-ii-project/runs/d0vm0a05
wandb: â­ï¸ View project at: https://wandb.ai/av670-university-of-cambridge/part-ii-project
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260215_031546-d0vm0a05/logs
finetune completed
evaluating model
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/ubuntu/Part-II-Project/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/ubuntu/Part-II-Project/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/ubuntu/Part-II-Project/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
{'accuracy': 0.3915, 'f1': 0.19578620056222606, 'precision_per_class': {'appreciation': np.float64(0.3167259786476868), 'depreciation': np.float64(0.31153846153846154), 'unchanged': np.float64(0.40370156159629844)}, 'recall_per_class': {'appreciation': np.float64(0.07416666666666667), 'depreciation': np.float64(0.0675), 'unchanged': np.float64(0.8725)}, 'f1_per_class': {'appreciation': np.float64(0.12018906144496962), 'depreciation': np.float64(0.11095890410958904), 'unchanged': np.float64(0.5519968366943456)}, 'support_per_class': {'appreciation': np.int64(1200), 'depreciation': np.int64(1200), 'unchanged': np.int64(1600)}}


              precision    recall  f1-score   support

appreciation       0.32      0.07      0.12      1200
depreciation       0.31      0.07      0.11      1200
   unchanged       0.40      0.87      0.55      1600
 unchanged",       0.00      0.00      0.00         0

    accuracy                           0.39      4000
   macro avg       0.26      0.25      0.20      4000
weighted avg       0.35      0.39      0.29      4000

evaluation completed
